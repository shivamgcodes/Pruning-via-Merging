{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from PIL import Image\n",
    "sys.path.append('..') \n",
    "from classes import IMAGENET2012_CLASSES\n",
    "import sys\n",
    "import seaborn as sns\n",
    "from fvcore.nn import FlopCountAnalysis, parameter_count_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,  \n",
    "    AutoTokenizer,        \n",
    "    AutoModel,\n",
    "    AutoImageProcessor ,\n",
    "    ViTForImageClassification\n",
    ")\n",
    "import argparse  \n",
    "import time      \n",
    "import numpy as np  \n",
    "import json          \n",
    "from tqdm import tqdm \n",
    "import random         \n",
    "import pandas as pd    \n",
    "from sklearn.feature_selection import mutual_info_regression  \n",
    "from sklearn.neighbors import NearestNeighbors              \n",
    "import pickle          \n",
    "import logging         \n",
    "import gc   \n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "           \n",
    "\n",
    "# Define the possible choices for multiple-choice questions\n",
    "choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "def format_example(df, idx, include_answer=True):\n",
    "    \"\"\"\n",
    "    Formats a single example from the DataFrame into a image prompt.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        idx (int): The index of the row to format.\n",
    "        include_answer (bool): Whether to include the correct answer.\n",
    "\n",
    "    Returns:\n",
    "        PIL object: The PIL string.\n",
    "    \"\"\"\n",
    "    val_dir = r'C:\\Users\\hp\\Desktop\\mka on VIT-mae\\Pruning-via-Merging\\val_images'\n",
    "    image_path = os.path.join(val_dir, df.iloc[idx,0])\n",
    "    return Image.open(image_path)\n",
    "\n",
    "def set_seed(seed: int = 1):\n",
    "    \"\"\"\n",
    "    Sets the random seed for reproducibility across various libraries and environments.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): The seed value to set. Defaults to 1.\n",
    "    \"\"\"\n",
    "    random.seed(seed)  # Set seed for Python's random module\n",
    "    np.random.seed(seed)  # Set seed for NumPy\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)  # Set seed for Python hash-based operations\n",
    "    torch.manual_seed(seed)  # Set seed for PyTorch CPU\n",
    "    torch.cuda.manual_seed(seed)  # Set seed for PyTorch CUDA\n",
    "    torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior in cuDNN\n",
    "    torch.backends.cudnn.benchmark = False     # Disable cuDNN benchmark for consistency\n",
    "\n",
    "def adaptive_chunk_size(total_size, preferred_size=100):\n",
    "    \"\"\"\n",
    "    Determines the optimal chunk size for processing to maximize efficiency.\n",
    "\n",
    "    Args:\n",
    "        total_size (int): The total number of elements to process.\n",
    "        preferred_size (int, optional): The preferred chunk size. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        int: The adaptive chunk size.\n",
    "    \"\"\"\n",
    "    # Iterate from preferred_size down to 1 to find the largest divisor of total_size\n",
    "    for size in range(preferred_size, 0, -1):\n",
    "        if total_size % size == 0:\n",
    "            return size\n",
    "    return 1  # Fallback to 1 if no divisor is found\n",
    "\n",
    "def L2_distance_chunked(a, b, df, total_size):\n",
    "    \"\"\"\n",
    "    Generates L2 distance chunks between two arrays in an adaptive chunked manner.\n",
    "\n",
    "    Args:\n",
    "        a (np.ndarray): First array of shape (n_samples_a, n_features).\n",
    "        b (np.ndarray): Second array of shape (n_samples_b, n_features).\n",
    "        df (int): Flag to determine if diagonal should be zeroed.\n",
    "        total_size (int): Total number of samples.\n",
    "\n",
    "    Yields:\n",
    "        np.ndarray: A chunk of L2 distances.\n",
    "    \"\"\"\n",
    "    # Determine the chunk size adaptively\n",
    "    chunk_size = adaptive_chunk_size(total_size)\n",
    "    # Reshape a and b if they have more than 2 dimensions\n",
    "    if a.ndim > 2:\n",
    "        a = a.reshape(-1, a.shape[-1])\n",
    "    if b.ndim > 2:\n",
    "        b = b.reshape(-1, b.shape[-1])\n",
    "\n",
    "    # Ensure a and b have the same number of features\n",
    "    assert a.shape[1] == b.shape[1], \"Incompatible shapes\"\n",
    "\n",
    "    # Iterate over chunks of a\n",
    "    for i in range(0, a.shape[0], chunk_size):\n",
    "        # Compute squared norms for the current chunk of a\n",
    "        aa = np.sum(a[i : i + chunk_size] ** 2, axis=1, keepdims=True)\n",
    "        # Iterate over chunks of b\n",
    "        for j in range(0, b.shape[0], chunk_size):\n",
    "            # Compute squared norms for the current chunk of b\n",
    "            bb = np.sum(b[j : j + chunk_size] ** 2, axis=1, keepdims=True).T\n",
    "            # Compute the dot product between chunks of a and b\n",
    "            ab = a[i : i + chunk_size] @ b[j : j + chunk_size].T\n",
    "            # Compute the L2 distance chunk\n",
    "            d_chunk = np.sqrt(np.abs(aa + bb - 2 * ab))\n",
    "\n",
    "            # If df flag is set to 1 and processing diagonal chunks, set diagonal to 0\n",
    "            if df == 1:\n",
    "                if i == j:\n",
    "                    np.fill_diagonal(d_chunk, 0)  # Set diagonal to 0 if needed\n",
    "\n",
    "            # Yield the computed distance chunk\n",
    "            yield d_chunk\n",
    "\n",
    "def diffusionKernel(X, sigmaK, alpha, d, total_size):\n",
    "    \"\"\"\n",
    "    Computes the diffusion kernel embedding for the dataset X.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Input data of shape (n_samples, n_features).\n",
    "        sigmaK (float): Kernel scale parameter.\n",
    "        alpha (float): Scaling factor for normalization.\n",
    "        d (int): Target dimensionality for embedding.\n",
    "        total_size (int): Total number of samples.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Embedded data of shape (n_samples, d).\n",
    "    \"\"\"\n",
    "    # Determine the optimal chunk size for processing\n",
    "    chunk_size = adaptive_chunk_size(total_size)\n",
    "    print(\"Starting diffusion kernel computation...\")\n",
    "    kernel_start_time = time.time()\n",
    "\n",
    "    n = X.shape[0]  # Number of samples\n",
    "    # Initialize the kernel matrix with zeros\n",
    "    K = np.zeros((n, n), dtype=np.float32)\n",
    "\n",
    "    # Iterate over chunks of X to compute the kernel matrix\n",
    "    for i in range(0, n, chunk_size):\n",
    "        for j in range(0, n, chunk_size):\n",
    "            i_end = min(i + chunk_size, n)\n",
    "            j_end = min(j + chunk_size, n)\n",
    "            # Compute the L2 distance chunk between X[i:i_end] and X[j:j_end]\n",
    "            D_chunk = next(L2_distance_chunked(X[i:i_end], X[j:j_end], df=1, total_size=n))\n",
    "            # Compute the kernel chunk using the diffusion kernel formula\n",
    "            K_chunk = np.exp(-((D_chunk / sigmaK) ** 0.5))\n",
    "            # Assign the computed chunk to the appropriate position in K\n",
    "            K[i:i_end, j:j_end] = K_chunk[: i_end - i, : j_end - j]\n",
    "\n",
    "    # Calculate the sum of the kernel matrix along columns\n",
    "    p = np.sum(K, axis=0)\n",
    "    # Normalize the kernel matrix\n",
    "    K1 = K / (p * p.reshape(-1, 1)) ** alpha\n",
    "    # Compute the normalization factor\n",
    "    v = np.sqrt(np.sum(K1, axis=0))\n",
    "    # Normalize the kernel matrix further\n",
    "    A = K1 / np.outer(v, v)\n",
    "\n",
    "    # Compute the condition number of the matrix A for numerical stability\n",
    "    cond_num = np.linalg.cond(A)\n",
    "    print(f\"Condition number: {cond_num}\")\n",
    "\n",
    "    # If the condition number is infinite, apply regularization to stabilize\n",
    "    if np.isinf(cond_num):\n",
    "        print(\"Infinite condition number detected. Applying regularization...\")\n",
    "        regularization = 1e-6\n",
    "        max_iterations = 10\n",
    "        iteration = 0\n",
    "        while np.isinf(cond_num) and iteration < max_iterations:\n",
    "            # Add a small value to the diagonal for regularization\n",
    "            A += np.eye(A.shape[0]) * regularization\n",
    "            cond_num = np.linalg.cond(A)\n",
    "            regularization *= 10  # Increase regularization factor exponentially\n",
    "            iteration += 1\n",
    "        print(f\"Regularization applied. New condition number: {cond_num}\")\n",
    "\n",
    "    # Replace any NaNs in A with zero\n",
    "    A = np.nan_to_num(A)\n",
    "\n",
    "    # Handle very small values by setting them to a minimum threshold\n",
    "    zero_mask = np.abs(A) < 1e-12\n",
    "    A[zero_mask] = 1e-12\n",
    "\n",
    "    # Perform Singular Value Decomposition (SVD) on the matrix A\n",
    "    U, S, V = np.linalg.svd(A, full_matrices=False)\n",
    "    # Retain only the top (d + 1) singular vectors\n",
    "    U = U[:, :d + 1]\n",
    "    # Avoid division by zero by replacing zeros in the first column\n",
    "    U[:, 0] = np.where(U[:, 0] == 0, 1e-8, U[:, 0])\n",
    "    # Normalize U by the first column\n",
    "    U = U / U[:, 0].reshape(-1, 1)\n",
    "\n",
    "    # Extract the embedded coordinates excluding the first column\n",
    "    Y = U[:, 1 : d + 1]\n",
    "\n",
    "    kernel_end_time = time.time()\n",
    "    print(f\"Diffusion kernel computation completed in {kernel_end_time - kernel_start_time:.2f} seconds.\")\n",
    "    return Y\n",
    "\n",
    "def extract_layer_params(model, layer_idx, input_ids):\n",
    "    \"\"\"\n",
    "    Extracts the activations from a specific layer of the model given input tokens.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The language model.\n",
    "        layer_idx (int): The index of the layer to extract.\n",
    "        input_ids (torch.Tensor): Tokenized input IDs.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Activations from the specified layer, adjusted to a maximum length of 512.\n",
    "    \"\"\"\n",
    "    # Perform a forward pass with no gradient computation to get hidden states\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**input_ids, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states  # List of hidden states from each layer\n",
    "        # Extract activations from the specified layer and move to CPU\n",
    "        activations = hidden_states[layer_idx].detach().float().cpu().numpy()\n",
    "\n",
    "    # Define the maximum sequence length\n",
    "    max_length = 512\n",
    "    # If the sequence length is shorter than max_length, pad with zeros\n",
    "    if activations.shape[1] < max_length:\n",
    "        padding = max_length - activations.shape[1]\n",
    "        activations = np.pad(activations, ((0, 0), (0, padding), (0, 0)), \"constant\")\n",
    "    # If the sequence length is longer than max_length, truncate\n",
    "    elif activations.shape[1] > max_length:\n",
    "        activations = activations[:, :max_length, :]\n",
    "\n",
    "    return activations\n",
    "\n",
    "def load_embeddings(directory_path):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses layer embeddings from pickle files in the specified directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing embedding files.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of NumPy arrays containing embeddings for each layer.\n",
    "    \"\"\"\n",
    "    embeddings = []  # List to store embeddings from each file\n",
    "    # Sort filenames based on the numerical value after the first underscore\n",
    "    filenames = sorted(\n",
    "        os.listdir(directory_path), key=lambda x: int(x.split(\"_\")[1].split(\".\")[0])\n",
    "    )\n",
    "    # Iterate over each file in the sorted list\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".pkl\"):  # Process only pickle files\n",
    "            with open(os.path.join(directory_path, filename), \"rb\") as f:\n",
    "                embedding = pickle.load(f)\n",
    "                # Replace NaNs and infinite values with zeros\n",
    "                embedding = np.nan_to_num(embedding, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "                # Apply rank normalization to the embeddings\n",
    "                embedding = (\n",
    "                    np.argsort(np.argsort(embedding, axis=0), axis=0)\n",
    "                    / embedding.shape[0]\n",
    "                )\n",
    "\n",
    "                # Append the preprocessed embedding to the list\n",
    "                embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "def entropy_estimator_knn(x, k=1):\n",
    "    \"\"\"\n",
    "    Estimates the entropy of the dataset x using a k-nearest neighbors approach.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Input data of shape (n_samples, n_features).\n",
    "        k (int, optional): Number of neighbors to consider. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        float: Estimated entropy.\n",
    "    \"\"\"\n",
    "    n, d = x.shape  # Number of samples and dimensions\n",
    "    # Initialize the NearestNeighbors model\n",
    "    nbrs = NearestNeighbors(n_neighbors=k + 1, algorithm=\"auto\").fit(x)\n",
    "    # Compute the distances to the nearest neighbors\n",
    "    distances, _ = nbrs.kneighbors(x)\n",
    "    # Take the distance to the k-th neighbor (excluding the point itself)\n",
    "    distances = distances[:, -1]\n",
    "    # Compute the entropy estimate using the KNN formula\n",
    "    return -np.mean(np.log(k / (n * distances**d)))\n",
    "\n",
    "def compute_similarity_matrix_npib_global(embeddings, n_neighbors=5, k_entropy=50):\n",
    "    \"\"\"\n",
    "    Computes a similarity matrix between different layers based on normalized pointwise information bottleneck (NPIB).\n",
    "\n",
    "    Args:\n",
    "        embeddings (list): List of NumPy arrays containing embeddings for each layer.\n",
    "        n_neighbors (int, optional): Number of neighbors for mutual information computation. Defaults to 5.\n",
    "        k_entropy (int, optional): Number of neighbors for entropy estimation. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The computed similarity matrix of shape (num_layers, num_layers).\n",
    "    \"\"\"\n",
    "    num_layers = len(embeddings)  # Number of layers\n",
    "    # Initialize the similarity matrix with zeros\n",
    "    similarity_matrix = np.zeros((num_layers, num_layers))\n",
    "\n",
    "    # Iterate over each pair of layers\n",
    "    for i in range(num_layers):\n",
    "        for j in range(i, num_layers):\n",
    "            emb_i = embeddings[i]  # Embeddings for layer i\n",
    "            emb_j = embeddings[j]  # Embeddings for layer j\n",
    "\n",
    "            # Ensure both embeddings have the same number of samples by taking the minimum\n",
    "            min_samples = min(emb_i.shape[0], emb_j.shape[0])\n",
    "            emb_i = emb_i[:min_samples, :]\n",
    "            emb_j = emb_j[:min_samples, :]\n",
    "\n",
    "            # List to store mutual information scores for each dimension\n",
    "            mi_scores = []\n",
    "            # Compute mutual information between each dimension of emb_j and the entire emb_i\n",
    "            for dim in range(emb_j.shape[1]):\n",
    "                mi_score = mutual_info_regression(\n",
    "                    emb_i,\n",
    "                    emb_j[:, dim],\n",
    "                    discrete_features=False,\n",
    "                    n_neighbors=n_neighbors,\n",
    "                )\n",
    "                # Take the mean mutual information score for the current dimension\n",
    "                mi_scores.append(np.mean(mi_score))\n",
    "\n",
    "            # Compute the average mutual information across all dimensions\n",
    "            mutual_info = np.mean(mi_scores)\n",
    "            # Estimate the entropy for both embeddings\n",
    "            entropy_i = entropy_estimator_knn(emb_i, k=k_entropy)\n",
    "            entropy_j = entropy_estimator_knn(emb_j, k=k_entropy)\n",
    "            # Compute the normalized pointwise information bottleneck (NPIB)\n",
    "            npib = mutual_info / np.sqrt(entropy_i * entropy_j)\n",
    "\n",
    "            # Assign the computed similarity to the matrix (symmetrically)\n",
    "            similarity_matrix[i, j] = npib\n",
    "            similarity_matrix[j, i] = npib\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "def compute_fusion_ratios(similarity_matrix, sorted_pairs, beta=1.0):\n",
    "    \"\"\"\n",
    "    Computes fusion ratios based on the similarity matrix and sorted layer pairs.\n",
    "\n",
    "    Args:\n",
    "        similarity_matrix (np.ndarray): The similarity matrix between layers.\n",
    "        sorted_pairs (list of tuples): List of layer index pairs to fuse.\n",
    "        beta (float, optional): Scaling factor for the fusion ratio. Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: List containing (ratio_i, ratio_j) for each pair.\n",
    "    \"\"\"\n",
    "    fusion_ratios = []  # List to store fusion ratios for each pair\n",
    "    # Iterate over each sorted pair of layers\n",
    "    for i, j in sorted_pairs:\n",
    "        # Compute the mean similarity for each layer across all other layers\n",
    "        similarity_i = np.mean(similarity_matrix[i, :])\n",
    "        similarity_j = np.mean(similarity_matrix[j, :])\n",
    "        # Compute the total similarity for normalization\n",
    "        total_similarity = similarity_i + similarity_j\n",
    "\n",
    "        # Calculate the ratio for each layer based on their similarity\n",
    "        ratio_i = similarity_i / total_similarity\n",
    "        ratio_j = similarity_j / total_similarity\n",
    "\n",
    "        # Apply a sigmoid-like adjustment to the ratios using beta\n",
    "        adjusted_ratio_i = np.exp(beta * ratio_i) / (1 + np.exp(beta * ratio_i))\n",
    "        adjusted_ratio_j = 1 - adjusted_ratio_i\n",
    "\n",
    "        # Append the adjusted ratios as a tuple\n",
    "        fusion_ratios.append((adjusted_ratio_i, adjusted_ratio_j))\n",
    "\n",
    "    return fusion_ratios    \n",
    "\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"\n",
    "    Clears Python and CUDA memory to free up resources.\n",
    "    \"\"\"\n",
    "    gc.collect()  # Trigger garbage collection\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()  # Empty CUDA cache if available      \n",
    "\n",
    "def layer_fusion(model, layer1_idx, layer2_idx, ratio_i, weight_types):\n",
    "    \"\"\"\n",
    "    Fuses two specified layers of the model by blending their weights based on given ratios.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The language model.\n",
    "        layer1_idx (int): Index of the first layer to fuse.\n",
    "        layer2_idx (int): Index of the second layer to fuse.\n",
    "        ratio_i (float): Fusion ratio for the first layer.\n",
    "        weight_types (list): List of weight attribute names to fuse.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The model after layer fusion.\n",
    "    \"\"\"\n",
    "    print(f\"Starting fusion of layers {layer1_idx} and {layer2_idx} with ratio {ratio_i}\")\n",
    "\n",
    "    # Retrieve parameters from the first layer based on weight types\n",
    "    layer1_params = {\n",
    "        name: param\n",
    "        for name, param in model.named_parameters()\n",
    "        if f\"vit.encoder.layer.{layer1_idx}.\" in name\n",
    "    }\n",
    "    # Retrieve parameters from the second layer based on weight types\n",
    "    layer2_params = {\n",
    "        name: param\n",
    "        for name, param in model.named_parameters()\n",
    "        if f\"vit.encoder.layer.{layer2_idx}.\" in name\n",
    "    }\n",
    "\n",
    "    # Display parameters of the first layer before fusion\n",
    "    print(f\"Layer {layer1_idx} parameters before fusion:\")\n",
    "    for name in layer1_params:\n",
    "        print(f\"{name}: {layer1_params[name].shape}\")\n",
    "\n",
    "    # Display parameters of the second layer before fusion\n",
    "    print(f\"Layer {layer2_idx} parameters before fusion:\")\n",
    "    for name in layer2_params:\n",
    "        print(f\"{name}: {layer2_params[name].shape}\")\n",
    "\n",
    "    # Fuse each specified weight type\n",
    "    for weight_type in weight_types:\n",
    "#         # Get weights from both layers\n",
    "#         encoder.layer.0.attention.attention.query.bias\n",
    "# encoder.layer.0.attention.attention.key.weight\n",
    "        w1 = layer1_params.get(f\"model.vit.encoder.layer.{layer1_idx}.{weight_type}\")\n",
    "        w2 = layer2_params.get(f\"model.vit.encoder.layer.{layer2_idx}.{weight_type}\")\n",
    "        if w1 is not None and w2 is not None:\n",
    "            ratio_j = 1 - ratio_i  # Complementary ratio for the second layer\n",
    "            # Compute the fused weights as a weighted sum of both layers' weights\n",
    "            w_fused = ratio_i * w1.detach().float().cpu().numpy() + ratio_j * w2.detach().float().cpu().numpy()\n",
    "            # Convert the fused weights back to a PyTorch tensor and move to the appropriate device\n",
    "            w_fused_tensor = torch.tensor(w_fused).to(w1.device)\n",
    "            # Update the model's state dictionary with the fused weights\n",
    "            model.state_dict()[f\"model.vit.encoder.layer.{layer1_idx}.{weight_type}\"] = w_fused_tensor.view_as(w1).to(w1.dtype)\n",
    "\n",
    "    # Display parameters of the first layer after fusion\n",
    "    print(f\"Layer {layer1_idx} parameters after fusion:\")\n",
    "    for name in layer1_params:\n",
    "        print(f\"{name}: {layer1_params[name].shape}\")\n",
    "\n",
    "    # Remove the second layer from the model's layer list\n",
    "    model.vit.encoder.layer = torch.nn.ModuleList(\n",
    "        [layer for k, layer in enumerate(model.vit.encoder.layer) if k != layer2_idx]\n",
    "    )\n",
    "\n",
    "    print(f\"layer removed -  {layer2_idx}\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_csv = r\"..\\val_csv\"\n",
    "val_images = r\"..\\val_images\"\n",
    "test_images = r\"..\\test_imgs\"\n",
    "classA = 'n07920052'\n",
    "classB = 'n01484850'\n",
    "num_images = 25\n",
    "num_images_accuracy = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centering(K):\n",
    "    n = K.shape[0]\n",
    "    data_type= K.dtype\n",
    "    unit = torch.ones([n, n]).to(device, dtype=data_type)\n",
    "    I = torch.eye(n).to(device, dtype=data_type)\n",
    "    H = I - unit / n\n",
    "\n",
    "    return ((H@ K)@ H)  # HKH are the same with KH, KH is the first centering, H(KH) do the second time, results are the sme with one time centering\n",
    "    # return np.dot(H, K)  # KH\n",
    "\n",
    "\n",
    "def rbf(X, sigma=None):\n",
    "    GX = (X@ X.T).to(device)\n",
    "    KX = torch.diag(GX) - GX + (torch.diag(GX) - GX).T\n",
    "    if sigma is None:\n",
    "        mdist = torch.median(KX[KX != 0])\n",
    "        sigma = torch.sqrt(mdist)\n",
    "    KX *= - 0.5 / (sigma * sigma)\n",
    "    KX = torch.exp(KX).to(device)\n",
    "    return KX\n",
    "\n",
    "\n",
    "def kernel_HSIC(X, Y, sigma):\n",
    "    return torch.sum(centering(rbf(X, sigma)) * centering(rbf(Y, sigma)))\n",
    "\n",
    "\n",
    "def linear_HSIC(X, Y):\n",
    "    L_X = (X@ X.T)\n",
    "    L_Y = (Y@ Y.T)\n",
    "    return torch.sum(centering(L_X) * centering(L_Y))\n",
    "\n",
    "\n",
    "def linear_CKA(X, Y):\n",
    "    hsic = linear_HSIC(X, Y)\n",
    "    var1 = torch.sqrt(linear_HSIC(X, X))\n",
    "    var2 = torch.sqrt(linear_HSIC(Y, Y))\n",
    "\n",
    "    return hsic / (var1 * var2)\n",
    "\n",
    "\n",
    "def kernel_CKA(X, Y, sigma=None):\n",
    "    hsic = kernel_HSIC(X, Y, sigma)\n",
    "    var1 = torch.sqrt(kernel_HSIC(X, X, sigma))\n",
    "    var2 = torch.sqrt(kernel_HSIC(Y, Y, sigma))\n",
    "\n",
    "    return hsic / (var1 * var2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def latent_embeddings_for_class(classA, num_images, model):\n",
    "    cur_csv = os.path.join(val_csv , (classA+ '.csv'))\n",
    "    df = pd.read_csv(cur_csv, header = None)\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(num_images)):\n",
    "        image_name = None\n",
    "        if(num_images == 50):\n",
    "            print('complete run')\n",
    "            image_name = df.iloc[i+1]\n",
    "        else:\n",
    "            image_name = df.iloc[random.randint(1, 50), 0]\n",
    "        image_path = os.path.join(val_images, image_name)\n",
    "        image =Image.open(image_path)\n",
    "\n",
    "        inputs = tokenizer(image, return_tensors=\"pt\").to(device).to(torch.bfloat16)\n",
    "        \n",
    "        output = model(**inputs)\n",
    "        embeddings.append(output.hidden_states[-1][0,0])\n",
    "\n",
    "        del inputs\n",
    "        del image\n",
    "        del output\n",
    "        clear_memory()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    embeddings = torch.stack(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "def map_image_name_to_class_id(image_name):\n",
    "    # eg ILSVRC2012_val_00000002_n09193705.JPEG\n",
    "    pattern = r\"n\\d+\"\n",
    "    match = re.search(pattern, image_name)\n",
    "    match = match.group(0)\n",
    "    class_name = IMAGENET2012_CLASSES[match]\n",
    "    class_id = label2id[class_name]\n",
    "    return class_id\n",
    "\n",
    "@torch.no_grad\n",
    "def calculate_accuracy(num_images_accuracy):\n",
    "    files = os.listdir(val_images)\n",
    "    acc_arr = []\n",
    "    if(num_images_accuracy == 50000):\n",
    "            print('complete run')\n",
    "    for i in tqdm(range(num_images_accuracy), desc = 'accuracy'):\n",
    "        cur_file = None\n",
    "        if(num_images_accuracy == 50000):\n",
    "        \n",
    "            cur_file = files[i]\n",
    "        else:\n",
    "            cur_file = random.choice(files)\n",
    "        file_path = os.path.join(val_images, cur_file)\n",
    "        inputs = tokenizer(Image.open(file_path), return_tensors = \"pt\").to(device).to(torch.bfloat16)\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        predicted_class_idx = logits.argmax(-1).item()\n",
    "\n",
    "        true_class_idx = (map_image_name_to_class_id(cur_file))\n",
    "        if(predicted_class_idx == true_class_idx):\n",
    "            \n",
    "            acc_arr.append(1)\n",
    "        else:\n",
    "            \n",
    "            acc_arr.append(0)\n",
    "    \n",
    "    return (sum(acc_arr)/ len(acc_arr))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def eval_meth(classA, classB, num_images, num_images_accuracy, model ):\n",
    "    def benchmark():\n",
    "        with torch.no_grad():\n",
    "            model(inputs)\n",
    "    inputs = torch.randn(1, 3, 224, 224).to(device).to(torch.bfloat16)\n",
    "    flops = FlopCountAnalysis(model, inputs)\n",
    "    num_trials = 5000  # Number of times to run inference\n",
    "    latency = timeit.timeit(benchmark, number=num_trials) / num_trials * 1000  # Convert to ms\n",
    "    print(f\"Total FLOPs: {flops.total()} FLOPs\")\n",
    "    print(f\"latency of model = {latency:.2f} ms\")\n",
    "    X= latent_embeddings_for_class( classA, num_images, model)\n",
    "    Y= latent_embeddings_for_class( classB, num_images, model)\n",
    "    \n",
    "    cka_score = linear_CKA(X,Y)\n",
    "    clear_memory()\n",
    "    accuracy = calculate_accuracy(num_images_accuracy)\n",
    "    return (cka_score, accuracy, flops.total(), latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "selecting_layesrs = ['original', 'sorted_naive_paper', 'sorted_consecutive']\n",
    "\n",
    "def select_layer_original(num_layers):\n",
    "    return((num_layers-2, num_layers-1))\n",
    "\n",
    "def sorted_consecutive(pairs_with_similarity_scores, similarity_matrix):\n",
    "   \n",
    "    i , j  = 0 , 0 \n",
    "    idx = 0 \n",
    "    for idx in  range(len(pairs_with_similarity_scores)):\n",
    "        i,j,_ = pairs_with_similarity_scores[idx]\n",
    "        if(j -1 == i):\n",
    "            break\n",
    "    \n",
    "    similarity_matrix = np.delete(similarity_matrix, j , axis = 0)\n",
    "\n",
    "    pairs_with_similarity_scores.pop(idx) #removing the pair that we used\n",
    "    for idx in range(len(pairs_with_similarity_scores)):   #updating the layers ahead of j, by reducing their indices\n",
    "        a, b, score = pairs_with_similarity_scores[idx]\n",
    "        new_a = a - 1 if a >= j else a\n",
    "        new_b = b - 1 if b >= j else b\n",
    "        pairs_with_similarity_scores[idx] = (new_a, new_b, score)\n",
    "\n",
    "    return(i, j, similarity_matrix)\n",
    "        \n",
    "        \n",
    "\n",
    "def sorted_naive_paper(pairs_with_similarity_scores, similarity_matrix):\n",
    "    #does not care about consecutiveness and merges whatever two layers have the highest similarity'\n",
    "\n",
    "    #in future i can upgrade the pairs list to have all the pairs, and here i can just take a subset of tht list, operate on that and then \n",
    "    #remove the corresponding arrays ffrom the entire array, i am foing it rn\n",
    "    top_pair = pairs_with_similarity_scores[0]\n",
    "\n",
    "    i, j , _ = top_pair\n",
    "    similarity_matrix = np.delete(similarity_matrix, j , axis = 0)\n",
    "\n",
    "    pairs_with_similarity_scores.pop(0)\n",
    "\n",
    "    for idx in range(len(pairs_with_similarity_scores)):\n",
    "        a, b, score = pairs_with_similarity_scores[idx]\n",
    "        new_a = a - 1 if a >= j else a\n",
    "        new_b = b - 1 if b >= j else b\n",
    "        pairs_with_similarity_scores[idx] = (new_a, new_b, score)\n",
    "\n",
    "    return(i,j, similarity_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters in the model :-  86,567,656\n",
      "Total FLOPs: 16867412736 FLOPs\n",
      "latency of model = 36.35 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  5.36it/s]\n",
      "100%|██████████| 25/25 [00:04<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 100%|██████████| 50000/50000 [45:09<00:00, 18.46it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model metadata:\n",
      "Number of layers: 12\n",
      "Config num_hidden_layers: 12\n",
      "2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2200/2200 [21:09<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting diffusion kernel computation...\n",
      "Condition number: 6.119002550703646e+19\n",
      "Diffusion kernel computation completed in 6.90 seconds.\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.22 GiB for an array with shape (2200, 512, 768) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 295\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;66;03m# Optional: Print example tensor values from the state dictionary for small tensors\u001b[39;00m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;66;03m# This helps in verifying the actual data without overwhelming the output\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;66;03m# print(\"\\nExample tensor values from state dict (limited to small tensors for readability):\")\u001b[39;00m\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;66;03m# for key, tensor in state_dict.items():\u001b[39;00m\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;66;03m#     if tensor.numel() < 10:  # Only print tensors with fewer than 10 elements\u001b[39;00m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;66;03m#         print(f\"{key}: {tensor.tolist()}\")\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 295\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 174\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Apply manifold learning (diffusion kernel) to the stacked activations of each layer\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers):\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Stack all activations for the current layer vertically\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m     stacked_activations \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_layers_activations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;66;03m# Compute the embedded activations using the diffusion kernel\u001b[39;00m\n\u001b[0;32m    176\u001b[0m     embedded_activations \u001b[38;5;241m=\u001b[39m diffusionKernel(stacked_activations, sigmaK\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, d\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, total_size\u001b[38;5;241m=\u001b[39mstacked_activations\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\mka_research\\lib\\site-packages\\numpy\\_core\\shape_base.py:287\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    286\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m (arrs,)\n\u001b[1;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.22 GiB for an array with shape (2200, 512, 768) and data type float32"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    The main function that orchestrates the entire process: parsing arguments, loading the model,\n",
    "    processing data, computing embeddings and similarities, fusing layers, and saving the modified model.\n",
    "    \"\"\"\n",
    "    if '--f' in sys.argv:\n",
    "        sys.argv.remove('--f')\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Define command-line arguments with descriptions and default values\n",
    "    parser.add_argument(\"--ntrain\", \"-k\", type=int, default=5, help=\"Number of training examples to include in prompts\")\n",
    "    parser.add_argument(\"--ngpu\", \"-g\", type=int, default=4, help=\"Number of GPUs to use\")\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"google/vit-base-patch16-224\", help=\"Path to the pre-trained model\")\n",
    "    parser.add_argument(\"--num_tasks\", \"-n\", type=int, default=200, help=\"Number of Imagenet classes to process (default: 57)\")\n",
    "    parser.add_argument(\"--num_samples\", \"-m\", type=int, default=5, help=\"Number of images per class (default: 1)\")\n",
    "    parser.add_argument(\"--data_dir\", \"-d\", type=str, default=r\"C:\\Users\\hp\\Desktop\\mka on VIT-mae\\Pruning-via-Merging\\val_csv\", help=\"Directory containing the data\")\n",
    "    parser.add_argument(\"--num_layer\", \"-i\", type=int, default=8, help=\"Number of layers to fuse (default: 1)\")\n",
    "    args = parser.parse_args([])\n",
    "\n",
    "    global model_name\n",
    "    # Extract the model name from the provided model path\n",
    "    model_name = args.model_path.split(\"/\")[-1]\n",
    "    # Define the base directory for storing fused model information\n",
    "    base_dir = f\"/visual_data/yangzhao/point/EMNLP2024/layer_fus/al/{model_name}/fused_{args.num_layer}_layers\"\n",
    "\n",
    "    # Define directories for embeddings, fusion info, and merged weights\n",
    "    iteration_dir = os.path.join(base_dir, f\"iteration\")\n",
    "    embeddings_dir = os.path.join(iteration_dir, \"embeddings\")\n",
    "    fusion_info_dir = os.path.join(iteration_dir, \"fusion_info\")\n",
    "    merged_weights_dir = os.path.join(iteration_dir, \"merged_weights\")\n",
    "\n",
    "    # Create the necessary directories if they don't exist\n",
    "    os.makedirs(embeddings_dir, exist_ok=True)\n",
    "    os.makedirs(fusion_info_dir, exist_ok=True)\n",
    "    os.makedirs(merged_weights_dir, exist_ok=True)\n",
    "\n",
    "    # Configure logging to write logs to a file within fusion_info_dir\n",
    "    logging.basicConfig(filename=os.path.join(fusion_info_dir, 'experiment.log'), level=logging.INFO)\n",
    "    # Set random seeds for reproducibility\n",
    "    set_seed(1)\n",
    "\n",
    "    # Initialize the tokenizer from the pre-trained model\n",
    "    global tokenizer  # Declare as global to use in other functions if needed\n",
    "    global model\n",
    "    global cka_score_list\n",
    "    global accuracy_score_list\n",
    "    global label2id\n",
    "    global pairs_with_smiliarity_scores\n",
    "    global model_param_count\n",
    "    global flop_count_list\n",
    "    global latency_list\n",
    "    \n",
    "    accuracy_score_list = []\n",
    "    cka_score_list = []\n",
    "    pairs_with_similarity_scores = []\n",
    "    model_param_count = []\n",
    "    flop_count_list = []\n",
    "    latency_list = []\n",
    "\n",
    "    tokenizer = AutoImageProcessor.from_pretrained(\n",
    "        args.model_path,\n",
    "        use_fast=True,             # Use the fast tokenizer implementation\n",
    "        trust_remote_code=True,    # Trust remote code (required for some models)\n",
    "        add_bos_token=False,       # Do not add beginning-of-sequence token\n",
    "        add_eos_token=False,       # Do not add end-of-sequence token\n",
    "        padding_side=\"left\"        # Pad sequences on the left side\n",
    "    )\n",
    "\n",
    "    # Load the pre-trained causal language model with appropriate settings\n",
    "    model = ViTForImageClassification.from_pretrained(\n",
    "        args.model_path,\n",
    "        output_hidden_states=True,\n",
    "        trust_remote_code=True,    # Trust remote code (required for some models)\n",
    "        device_map=\"auto\",         # Automatically map layers to available devices\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32,  # Use bfloat16 if supported\n",
    "    )\n",
    "\n",
    "    label2id = model.config.label2id \n",
    "    #'n03126707' - mechanical cranes (crane2) #model gives crane, 517 for mechanical cranes\n",
    "    #n02012849 - biological cranes (crane) #model gives crane, 134 for biological cranes\n",
    "    label2id.update({'crane':134, 'crane2':517})\n",
    "    model = model.to(device)\n",
    "\n",
    "    cur_param_count =sum(p.numel() for p in model.parameters())\n",
    "    print('number of parameters in the model :- ' ,f\"{cur_param_count:,}\")\n",
    "    model_param_count.append(cur_param_count)\n",
    "    #checking the original model, with all the layers\n",
    "    eval_result = eval_meth(classA, classB, num_images,num_images_accuracy , model)\n",
    "    \n",
    "    cka_score = eval_result[0].item()\n",
    "    accuracy= eval_result[1]       \n",
    "    flop_count = eval_result[2]\n",
    "    latency = eval_result[3]\n",
    "    cka_score_list.append(cka_score)\n",
    "    accuracy_score_list.append(accuracy)\n",
    "    flop_count_list.append(flop_count)\n",
    "    latency_list.append(latency)\n",
    "\n",
    "\n",
    "        #The model is set in evaluation mode by default using model.eval()\n",
    "   # print(f\"Initial model configuration: {model.config}\")  # Display the model's configuration\n",
    "\n",
    "    # Define the types of weights to be fused between layers\n",
    "    # weight_types = [\n",
    "    #     \"mlp.down_proj.weight\",\n",
    "    #     \"mlp.up_proj.weight\", \n",
    "    #     \"mlp.gate_proj.weight\",\n",
    "    #     \"self_attn.k_proj.weight\",\n",
    "    #     \"self_attn.o_proj.weight\",\n",
    "    #     \"self_attn.q_proj.weight\",\n",
    "    #     \"self_attn.v_proj.weight\",\n",
    "#     # ]\n",
    "\n",
    "# encoder.layer.0.attention.attention.query.weight\n",
    "# encoder.layer.0.attention.attention.key.weight\n",
    "# encoder.layer.0.attention.attention.value.weight\n",
    "# encoder.layer.0.attention.output.dense.weight\n",
    "# encoder.layer.0.intermediate.dense.weight\n",
    "# encoder.layer.0.output.dense.weight\n",
    "    weight_types = [\n",
    "    \"attention.attention.query.weight\",\n",
    "    \"attention.attention.key.weight\",\n",
    "    \"attention.attention.value.weight\",\n",
    "    \"attention.output.dense.weight\",\n",
    "    \"intermediate.dense.weight\",\n",
    "    \"output.dense.weight\"\n",
    "                    ]\n",
    "    \n",
    "    # Display metadata about the model\n",
    "    print(\"Model metadata:\")\n",
    "    print(f\"Number of layers: {len(model.vit.encoder.layer)}\")\n",
    "    print(f\"Config num_hidden_layers: {model.config.num_hidden_layers}\")\n",
    "\n",
    "    # Identify all classes by listing test files and extracting subject names\n",
    "    classes = list(IMAGENET2012_CLASSES.items())  #data\n",
    "    \n",
    "\n",
    "    num_layers = model.config.num_hidden_layers  # Total number of hidden layers in the model\n",
    "    # Initialize a dictionary to store activations for each layer\n",
    "    all_layers_activations = {i: [] for i in range(num_layers)}\n",
    "\n",
    "    # # Iterate over each subject up to the specified number of tasks\n",
    "    # for cur_count, (current_class,val) in enumerate(classes[:args.num_tasks]):\n",
    "    #     # Load the test set for the current subject\n",
    "\n",
    "    #     test_df = pd.read_csv(os.path.join(args.data_dir, current_class + \".csv\"), header=None)  #data\n",
    "    #     test_df = test_df[1:]\n",
    "    #     # Determine the number of samples to process for the current subject\n",
    "    #     num_samples = min(args.num_samples, test_df.shape[0])\n",
    "    #     # Randomly select sample indices from the test set\n",
    "    #     sample_indices = random.sample(range(test_df.shape[0]), num_samples)\n",
    "\n",
    "    # Iterate over each selected sample index with a progress bar\n",
    "    val_merged_imgs = r'..\\val_merged_imgs'\n",
    "    print(len(os.listdir(val_merged_imgs)))\n",
    "    for file in tqdm(os.listdir(val_merged_imgs)):\n",
    "        # Format the test example without the answer\n",
    "        prompt = Image.open(os.path.join(val_merged_imgs, file))   #data read file\n",
    "        # Tokenize the prompt and move to GPU\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        input_ids = input_ids.to(torch.bfloat16)\n",
    "        # Iterate over each layer to extract activations\n",
    "        for layer_idx in range(num_layers):\n",
    "            activations = extract_layer_params(model, layer_idx, input_ids)\n",
    "            # Append the extracted activations to the corresponding layer's list\n",
    "            all_layers_activations[layer_idx].append(activations)\n",
    "\n",
    "        # Clear memory after processing each sample to free up resources\n",
    "        clear_memory()\n",
    "\n",
    "    # Apply manifold learning (diffusion kernel) to the stacked activations of each layer\n",
    "    for layer_idx in range(num_layers):\n",
    "        # Stack all activations for the current layer vertically\n",
    "        stacked_activations = np.vstack(all_layers_activations[layer_idx])\n",
    "        # Compute the embedded activations using the diffusion kernel\n",
    "        embedded_activations = diffusionKernel(stacked_activations, sigmaK=8, alpha=0.5, d=2, total_size=stacked_activations.shape[0])\n",
    "\n",
    "        # Define the output file path for the embedded activations\n",
    "        output_file = os.path.join(embeddings_dir, f\"layer_{layer_idx}_embedded.pkl\")\n",
    "        # Save the embedded activations to a pickle file\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            pickle.dump(embedded_activations, f)\n",
    "\n",
    "    # Load all precomputed embeddings from the embeddings directory\n",
    "    embeddings = load_embeddings(embeddings_dir)\n",
    "\n",
    "    # Compute the similarity matrix based on the loaded embeddings\n",
    "    similarity_matrix = compute_similarity_matrix_npib_global(embeddings)\n",
    "\n",
    "    for i in range(len(similarity_matrix) - 1):  # avoid out-of-bounds\n",
    "        for j in range(i+1, len(similarity_matrix)):\n",
    "            similarity_score = similarity_matrix[i, j]\n",
    "            pairs_with_similarity_scores.append((i, j, similarity_score))\n",
    "\n",
    "    \n",
    "    # Sort the pairs by similarity score in descending order\n",
    "    pairs_with_similarity_scores = sorted(pairs_with_similarity_scores, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "\n",
    "    print('len of pairs_with_similarity_score : ' , len(pairs_with_similarity_scores))\n",
    "    print(pairs_with_similarity_scores)\n",
    "    #matrix = np.array(similarity_matrix)\n",
    "    df = pd.DataFrame(similarity_matrix)\n",
    "    sns.heatmap(df, annot=True, fmt=\".2f\", cmap='coolwarm', linewidths=0.5, linecolor='black', cbar=True)\n",
    "    # Set the title and labels\n",
    "    plt.title(\"Similarity Matrix Heatmap\")\n",
    "    plt.xlabel(\"Columns\")\n",
    "    plt.ylabel(\"Rows\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "    print(df)\n",
    "    \n",
    "    #sort the layers here -shivam\n",
    "\n",
    "    # Merge layers iteratively from the last layer towards the first\n",
    "    for _ in range(args.num_layer):\n",
    "        if num_layers <= 1:\n",
    "            break  # Stop if there is only one layer left\n",
    "\n",
    "        # Define the indices of the two layers to fuse (last two layers)\n",
    "        \n",
    "        #layer_indices = select_layer_original(num_layers)\n",
    "        layer_indices = sorted_consecutive(pairs_with_similarity_scores, similarity_matrix)\n",
    "        layer1_idx = layer_indices[0] #num_layers - 2 in original\n",
    "        layer2_idx = layer_indices[1] #num_layers - 1 in original \n",
    "        # layer2 gets removed\n",
    "        \n",
    "        # Compute fusion ratios for the current pair of layers based on similarity\n",
    "        fusion_ratios = compute_fusion_ratios(similarity_matrix, [(layer1_idx, layer2_idx)])\n",
    "        adjusted_ratio_i, adjusted_ratio_j = fusion_ratios[0]\n",
    "\n",
    "        similarity_matrix = layer_indices[2] #smaller similarity matrix\n",
    "\n",
    "        print(f\"Merging Layer {layer1_idx} (Fusion Ratio: {adjusted_ratio_i:.4f}) and Layer {layer2_idx} (Fusion Ratio: {adjusted_ratio_j:.4f})\")\n",
    "\n",
    "        # Perform the actual layer fusion using the computed ratios\n",
    "        merged_model = layer_fusion(model, layer1_idx, layer2_idx, adjusted_ratio_i, weight_types)\n",
    "        model = merged_model  # Update the model with the fused layers\n",
    "\n",
    "        cur_param_count =sum(p.numel() for p in model.parameters())\n",
    "        print('number of parameters in the model :- ' ,f\"{cur_param_count:,}\")\n",
    "        model_param_count.append(cur_param_count)\n",
    "\n",
    "        num_layers -= 1  # Decrement the layer count as one layer has been merged\n",
    "         \n",
    "        eval_result = eval_meth(classA, classB, num_images,num_images_accuracy,  model)\n",
    "        \n",
    "        cka_score = eval_result[0].item()\n",
    "        accuracy= eval_result[1]       \n",
    "        flop_count = eval_result[2]\n",
    "        latency = eval_result[3]\n",
    "        cka_score_list.append(cka_score)\n",
    "        accuracy_score_list.append(accuracy)\n",
    "        flop_count_list.append(flop_count)\n",
    "        latency_list.append(latency)\n",
    "        #eval code\n",
    "\n",
    "    # Log the completion of layer fusion\n",
    "    logging.info(f\"Completed layer fusion with {args.num_layer} layers.\")\n",
    "\n",
    "    # Update the model's configuration to reflect the new number of hidden layers\n",
    "    model.config.num_hidden_layers = num_layers\n",
    "    # Save the model's configuration to the merged_weights directory\n",
    "    model.config.save_pretrained(merged_weights_dir)\n",
    "    # Save the fused model's state dictionary\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    # Display the keys and tensor shapes from the state dictionary for verification\n",
    "    print(\"Model state dict keys and tensor shapes after fusion:\")\n",
    "    for key, tensor in state_dict.items():\n",
    "        print(f\"{key}: {tensor.size()}\")\n",
    "\n",
    "    # Additionally, check and display tensor data types in the state dictionary\n",
    "    print(\"\\nChecking tensor data types in state dict:\")\n",
    "    for key, tensor in state_dict.items():\n",
    "        print(f\"{key}: {tensor.dtype}\")\n",
    "\n",
    "    # Define the save path for the merged model's state dictionary\n",
    "    save_path = os.path.join(merged_weights_dir, \"pytorch_model.bin\")\n",
    "    # Save the state dictionary to the specified path using PyTorch's save function\n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f\"Model successfully saved to {save_path}.\")\n",
    "\n",
    "    # Optional: Print example tensor values from the state dictionary for small tensors\n",
    "    # This helps in verifying the actual data without overwhelming the output\n",
    "    # print(\"\\nExample tensor values from state dict (limited to small tensors for readability):\")\n",
    "    # for key, tensor in state_dict.items():\n",
    "    #     if tensor.numel() < 10:  # Only print tensors with fewer than 10 elements\n",
    "    #         print(f\"{key}: {tensor.tolist()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_latency = latency_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vit_msn_base\n",
    "#cka\n",
    "plt.plot(cka_score_list, color='green', linestyle=':', marker='o', label = 'cka_score')  # color, dotted line, and dots for points\n",
    "\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.ylim(0, 1)\n",
    "for i, value in enumerate(cka_score_list):\n",
    "    plt.text(i, value, f\"{value:.2f}\", ha='right', va='bottom', fontsize=10)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#the best model will have a score of zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vit_msn_base\n",
    "#accuracy\n",
    "plt.plot(accuracy_score_list, color='green', linestyle=':', marker='o', label = 'accuracy')  # color, dotted line, and dots for points\n",
    "\n",
    "for i, value in enumerate(accuracy_score_list):\n",
    "    plt.text(i, value, f\"{value:.2f}\", ha='right', va='bottom', fontsize=10)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#the best model will have a score of zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#vit_msn_base\n",
    "#flop\n",
    "flop_count_list = [x/flop_count_list[0] for x in flop_count_list]\n",
    "plt.plot(flop_count_list, color='green', linestyle=':', marker='o', label = 'flop')  # color, dotted line, and dots for points\n",
    "\n",
    "for i, value in enumerate(flop_count_list):\n",
    "    plt.text(i, value, f\"{value:.2f}\", ha='right', va='bottom', fontsize=10)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#the best model will have a score of zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming model_param_count is already defined\n",
    "print(model_param_count)\n",
    "\n",
    "model_param_count = [x / model_param_count[0] * 100 for x in model_param_count]\n",
    "\n",
    "plt.plot(model_param_count, color='green', linestyle=':', marker='o', label=\"Model Param Count\")  \n",
    "\n",
    "# Add value labels\n",
    "for i, value in enumerate(model_param_count):\n",
    "    plt.text(i, value, f\"{value:.2f}\", ha='right', va='bottom', fontsize=10)\n",
    "\n",
    "plt.xlabel('layers removed')\n",
    "plt.ylabel('flop %')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(accuracy_score_list, color='green', linestyle=':', marker='o', label=\"latency and param count\")  \n",
    "\n",
    "# Use alternate_values for the text labels\n",
    "for i, value in enumerate(latency_list):\n",
    "    plt.text(i, accuracy_score_list[i], f\"{value:.2f}\", ha='right', va='bottom', fontsize=10)\n",
    "\n",
    "plt.xlabel('layers removed')\n",
    "plt.ylabel('accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming model_param_count is already defined\n",
    "print(latency_list)\n",
    "\n",
    "latency_list = [x / latency_list[0] * 100 for x in latency_list]\n",
    "\n",
    "plt.plot(latency_list, color='green', linestyle=':', marker='o', label=\"latency_list\")  \n",
    "\n",
    "# Add value labels\n",
    "for i, value in enumerate(latency_list):\n",
    "    plt.text(i, value, f\"{value:.2f}\", ha='right', va='bottom', fontsize=10)\n",
    "\n",
    "plt.xlabel('layers removed')\n",
    "plt.ylabel('flop %')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(accuracy_score_list, color='green', linestyle=':', marker='o', label=\"acc and param count\")  \n",
    "\n",
    "# Use alternate_values for the text labels\n",
    "for i, value in enumerate(model_param_count):\n",
    "    plt.text(i, accuracy_score_list[i], f\"{value:.2f}\", ha='right', va='bottom', fontsize=10)\n",
    "\n",
    "plt.xlabel('layers removed')\n",
    "plt.ylabel('accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(accuracy_score_list, color='green', linestyle=':', marker='o', label=\"latency and param count\")  \n",
    "\n",
    "# Use alternate_values for the text labels\n",
    "for i, value in enumerate(latency_list):\n",
    "    plt.text(i, accuracy_score_list[i], f\"{value:.2f}\", ha='right', va='bottom', fontsize=10)\n",
    "\n",
    "plt.xlabel('layers removed')\n",
    "plt.ylabel('accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_list_percent = [a/accuracy_score_list[0]*100 for a in accuracy_score_list]\n",
    "alternate_values = [1,2,3,4]  # Make sure this is the same length as model_param_count\n",
    "\n",
    "plt.plot(accuracy_score_list_percent, color='green', linestyle=':', marker='o', label=\"acc and param count\")  \n",
    "\n",
    "# Use alternate_values for the text labels\n",
    "for i, value in enumerate(model_param_count):\n",
    "    plt.text(i, accuracy_score_list_percent[i], f\"{value:.2f}\", ha='right', va='bottom', fontsize=10)\n",
    "\n",
    "plt.xlabel('layers removed')\n",
    "plt.ylabel('accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_drop_list = [accuracy_score_list_percent[0] - x for x in accuracy_score_list]\n",
    "param_drop_list = [model_param_count[0] - x for x in model_param_count]\n",
    "\n",
    "plt.plot(acc_drop_list, color='green', linestyle=':', marker='o', label=\"acc drop and param  %\")  \n",
    "\n",
    "# Use alternate_values for the text labels\n",
    "for i, value in enumerate(model_param_count):\n",
    "    plt.text(i, acc_drop_list[i], f\"{value:.2f}\", ha='right', va='bottom', fontsize=10)\n",
    "\n",
    "plt.xlabel('layers removed')\n",
    "plt.ylabel('accuracy drop %')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(accuracy_score_list, color='green', linestyle=':', marker='o', label=\"acc and flop %\")  \n",
    "\n",
    "# Use alternate_values for the text labels\n",
    "for i, value in enumerate(flop_count_list):\n",
    "    plt.text(i, accuracy_score_list[i], f\"{value:.2f}\", ha='right', va='bottom', fontsize=10)\n",
    "\n",
    "plt.xlabel('layers removed')\n",
    "plt.ylabel('accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc_drop_list)\n",
    "print(accuracy_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "print(len(accuracy_score_list))\n",
    "print(len(cka_score_list))\n",
    "print(len(model_param_count))\n",
    "print(len(flop_count_list))\n",
    "print(len(accuracy_score_list_percent))\n",
    "print(len(latency_list))\n",
    "print(len(original_latency))\n",
    "data = {\n",
    "\n",
    "'accurcy' : accuracy_score_list,\n",
    "'cka' : cka_score_list, \n",
    "'parameters' : model_param_count, \n",
    "'flops': flop_count_list,\n",
    "'accuracy_percent': accuracy_score_list_percent,\n",
    "'latency_percent':latency_list,\n",
    "'raw_latency':original_latency }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Generate a timestamp string (e.g., '2025-02-01_15-30-45')\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Construct the file name using the timestamp\n",
    "filename = f\"{model_name}_naive_algo(MKA once)_sorted_consecutive_paper_{timestamp}.csv\"\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "df.to_csv(filename, index=False)\n",
    "print(df.head())\n",
    "print(f\"Data written to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do \n",
    "# change the model.layers to encoder.layer\n",
    "# vet the code\n",
    "# also check ki abhi kya ho rha hai??\n",
    "# give more power to a config and remove this command line bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,v in model.named_parameters():\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mka_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
