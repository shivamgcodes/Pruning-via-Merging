{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,  \n",
    "    AutoTokenizer,        \n",
    ")\n",
    "import argparse  \n",
    "import time      \n",
    "import numpy as np  \n",
    "import json          \n",
    "from tqdm import tqdm \n",
    "import random         \n",
    "import pandas as pd    \n",
    "from sklearn.feature_selection import mutual_info_regression  \n",
    "from sklearn.neighbors import NearestNeighbors              \n",
    "import pickle          \n",
    "import logging         \n",
    "import gc              \n",
    "\n",
    "# Define the possible choices for multiple-choice questions\n",
    "choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "def format_subject(subject):\n",
    "    \"\"\"\n",
    "    Formats the subject string by replacing underscores with spaces.\n",
    "\n",
    "    Args:\n",
    "        subject (str): The subject string with underscores.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted subject string with spaces.\n",
    "    \"\"\"\n",
    "    # Split the subject by underscores\n",
    "    l = subject.split(\"_\")\n",
    "    s = \"\"\n",
    "    # Concatenate each part with a space\n",
    "    for entry in l:\n",
    "        s += \" \" + entry\n",
    "    return s\n",
    "\n",
    "def format_example(df, idx, include_answer=True):\n",
    "    \"\"\"\n",
    "    Formats a single example from the DataFrame into a string prompt.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        idx (int): The index of the row to format.\n",
    "        include_answer (bool): Whether to include the correct answer.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted example string.\n",
    "    \"\"\"\n",
    "    # Extract the question prompt from the first column\n",
    "    prompt = df.iloc[idx, 0]\n",
    "    # Determine the number of choices based on DataFrame columns\n",
    "    k = df.shape[1] - 2\n",
    "    # Append each choice to the prompt\n",
    "    for j in range(k):\n",
    "        prompt += \"\\n{}. {}\".format(choices[j], df.iloc[idx, j + 1])\n",
    "    # Add the \"Answer:\" prompt\n",
    "    prompt += \"\\nAnswer:\"\n",
    "    # Optionally include the correct answer\n",
    "    if include_answer:\n",
    "        prompt += \" {}\\n\\n\".format(df.iloc[idx, k + 1])\n",
    "    return prompt\n",
    "\n",
    "def gen_prompt(train_df, subject, k=-1):\n",
    "    \"\"\"\n",
    "    Generates a prompt containing multiple training examples for the given subject.\n",
    "\n",
    "    Args:\n",
    "        train_df (pd.DataFrame): The DataFrame containing training data.\n",
    "        subject (str): The subject name.\n",
    "        k (int, optional): Number of training examples to include. Defaults to -1 (all).\n",
    "\n",
    "    Returns:\n",
    "        str: The generated prompt string.\n",
    "    \"\"\"\n",
    "    # Start the prompt with a description of the task\n",
    "    prompt = \"The following are multiple choice questions (with answers) about {}.\\n\\n\".format(\n",
    "        format_subject(subject)\n",
    "    )\n",
    "    # If k is not specified, use all training examples\n",
    "    if k == -1:\n",
    "        k = train_df.shape[0]\n",
    "    # Append each training example to the prompt\n",
    "    for i in range(k):\n",
    "        prompt += format_example(train_df, i)\n",
    "    return prompt\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval(args, subject, model, tokenizer, dev_df, test_df):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test dataset for a specific subject.\n",
    "\n",
    "    Args:\n",
    "        args (argparse.Namespace): Parsed command-line arguments.\n",
    "        subject (str): The subject name.\n",
    "        model (torch.nn.Module): The language model.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer.\n",
    "        dev_df (pd.DataFrame): Development set DataFrame.\n",
    "        test_df (pd.DataFrame): Test set DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (list of correctness for each example, accuracy, perplexity)\n",
    "    \"\"\"\n",
    "    cors = []          # List to store correctness of each prediction\n",
    "    all_probs = []     # List to store probabilities (unused in current code)\n",
    "    total_loss = 0     # Accumulator for total loss to compute perplexity\n",
    "\n",
    "    # Iterate over each test example with a progress bar\n",
    "    for i in tqdm(range(test_df.shape[0]), desc=f\"Evaluating {subject}\"):\n",
    "        k = args.ntrain  # Number of training examples to include in the prompt\n",
    "        # Format the current test example without the answer\n",
    "        prompt_end = format_example(test_df, i, include_answer=False)\n",
    "        # Generate the training prompt with k examples\n",
    "        train_prompt = gen_prompt(dev_df, subject, k)\n",
    "        # Combine training prompt and test example prompt\n",
    "        prompt = train_prompt + prompt_end\n",
    "        # Tokenize the combined prompt and move to GPU\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "        # Clone input_ids for labels\n",
    "        labels = input_ids.clone()\n",
    "        # Mask the training part of the prompt in labels by setting them to -100\n",
    "        labels[:, :-len(tokenizer(prompt_end).input_ids)] = -100\n",
    "\n",
    "        # Forward pass through the model to get outputs\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        # Extract logits for the last token\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        # Extract loss for the current example\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Accumulate the loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Compute probabilities using softmax on logits\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1).detach().float().cpu().numpy()\n",
    "        # Determine the predicted choice by selecting the choice with the highest probability\n",
    "        pred = choices[np.argmax(probs[:, [tokenizer(c).input_ids[-1] for c in choices]])]\n",
    "        # Extract the true label from the test DataFrame\n",
    "        label = test_df.iloc[i, test_df.shape[1] - 1]\n",
    "\n",
    "        # Check if the prediction is correct\n",
    "        cor = pred == label\n",
    "        cors.append(cor)\n",
    "\n",
    "    # Calculate average accuracy\n",
    "    acc = np.mean(cors)\n",
    "    print(\"Average accuracy {:.3f} - {}\".format(acc, subject))\n",
    "\n",
    "    # Calculate the average loss and then the perplexity\n",
    "    avg_loss = total_loss / len(test_df)\n",
    "    ppl = np.exp(avg_loss)\n",
    "    print(\"Perplexity {:.3f} - {}\".format(ppl, subject))\n",
    "\n",
    "    return cors, acc, ppl\n",
    "\n",
    "def set_seed(seed: int = 1):\n",
    "    \"\"\"\n",
    "    Sets the random seed for reproducibility across various libraries and environments.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): The seed value to set. Defaults to 1.\n",
    "    \"\"\"\n",
    "    random.seed(seed)  # Set seed for Python's random module\n",
    "    np.random.seed(seed)  # Set seed for NumPy\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)  # Set seed for Python hash-based operations\n",
    "    torch.manual_seed(seed)  # Set seed for PyTorch CPU\n",
    "    torch.cuda.manual_seed(seed)  # Set seed for PyTorch CUDA\n",
    "    torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior in cuDNN\n",
    "    torch.backends.cudnn.benchmark = False     # Disable cuDNN benchmark for consistency\n",
    "\n",
    "def adaptive_chunk_size(total_size, preferred_size=100):\n",
    "    \"\"\"\n",
    "    Determines the optimal chunk size for processing to maximize efficiency.\n",
    "\n",
    "    Args:\n",
    "        total_size (int): The total number of elements to process.\n",
    "        preferred_size (int, optional): The preferred chunk size. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        int: The adaptive chunk size.\n",
    "    \"\"\"\n",
    "    # Iterate from preferred_size down to 1 to find the largest divisor of total_size\n",
    "    for size in range(preferred_size, 0, -1):\n",
    "        if total_size % size == 0:\n",
    "            return size\n",
    "    return 1  # Fallback to 1 if no divisor is found\n",
    "\n",
    "def L2_distance_chunked(a, b, df, total_size):\n",
    "    \"\"\"\n",
    "    Generates L2 distance chunks between two arrays in an adaptive chunked manner.\n",
    "\n",
    "    Args:\n",
    "        a (np.ndarray): First array of shape (n_samples_a, n_features).\n",
    "        b (np.ndarray): Second array of shape (n_samples_b, n_features).\n",
    "        df (int): Flag to determine if diagonal should be zeroed.\n",
    "        total_size (int): Total number of samples.\n",
    "\n",
    "    Yields:\n",
    "        np.ndarray: A chunk of L2 distances.\n",
    "    \"\"\"\n",
    "    # Determine the chunk size adaptively\n",
    "    chunk_size = adaptive_chunk_size(total_size)\n",
    "    # Reshape a and b if they have more than 2 dimensions\n",
    "    if a.ndim > 2:\n",
    "        a = a.reshape(-1, a.shape[-1])\n",
    "    if b.ndim > 2:\n",
    "        b = b.reshape(-1, b.shape[-1])\n",
    "\n",
    "    # Ensure a and b have the same number of features\n",
    "    assert a.shape[1] == b.shape[1], \"Incompatible shapes\"\n",
    "\n",
    "    # Iterate over chunks of a\n",
    "    for i in range(0, a.shape[0], chunk_size):\n",
    "        # Compute squared norms for the current chunk of a\n",
    "        aa = np.sum(a[i : i + chunk_size] ** 2, axis=1, keepdims=True)\n",
    "        # Iterate over chunks of b\n",
    "        for j in range(0, b.shape[0], chunk_size):\n",
    "            # Compute squared norms for the current chunk of b\n",
    "            bb = np.sum(b[j : j + chunk_size] ** 2, axis=1, keepdims=True).T\n",
    "            # Compute the dot product between chunks of a and b\n",
    "            ab = a[i : i + chunk_size] @ b[j : j + chunk_size].T\n",
    "            # Compute the L2 distance chunk\n",
    "            d_chunk = np.sqrt(np.abs(aa + bb - 2 * ab))\n",
    "\n",
    "            # If df flag is set to 1 and processing diagonal chunks, set diagonal to 0\n",
    "            if df == 1:\n",
    "                if i == j:\n",
    "                    np.fill_diagonal(d_chunk, 0)  # Set diagonal to 0 if needed\n",
    "\n",
    "            # Yield the computed distance chunk\n",
    "            yield d_chunk\n",
    "\n",
    "def diffusionKernel(X, sigmaK, alpha, d, total_size):\n",
    "    \"\"\"\n",
    "    Computes the diffusion kernel embedding for the dataset X.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Input data of shape (n_samples, n_features).\n",
    "        sigmaK (float): Kernel scale parameter.\n",
    "        alpha (float): Scaling factor for normalization.\n",
    "        d (int): Target dimensionality for embedding.\n",
    "        total_size (int): Total number of samples.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Embedded data of shape (n_samples, d).\n",
    "    \"\"\"\n",
    "    # Determine the optimal chunk size for processing\n",
    "    chunk_size = adaptive_chunk_size(total_size)\n",
    "    print(\"Starting diffusion kernel computation...\")\n",
    "    kernel_start_time = time.time()\n",
    "\n",
    "    n = X.shape[0]  # Number of samples\n",
    "    # Initialize the kernel matrix with zeros\n",
    "    K = np.zeros((n, n), dtype=np.float32)\n",
    "\n",
    "    # Iterate over chunks of X to compute the kernel matrix\n",
    "    for i in range(0, n, chunk_size):\n",
    "        for j in range(0, n, chunk_size):\n",
    "            i_end = min(i + chunk_size, n)\n",
    "            j_end = min(j + chunk_size, n)\n",
    "            # Compute the L2 distance chunk between X[i:i_end] and X[j:j_end]\n",
    "            D_chunk = next(L2_distance_chunked(X[i:i_end], X[j:j_end], df=1, total_size=n))\n",
    "            # Compute the kernel chunk using the diffusion kernel formula\n",
    "            K_chunk = np.exp(-((D_chunk / sigmaK) ** 0.5))\n",
    "            # Assign the computed chunk to the appropriate position in K\n",
    "            K[i:i_end, j:j_end] = K_chunk[: i_end - i, : j_end - j]\n",
    "\n",
    "    # Calculate the sum of the kernel matrix along columns\n",
    "    p = np.sum(K, axis=0)\n",
    "    # Normalize the kernel matrix\n",
    "    K1 = K / (p * p.reshape(-1, 1)) ** alpha\n",
    "    # Compute the normalization factor\n",
    "    v = np.sqrt(np.sum(K1, axis=0))\n",
    "    # Normalize the kernel matrix further\n",
    "    A = K1 / np.outer(v, v)\n",
    "\n",
    "    # Compute the condition number of the matrix A for numerical stability\n",
    "    cond_num = np.linalg.cond(A)\n",
    "    print(f\"Condition number: {cond_num}\")\n",
    "\n",
    "    # If the condition number is infinite, apply regularization to stabilize\n",
    "    if np.isinf(cond_num):\n",
    "        print(\"Infinite condition number detected. Applying regularization...\")\n",
    "        regularization = 1e-6\n",
    "        max_iterations = 10\n",
    "        iteration = 0\n",
    "        while np.isinf(cond_num) and iteration < max_iterations:\n",
    "            # Add a small value to the diagonal for regularization\n",
    "            A += np.eye(A.shape[0]) * regularization\n",
    "            cond_num = np.linalg.cond(A)\n",
    "            regularization *= 10  # Increase regularization factor exponentially\n",
    "            iteration += 1\n",
    "        print(f\"Regularization applied. New condition number: {cond_num}\")\n",
    "\n",
    "    # Replace any NaNs in A with zero\n",
    "    A = np.nan_to_num(A)\n",
    "\n",
    "    # Handle very small values by setting them to a minimum threshold\n",
    "    zero_mask = np.abs(A) < 1e-12\n",
    "    A[zero_mask] = 1e-12\n",
    "\n",
    "    # Perform Singular Value Decomposition (SVD) on the matrix A\n",
    "    U, S, V = np.linalg.svd(A, full_matrices=False)\n",
    "    # Retain only the top (d + 1) singular vectors\n",
    "    U = U[:, :d + 1]\n",
    "    # Avoid division by zero by replacing zeros in the first column\n",
    "    U[:, 0] = np.where(U[:, 0] == 0, 1e-8, U[:, 0])\n",
    "    # Normalize U by the first column\n",
    "    U = U / U[:, 0].reshape(-1, 1)\n",
    "\n",
    "    # Extract the embedded coordinates excluding the first column\n",
    "    Y = U[:, 1 : d + 1]\n",
    "\n",
    "    kernel_end_time = time.time()\n",
    "    print(f\"Diffusion kernel computation completed in {kernel_end_time - kernel_start_time:.2f} seconds.\")\n",
    "    return Y\n",
    "\n",
    "def extract_layer_params(model, layer_idx, input_ids):\n",
    "    \"\"\"\n",
    "    Extracts the activations from a specific layer of the model given input tokens.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The language model.\n",
    "        layer_idx (int): The index of the layer to extract.\n",
    "        input_ids (torch.Tensor): Tokenized input IDs.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Activations from the specified layer, adjusted to a maximum length of 512.\n",
    "    \"\"\"\n",
    "    # Perform a forward pass with no gradient computation to get hidden states\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states  # List of hidden states from each layer\n",
    "        # Extract activations from the specified layer and move to CPU\n",
    "        activations = hidden_states[layer_idx].detach().float().cpu().numpy()\n",
    "\n",
    "    # Define the maximum sequence length\n",
    "    max_length = 512\n",
    "    # If the sequence length is shorter than max_length, pad with zeros\n",
    "    if activations.shape[1] < max_length:\n",
    "        padding = max_length - activations.shape[1]\n",
    "        activations = np.pad(activations, ((0, 0), (0, padding), (0, 0)), \"constant\")\n",
    "    # If the sequence length is longer than max_length, truncate\n",
    "    elif activations.shape[1] > max_length:\n",
    "        activations = activations[:, :max_length, :]\n",
    "\n",
    "    return activations\n",
    "\n",
    "def load_embeddings(directory_path):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses layer embeddings from pickle files in the specified directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing embedding files.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of NumPy arrays containing embeddings for each layer.\n",
    "    \"\"\"\n",
    "    embeddings = []  # List to store embeddings from each file\n",
    "    # Sort filenames based on the numerical value after the first underscore\n",
    "    filenames = sorted(\n",
    "        os.listdir(directory_path), key=lambda x: int(x.split(\"_\")[1].split(\".\")[0])\n",
    "    )\n",
    "    # Iterate over each file in the sorted list\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".pkl\"):  # Process only pickle files\n",
    "            with open(os.path.join(directory_path, filename), \"rb\") as f:\n",
    "                embedding = pickle.load(f)\n",
    "                # Replace NaNs and infinite values with zeros\n",
    "                embedding = np.nan_to_num(embedding, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "                # Apply rank normalization to the embeddings\n",
    "                embedding = (\n",
    "                    np.argsort(np.argsort(embedding, axis=0), axis=0)\n",
    "                    / embedding.shape[0]\n",
    "                )\n",
    "\n",
    "                # Append the preprocessed embedding to the list\n",
    "                embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "def entropy_estimator_knn(x, k=1):\n",
    "    \"\"\"\n",
    "    Estimates the entropy of the dataset x using a k-nearest neighbors approach.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Input data of shape (n_samples, n_features).\n",
    "        k (int, optional): Number of neighbors to consider. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        float: Estimated entropy.\n",
    "    \"\"\"\n",
    "    n, d = x.shape  # Number of samples and dimensions\n",
    "    # Initialize the NearestNeighbors model\n",
    "    nbrs = NearestNeighbors(n_neighbors=k + 1, algorithm=\"auto\").fit(x)\n",
    "    # Compute the distances to the nearest neighbors\n",
    "    distances, _ = nbrs.kneighbors(x)\n",
    "    # Take the distance to the k-th neighbor (excluding the point itself)\n",
    "    distances = distances[:, -1]\n",
    "    # Compute the entropy estimate using the KNN formula\n",
    "    return -np.mean(np.log(k / (n * distances**d)))\n",
    "\n",
    "def compute_similarity_matrix_npib_global(embeddings, n_neighbors=5, k_entropy=50):\n",
    "    \"\"\"\n",
    "    Computes a similarity matrix between different layers based on normalized pointwise information bottleneck (NPIB).\n",
    "\n",
    "    Args:\n",
    "        embeddings (list): List of NumPy arrays containing embeddings for each layer.\n",
    "        n_neighbors (int, optional): Number of neighbors for mutual information computation. Defaults to 5.\n",
    "        k_entropy (int, optional): Number of neighbors for entropy estimation. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The computed similarity matrix of shape (num_layers, num_layers).\n",
    "    \"\"\"\n",
    "    num_layers = len(embeddings)  # Number of layers\n",
    "    # Initialize the similarity matrix with zeros\n",
    "    similarity_matrix = np.zeros((num_layers, num_layers))\n",
    "\n",
    "    # Iterate over each pair of layers\n",
    "    for i in range(num_layers):\n",
    "        for j in range(i, num_layers):\n",
    "            emb_i = embeddings[i]  # Embeddings for layer i\n",
    "            emb_j = embeddings[j]  # Embeddings for layer j\n",
    "\n",
    "            # Ensure both embeddings have the same number of samples by taking the minimum\n",
    "            min_samples = min(emb_i.shape[0], emb_j.shape[0])\n",
    "            emb_i = emb_i[:min_samples, :]\n",
    "            emb_j = emb_j[:min_samples, :]\n",
    "\n",
    "            # List to store mutual information scores for each dimension\n",
    "            mi_scores = []\n",
    "            # Compute mutual information between each dimension of emb_j and the entire emb_i\n",
    "            for dim in range(emb_j.shape[1]):\n",
    "                mi_score = mutual_info_regression(\n",
    "                    emb_i,\n",
    "                    emb_j[:, dim],\n",
    "                    discrete_features=False,\n",
    "                    n_neighbors=n_neighbors,\n",
    "                )\n",
    "                # Take the mean mutual information score for the current dimension\n",
    "                mi_scores.append(np.mean(mi_score))\n",
    "\n",
    "            # Compute the average mutual information across all dimensions\n",
    "            mutual_info = np.mean(mi_scores)\n",
    "            # Estimate the entropy for both embeddings\n",
    "            entropy_i = entropy_estimator_knn(emb_i, k=k_entropy)\n",
    "            entropy_j = entropy_estimator_knn(emb_j, k=k_entropy)\n",
    "            # Compute the normalized pointwise information bottleneck (NPIB)\n",
    "            npib = mutual_info / np.sqrt(entropy_i * entropy_j)\n",
    "\n",
    "            # Assign the computed similarity to the matrix (symmetrically)\n",
    "            similarity_matrix[i, j] = npib\n",
    "            similarity_matrix[j, i] = npib\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "def compute_fusion_ratios(similarity_matrix, sorted_pairs, beta=1.0):\n",
    "    \"\"\"\n",
    "    Computes fusion ratios based on the similarity matrix and sorted layer pairs.\n",
    "\n",
    "    Args:\n",
    "        similarity_matrix (np.ndarray): The similarity matrix between layers.\n",
    "        sorted_pairs (list of tuples): List of layer index pairs to fuse.\n",
    "        beta (float, optional): Scaling factor for the fusion ratio. Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: List containing (ratio_i, ratio_j) for each pair.\n",
    "    \"\"\"\n",
    "    fusion_ratios = []  # List to store fusion ratios for each pair\n",
    "    # Iterate over each sorted pair of layers\n",
    "    for i, j in sorted_pairs:\n",
    "        # Compute the mean similarity for each layer across all other layers\n",
    "        similarity_i = np.mean(similarity_matrix[i, :])\n",
    "        similarity_j = np.mean(similarity_matrix[j, :])\n",
    "        # Compute the total similarity for normalization\n",
    "        total_similarity = similarity_i + similarity_j\n",
    "\n",
    "        # Calculate the ratio for each layer based on their similarity\n",
    "        ratio_i = similarity_i / total_similarity\n",
    "        ratio_j = similarity_j / total_similarity\n",
    "\n",
    "        # Apply a sigmoid-like adjustment to the ratios using beta\n",
    "        adjusted_ratio_i = np.exp(beta * ratio_i) / (1 + np.exp(beta * ratio_i))\n",
    "        adjusted_ratio_j = 1 - adjusted_ratio_i\n",
    "\n",
    "        # Append the adjusted ratios as a tuple\n",
    "        fusion_ratios.append((adjusted_ratio_i, adjusted_ratio_j))\n",
    "\n",
    "    return fusion_ratios    \n",
    "\n",
    "def evaluate(model, tokenizer, args):\n",
    "    \"\"\"\n",
    "    Evaluates the model across all specified subjects and computes accuracy and perplexity.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The language model.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer.\n",
    "        args (argparse.Namespace): Parsed command-line arguments.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Dictionaries containing accuracy and perplexity for each subject.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Identify all subjects by listing test files and extracting subject names\n",
    "    subjects = sorted(\n",
    "        [\n",
    "            f.split(\"_test.csv\")[0]\n",
    "            for f in os.listdir(os.path.join(args.data_dir, \"test\")) \n",
    "            if \"_test.csv\" in f\n",
    "        ]\n",
    "    )\n",
    "    all_accs = {}  # Dictionary to store accuracy for each subject\n",
    "    all_ppls = {}  # Dictionary to store perplexity for each subject\n",
    "\n",
    "    # Iterate over each subject\n",
    "    for subject in subjects:\n",
    "        # Load the development set for the current subject and take the first k examples\n",
    "        dev_df = pd.read_csv(\n",
    "            os.path.join(args.data_dir, \"dev\", subject + \"_dev.csv\"), header=None  \n",
    "        )[: args.ntrain]\n",
    "        # Load the test set for the current subject\n",
    "        test_df = pd.read_csv(\n",
    "            os.path.join(args.data_dir, \"test\", subject + \"_test.csv\"), header=None\n",
    "        )\n",
    "        \n",
    "        # Evaluate the model on the current subject's test set\n",
    "        _, acc, ppl = eval(args, subject, model, tokenizer, dev_df, test_df)\n",
    "        \n",
    "        # Store the accuracy and perplexity\n",
    "        all_accs[subject] = acc\n",
    "        all_ppls[subject] = ppl\n",
    "        \n",
    "    model.train()  # Set the model back to training mode\n",
    "    return all_accs, all_ppls\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"\n",
    "    Clears Python and CUDA memory to free up resources.\n",
    "    \"\"\"\n",
    "    gc.collect()  # Trigger garbage collection\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()  # Empty CUDA cache if available      \n",
    "\n",
    "def layer_fusion(model, layer1_idx, layer2_idx, ratio_i, weight_types):\n",
    "    \"\"\"\n",
    "    Fuses two specified layers of the model by blending their weights based on given ratios.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The language model.\n",
    "        layer1_idx (int): Index of the first layer to fuse.\n",
    "        layer2_idx (int): Index of the second layer to fuse.\n",
    "        ratio_i (float): Fusion ratio for the first layer.\n",
    "        weight_types (list): List of weight attribute names to fuse.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The model after layer fusion.\n",
    "    \"\"\"\n",
    "    print(f\"Starting fusion of layers {layer1_idx} and {layer2_idx} with ratio {ratio_i}\")\n",
    "\n",
    "    # Retrieve parameters from the first layer based on weight types\n",
    "    layer1_params = {\n",
    "        name: param\n",
    "        for name, param in model.named_parameters()\n",
    "        if f\"model.layers.{layer1_idx}.\" in name\n",
    "    }\n",
    "    # Retrieve parameters from the second layer based on weight types\n",
    "    layer2_params = {\n",
    "        name: param\n",
    "        for name, param in model.named_parameters()\n",
    "        if f\"model.layers.{layer2_idx}.\" in name\n",
    "    }\n",
    "\n",
    "    # Display parameters of the first layer before fusion\n",
    "    print(f\"Layer {layer1_idx} parameters before fusion:\")\n",
    "    for name in layer1_params:\n",
    "        print(f\"{name}: {layer1_params[name].shape}\")\n",
    "\n",
    "    # Display parameters of the second layer before fusion\n",
    "    print(f\"Layer {layer2_idx} parameters before fusion:\")\n",
    "    for name in layer2_params:\n",
    "        print(f\"{name}: {layer2_params[name].shape}\")\n",
    "\n",
    "    # Fuse each specified weight type\n",
    "    for weight_type in weight_types:\n",
    "        # Get weights from both layers\n",
    "        w1 = layer1_params.get(f\"model.layers.{layer1_idx}.{weight_type}\")\n",
    "        w2 = layer2_params.get(f\"model.layers.{layer2_idx}.{weight_type}\")\n",
    "        if w1 is not None and w2 is not None:\n",
    "            ratio_j = 1 - ratio_i  # Complementary ratio for the second layer\n",
    "            # Compute the fused weights as a weighted sum of both layers' weights\n",
    "            w_fused = ratio_i * w1.detach().float().cpu().numpy() + ratio_j * w2.detach().float().cpu().numpy()\n",
    "            # Convert the fused weights back to a PyTorch tensor and move to the appropriate device\n",
    "            w_fused_tensor = torch.tensor(w_fused).to(w1.device)\n",
    "            # Update the model's state dictionary with the fused weights\n",
    "            model.state_dict()[f\"model.layers.{layer1_idx}.{weight_type}\"] = w_fused_tensor.view_as(w1).to(w1.dtype)\n",
    "\n",
    "    # Display parameters of the first layer after fusion\n",
    "    print(f\"Layer {layer1_idx} parameters after fusion:\")\n",
    "    for name in layer1_params:\n",
    "        print(f\"{name}: {layer1_params[name].shape}\")\n",
    "\n",
    "    # Remove the second layer from the model's layer list\n",
    "    model.model.layers = torch.nn.ModuleList(\n",
    "        [layer for k, layer in enumerate(model.model.layers) if k != layer2_idx]\n",
    "    )\n",
    "\n",
    "    print(f\"Model layers after removal of layer {layer2_idx}\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    The main function that orchestrates the entire process: parsing arguments, loading the model,\n",
    "    processing data, computing embeddings and similarities, fusing layers, and saving the modified model.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Define command-line arguments with descriptions and default values\n",
    "    parser.add_argument(\"--ntrain\", \"-k\", type=int, default=5, help=\"Number of training examples to include in prompts\")\n",
    "    parser.add_argument(\"--ngpu\", \"-g\", type=int, default=4, help=\"Number of GPUs to use\")\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"facebook/vit-msn-base-4\", help=\"Path to the pre-trained model\")\n",
    "    parser.add_argument(\"--num_tasks\", \"-n\", type=int, default=57, help=\"Number of MMLU tasks to process (default: 57)\")\n",
    "    parser.add_argument(\"--num_samples\", \"-m\", type=int, default=1, help=\"Number of samples per task (default: 1)\")\n",
    "    parser.add_argument(\"--data_dir\", \"-d\", type=str, default=\"data\", help=\"Directory containing the data\")\n",
    "    parser.add_argument(\"--num_layer\", \"-i\", type=int, default=1, help=\"Number of layers to fuse (default: 1)\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Extract the model name from the provided model path\n",
    "    model_name = args.model_path.split(\"/\")[-1]\n",
    "    # Define the base directory for storing fused model information\n",
    "    base_dir = f\"/visual_data/yangzhao/point/EMNLP2024/layer_fus/al/{model_name}/fused_{args.num_layer}_layers\"\n",
    "\n",
    "    # Define directories for embeddings, fusion info, and merged weights\n",
    "    iteration_dir = os.path.join(base_dir, f\"iteration\")\n",
    "    embeddings_dir = os.path.join(iteration_dir, \"embeddings\")\n",
    "    fusion_info_dir = os.path.join(iteration_dir, \"fusion_info\")\n",
    "    merged_weights_dir = os.path.join(iteration_dir, \"merged_weights\")\n",
    "\n",
    "    # Create the necessary directories if they don't exist\n",
    "    os.makedirs(embeddings_dir, exist_ok=True)\n",
    "    os.makedirs(fusion_info_dir, exist_ok=True)\n",
    "    os.makedirs(merged_weights_dir, exist_ok=True)\n",
    "\n",
    "    # Configure logging to write logs to a file within fusion_info_dir\n",
    "    logging.basicConfig(filename=os.path.join(fusion_info_dir, 'experiment.log'), level=logging.INFO)\n",
    "    # Set random seeds for reproducibility\n",
    "    set_seed(1)\n",
    "\n",
    "    # Initialize the tokenizer from the pre-trained model\n",
    "    global tokenizer  # Declare as global to use in other functions if needed\n",
    "\n",
    "    tokenizer = AutoImageProcessor.from_pretrained(\n",
    "        args.model_path,\n",
    "        use_fast=True,             # Use the fast tokenizer implementation\n",
    "        trust_remote_code=True,    # Trust remote code (required for some models)\n",
    "        add_bos_token=False,       # Do not add beginning-of-sequence token\n",
    "        add_eos_token=False,       # Do not add end-of-sequence token\n",
    "        padding_side=\"left\"        # Pad sequences on the left side\n",
    "    )\n",
    "\n",
    "    # Load the pre-trained causal language model with appropriate settings\n",
    "    model = AutoModel.from_pretrained(\n",
    "        args.model_path,\n",
    "        trust_remote_code=True,    # Trust remote code (required for some models)\n",
    "        device_map=\"auto\",         # Automatically map layers to available devices\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32,  # Use bfloat16 if supported\n",
    "    )\n",
    "        #The model is set in evaluation mode by default using model.eval()\n",
    "    print(f\"Initial model configuration: {model.config}\")  # Display the model's configuration\n",
    "\n",
    "    # Define the types of weights to be fused between layers\n",
    "    weight_types = [\n",
    "        \"mlp.down_proj.weight\",\n",
    "        \"mlp.up_proj.weight\", \n",
    "        \"mlp.gate_proj.weight\",\n",
    "        \"self_attn.k_proj.weight\",\n",
    "        \"self_attn.o_proj.weight\",\n",
    "        \"self_attn.q_proj.weight\",\n",
    "        \"self_attn.v_proj.weight\",\n",
    "    ]\n",
    "\n",
    "    # Display metadata about the model\n",
    "    print(\"Model metadata:\")\n",
    "    print(f\"Number of layers: {len(model.model.layers)}\")\n",
    "    print(f\"Config num_hidden_layers: {model.config.num_hidden_layers}\")\n",
    "\n",
    "    # Identify all subjects by listing test files and extracting subject names\n",
    "    subjects = sorted(\n",
    "        [\n",
    "            f.split(\"_test.csv\")[0]\n",
    "            for f in os.listdir(os.path.join(args.data_dir, \"test\"))\n",
    "            if \"_test.csv\" in f\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    num_layers = model.config.num_hidden_layers  # Total number of hidden layers in the model\n",
    "    # Initialize a dictionary to store activations for each layer\n",
    "    all_layers_activations = {i: [] for i in range(num_layers)}\n",
    "\n",
    "    # Iterate over each subject up to the specified number of tasks\n",
    "    for subject in subjects[:args.num_tasks]:\n",
    "        # Load the test set for the current subject\n",
    "        test_df = pd.read_csv(os.path.join(args.data_dir, \"test\", subject + \"_test.csv\"), header=None)\n",
    "\n",
    "        # Determine the number of samples to process for the current subject\n",
    "        num_samples = min(args.num_samples, test_df.shape[0])\n",
    "        # Randomly select sample indices from the test set\n",
    "        sample_indices = random.sample(range(test_df.shape[0]), num_samples)\n",
    "\n",
    "        # Iterate over each selected sample index with a progress bar\n",
    "        for index in tqdm(sample_indices, desc=f\"Processing {subject}\"):\n",
    "            # Format the test example without the answer\n",
    "            prompt = format_example(test_df, index, include_answer=False)\n",
    "            # Tokenize the prompt and move to GPU\n",
    "            input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "            # Iterate over each layer to extract activations\n",
    "            for layer_idx in range(num_layers):\n",
    "                activations = extract_layer_params(model, layer_idx, input_ids)\n",
    "                # Append the extracted activations to the corresponding layer's list\n",
    "                all_layers_activations[layer_idx].append(activations)\n",
    "\n",
    "            # Clear memory after processing each sample to free up resources\n",
    "            clear_memory()\n",
    "\n",
    "    # Apply manifold learning (diffusion kernel) to the stacked activations of each layer\n",
    "    for layer_idx in range(num_layers):\n",
    "        # Stack all activations for the current layer vertically\n",
    "        stacked_activations = np.vstack(all_layers_activations[layer_idx])\n",
    "        # Compute the embedded activations using the diffusion kernel\n",
    "        embedded_activations = diffusionKernel(stacked_activations, sigmaK=8, alpha=0.5, d=2, total_size=stacked_activations.shape[0])\n",
    "\n",
    "        # Define the output file path for the embedded activations\n",
    "        output_file = os.path.join(embeddings_dir, f\"layer_{layer_idx}_embedded.pkl\")\n",
    "        # Save the embedded activations to a pickle file\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            pickle.dump(embedded_activations, f)\n",
    "\n",
    "    # Load all precomputed embeddings from the embeddings directory\n",
    "    embeddings = load_embeddings(embeddings_dir)\n",
    "\n",
    "    # Compute the similarity matrix based on the loaded embeddings\n",
    "    similarity_matrix = compute_similarity_matrix_npib_global(embeddings)\n",
    "\n",
    "    # Merge layers iteratively from the last layer towards the first\n",
    "    for _ in range(args.num_layer):\n",
    "        if num_layers <= 1:\n",
    "            break  # Stop if there is only one layer left\n",
    "\n",
    "        # Define the indices of the two layers to fuse (last two layers)\n",
    "        layer1_idx = num_layers - 2\n",
    "        layer2_idx = num_layers - 1\n",
    "\n",
    "        # Compute fusion ratios for the current pair of layers based on similarity\n",
    "        fusion_ratios = compute_fusion_ratios(similarity_matrix, [(layer1_idx, layer2_idx)])\n",
    "        adjusted_ratio_i, adjusted_ratio_j = fusion_ratios[0]\n",
    "\n",
    "        print(f\"Merging Layer {layer1_idx} (Fusion Ratio: {adjusted_ratio_i:.4f}) and Layer {layer2_idx} (Fusion Ratio: {adjusted_ratio_j:.4f})\")\n",
    "\n",
    "        # Perform the actual layer fusion using the computed ratios\n",
    "        merged_model = layer_fusion(model, layer1_idx, layer2_idx, adjusted_ratio_i, weight_types)\n",
    "        model = merged_model  # Update the model with the fused layers\n",
    "\n",
    "        num_layers -= 1  # Decrement the layer count as one layer has been merged\n",
    "\n",
    "    # Log the completion of layer fusion\n",
    "    logging.info(f\"Completed layer fusion with {args.num_layer} layers.\")\n",
    "\n",
    "    # Update the model's configuration to reflect the new number of hidden layers\n",
    "    model.config.num_hidden_layers = num_layers\n",
    "    # Save the model's configuration to the merged_weights directory\n",
    "    model.config.save_pretrained(merged_weights_dir)\n",
    "\n",
    "    # Save the fused model's state dictionary\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    # Display the keys and tensor shapes from the state dictionary for verification\n",
    "    print(\"Model state dict keys and tensor shapes after fusion:\")\n",
    "    for key, tensor in state_dict.items():\n",
    "        print(f\"{key}: {tensor.size()}\")\n",
    "\n",
    "    # Additionally, check and display tensor data types in the state dictionary\n",
    "    print(\"\\nChecking tensor data types in state dict:\")\n",
    "    for key, tensor in state_dict.items():\n",
    "        print(f\"{key}: {tensor.dtype}\")\n",
    "\n",
    "    # Define the save path for the merged model's state dictionary\n",
    "    save_path = os.path.join(merged_weights_dir, \"pytorch_model.bin\")\n",
    "    # Save the state dictionary to the specified path using PyTorch's save function\n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f\"Model successfully saved to {save_path}.\")\n",
    "\n",
    "    # Optional: Print example tensor values from the state dictionary for small tensors\n",
    "    # This helps in verifying the actual data without overwhelming the output\n",
    "    print(\"\\nExample tensor values from state dict (limited to small tensors for readability):\")\n",
    "    for key, tensor in state_dict.items():\n",
    "        if tensor.numel() < 10:  # Only print tensors with fewer than 10 elements\n",
    "            print(f\"{key}: {tensor.tolist()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mka_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
