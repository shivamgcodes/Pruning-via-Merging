{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from PIL import Image\n",
    "sys.path.append('..') \n",
    "from classes import IMAGENET2012_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,  \n",
    "    AutoTokenizer,        \n",
    "    AutoModel,\n",
    "    AutoImageProcessor \n",
    ")\n",
    "import argparse  \n",
    "import time      \n",
    "import numpy as np  \n",
    "import json          \n",
    "from tqdm import tqdm \n",
    "import random         \n",
    "import pandas as pd    \n",
    "from sklearn.feature_selection import mutual_info_regression  \n",
    "from sklearn.neighbors import NearestNeighbors              \n",
    "import pickle          \n",
    "import logging         \n",
    "import gc   \n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "           \n",
    "\n",
    "# Define the possible choices for multiple-choice questions\n",
    "choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "def format_example(df, idx, include_answer=True):\n",
    "    \"\"\"\n",
    "    Formats a single example from the DataFrame into a image prompt.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        idx (int): The index of the row to format.\n",
    "        include_answer (bool): Whether to include the correct answer.\n",
    "\n",
    "    Returns:\n",
    "        PIL object: The PIL string.\n",
    "    \"\"\"\n",
    "    val_dir = r'C:\\Users\\hp\\Desktop\\mka on VIT-mae\\Pruning-via-Merging\\val_images'\n",
    "    image_path = os.path.join(val_dir, df.iloc[idx,0])\n",
    "    return Image.open(image_path)\n",
    "\n",
    "def set_seed(seed: int = 1):\n",
    "    \"\"\"\n",
    "    Sets the random seed for reproducibility across various libraries and environments.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): The seed value to set. Defaults to 1.\n",
    "    \"\"\"\n",
    "    random.seed(seed)  # Set seed for Python's random module\n",
    "    np.random.seed(seed)  # Set seed for NumPy\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)  # Set seed for Python hash-based operations\n",
    "    torch.manual_seed(seed)  # Set seed for PyTorch CPU\n",
    "    torch.cuda.manual_seed(seed)  # Set seed for PyTorch CUDA\n",
    "    torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior in cuDNN\n",
    "    torch.backends.cudnn.benchmark = False     # Disable cuDNN benchmark for consistency\n",
    "\n",
    "def adaptive_chunk_size(total_size, preferred_size=100):\n",
    "    \"\"\"\n",
    "    Determines the optimal chunk size for processing to maximize efficiency.\n",
    "\n",
    "    Args:\n",
    "        total_size (int): The total number of elements to process.\n",
    "        preferred_size (int, optional): The preferred chunk size. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        int: The adaptive chunk size.\n",
    "    \"\"\"\n",
    "    # Iterate from preferred_size down to 1 to find the largest divisor of total_size\n",
    "    for size in range(preferred_size, 0, -1):\n",
    "        if total_size % size == 0:\n",
    "            return size\n",
    "    return 1  # Fallback to 1 if no divisor is found\n",
    "\n",
    "def L2_distance_chunked(a, b, df, total_size):\n",
    "    \"\"\"\n",
    "    Generates L2 distance chunks between two arrays in an adaptive chunked manner.\n",
    "\n",
    "    Args:\n",
    "        a (np.ndarray): First array of shape (n_samples_a, n_features).\n",
    "        b (np.ndarray): Second array of shape (n_samples_b, n_features).\n",
    "        df (int): Flag to determine if diagonal should be zeroed.\n",
    "        total_size (int): Total number of samples.\n",
    "\n",
    "    Yields:\n",
    "        np.ndarray: A chunk of L2 distances.\n",
    "    \"\"\"\n",
    "    # Determine the chunk size adaptively\n",
    "    chunk_size = adaptive_chunk_size(total_size)\n",
    "    # Reshape a and b if they have more than 2 dimensions\n",
    "    if a.ndim > 2:\n",
    "        a = a.reshape(-1, a.shape[-1])\n",
    "    if b.ndim > 2:\n",
    "        b = b.reshape(-1, b.shape[-1])\n",
    "\n",
    "    # Ensure a and b have the same number of features\n",
    "    assert a.shape[1] == b.shape[1], \"Incompatible shapes\"\n",
    "\n",
    "    # Iterate over chunks of a\n",
    "    for i in range(0, a.shape[0], chunk_size):\n",
    "        # Compute squared norms for the current chunk of a\n",
    "        aa = np.sum(a[i : i + chunk_size] ** 2, axis=1, keepdims=True)\n",
    "        # Iterate over chunks of b\n",
    "        for j in range(0, b.shape[0], chunk_size):\n",
    "            # Compute squared norms for the current chunk of b\n",
    "            bb = np.sum(b[j : j + chunk_size] ** 2, axis=1, keepdims=True).T\n",
    "            # Compute the dot product between chunks of a and b\n",
    "            ab = a[i : i + chunk_size] @ b[j : j + chunk_size].T\n",
    "            # Compute the L2 distance chunk\n",
    "            d_chunk = np.sqrt(np.abs(aa + bb - 2 * ab))\n",
    "\n",
    "            # If df flag is set to 1 and processing diagonal chunks, set diagonal to 0\n",
    "            if df == 1:\n",
    "                if i == j:\n",
    "                    np.fill_diagonal(d_chunk, 0)  # Set diagonal to 0 if needed\n",
    "\n",
    "            # Yield the computed distance chunk\n",
    "            yield d_chunk\n",
    "\n",
    "def diffusionKernel(X, sigmaK, alpha, d, total_size):\n",
    "    \"\"\"\n",
    "    Computes the diffusion kernel embedding for the dataset X.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Input data of shape (n_samples, n_features).\n",
    "        sigmaK (float): Kernel scale parameter.\n",
    "        alpha (float): Scaling factor for normalization.\n",
    "        d (int): Target dimensionality for embedding.\n",
    "        total_size (int): Total number of samples.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Embedded data of shape (n_samples, d).\n",
    "    \"\"\"\n",
    "    # Determine the optimal chunk size for processing\n",
    "    chunk_size = adaptive_chunk_size(total_size)\n",
    "    print(\"Starting diffusion kernel computation...\")\n",
    "    kernel_start_time = time.time()\n",
    "\n",
    "    n = X.shape[0]  # Number of samples\n",
    "    # Initialize the kernel matrix with zeros\n",
    "    K = np.zeros((n, n), dtype=np.float32)\n",
    "\n",
    "    # Iterate over chunks of X to compute the kernel matrix\n",
    "    for i in range(0, n, chunk_size):\n",
    "        for j in range(0, n, chunk_size):\n",
    "            i_end = min(i + chunk_size, n)\n",
    "            j_end = min(j + chunk_size, n)\n",
    "            # Compute the L2 distance chunk between X[i:i_end] and X[j:j_end]\n",
    "            D_chunk = next(L2_distance_chunked(X[i:i_end], X[j:j_end], df=1, total_size=n))\n",
    "            # Compute the kernel chunk using the diffusion kernel formula\n",
    "            K_chunk = np.exp(-((D_chunk / sigmaK) ** 0.5))\n",
    "            # Assign the computed chunk to the appropriate position in K\n",
    "            K[i:i_end, j:j_end] = K_chunk[: i_end - i, : j_end - j]\n",
    "\n",
    "    # Calculate the sum of the kernel matrix along columns\n",
    "    p = np.sum(K, axis=0)\n",
    "    # Normalize the kernel matrix\n",
    "    K1 = K / (p * p.reshape(-1, 1)) ** alpha\n",
    "    # Compute the normalization factor\n",
    "    v = np.sqrt(np.sum(K1, axis=0))\n",
    "    # Normalize the kernel matrix further\n",
    "    A = K1 / np.outer(v, v)\n",
    "\n",
    "    # Compute the condition number of the matrix A for numerical stability\n",
    "    cond_num = np.linalg.cond(A)\n",
    "    print(f\"Condition number: {cond_num}\")\n",
    "\n",
    "    # If the condition number is infinite, apply regularization to stabilize\n",
    "    if np.isinf(cond_num):\n",
    "        print(\"Infinite condition number detected. Applying regularization...\")\n",
    "        regularization = 1e-6\n",
    "        max_iterations = 10\n",
    "        iteration = 0\n",
    "        while np.isinf(cond_num) and iteration < max_iterations:\n",
    "            # Add a small value to the diagonal for regularization\n",
    "            A += np.eye(A.shape[0]) * regularization\n",
    "            cond_num = np.linalg.cond(A)\n",
    "            regularization *= 10  # Increase regularization factor exponentially\n",
    "            iteration += 1\n",
    "        print(f\"Regularization applied. New condition number: {cond_num}\")\n",
    "\n",
    "    # Replace any NaNs in A with zero\n",
    "    A = np.nan_to_num(A)\n",
    "\n",
    "    # Handle very small values by setting them to a minimum threshold\n",
    "    zero_mask = np.abs(A) < 1e-12\n",
    "    A[zero_mask] = 1e-12\n",
    "\n",
    "    # Perform Singular Value Decomposition (SVD) on the matrix A\n",
    "    U, S, V = np.linalg.svd(A, full_matrices=False)\n",
    "    # Retain only the top (d + 1) singular vectors\n",
    "    U = U[:, :d + 1]\n",
    "    # Avoid division by zero by replacing zeros in the first column\n",
    "    U[:, 0] = np.where(U[:, 0] == 0, 1e-8, U[:, 0])\n",
    "    # Normalize U by the first column\n",
    "    U = U / U[:, 0].reshape(-1, 1)\n",
    "\n",
    "    # Extract the embedded coordinates excluding the first column\n",
    "    Y = U[:, 1 : d + 1]\n",
    "\n",
    "    kernel_end_time = time.time()\n",
    "    print(f\"Diffusion kernel computation completed in {kernel_end_time - kernel_start_time:.2f} seconds.\")\n",
    "    return Y\n",
    "\n",
    "def extract_layer_params(model, layer_idx, input_ids):\n",
    "    \"\"\"\n",
    "    Extracts the activations from a specific layer of the model given input tokens.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The language model.\n",
    "        layer_idx (int): The index of the layer to extract.\n",
    "        input_ids (torch.Tensor): Tokenized input IDs.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Activations from the specified layer, adjusted to a maximum length of 512.\n",
    "    \"\"\"\n",
    "    # Perform a forward pass with no gradient computation to get hidden states\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**input_ids, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states  # List of hidden states from each layer\n",
    "        # Extract activations from the specified layer and move to CPU\n",
    "        activations = hidden_states[layer_idx].detach().float().cpu().numpy()\n",
    "\n",
    "    # Define the maximum sequence length\n",
    "    max_length = 512\n",
    "    # If the sequence length is shorter than max_length, pad with zeros\n",
    "    if activations.shape[1] < max_length:\n",
    "        padding = max_length - activations.shape[1]\n",
    "        activations = np.pad(activations, ((0, 0), (0, padding), (0, 0)), \"constant\")\n",
    "    # If the sequence length is longer than max_length, truncate\n",
    "    elif activations.shape[1] > max_length:\n",
    "        activations = activations[:, :max_length, :]\n",
    "\n",
    "    return activations\n",
    "\n",
    "def load_embeddings(directory_path):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses layer embeddings from pickle files in the specified directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing embedding files.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of NumPy arrays containing embeddings for each layer.\n",
    "    \"\"\"\n",
    "    embeddings = []  # List to store embeddings from each file\n",
    "    # Sort filenames based on the numerical value after the first underscore\n",
    "    filenames = sorted(\n",
    "        os.listdir(directory_path), key=lambda x: int(x.split(\"_\")[1].split(\".\")[0])\n",
    "    )\n",
    "    # Iterate over each file in the sorted list\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".pkl\"):  # Process only pickle files\n",
    "            with open(os.path.join(directory_path, filename), \"rb\") as f:\n",
    "                embedding = pickle.load(f)\n",
    "                # Replace NaNs and infinite values with zeros\n",
    "                embedding = np.nan_to_num(embedding, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "                # Apply rank normalization to the embeddings\n",
    "                embedding = (\n",
    "                    np.argsort(np.argsort(embedding, axis=0), axis=0)\n",
    "                    / embedding.shape[0]\n",
    "                )\n",
    "\n",
    "                # Append the preprocessed embedding to the list\n",
    "                embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "def entropy_estimator_knn(x, k=1):\n",
    "    \"\"\"\n",
    "    Estimates the entropy of the dataset x using a k-nearest neighbors approach.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Input data of shape (n_samples, n_features).\n",
    "        k (int, optional): Number of neighbors to consider. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        float: Estimated entropy.\n",
    "    \"\"\"\n",
    "    n, d = x.shape  # Number of samples and dimensions\n",
    "    # Initialize the NearestNeighbors model\n",
    "    nbrs = NearestNeighbors(n_neighbors=k + 1, algorithm=\"auto\").fit(x)\n",
    "    # Compute the distances to the nearest neighbors\n",
    "    distances, _ = nbrs.kneighbors(x)\n",
    "    # Take the distance to the k-th neighbor (excluding the point itself)\n",
    "    distances = distances[:, -1]\n",
    "    # Compute the entropy estimate using the KNN formula\n",
    "    return -np.mean(np.log(k / (n * distances**d)))\n",
    "\n",
    "def compute_similarity_matrix_npib_global(embeddings, n_neighbors=5, k_entropy=50):\n",
    "    \"\"\"\n",
    "    Computes a similarity matrix between different layers based on normalized pointwise information bottleneck (NPIB).\n",
    "\n",
    "    Args:\n",
    "        embeddings (list): List of NumPy arrays containing embeddings for each layer.\n",
    "        n_neighbors (int, optional): Number of neighbors for mutual information computation. Defaults to 5.\n",
    "        k_entropy (int, optional): Number of neighbors for entropy estimation. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The computed similarity matrix of shape (num_layers, num_layers).\n",
    "    \"\"\"\n",
    "    num_layers = len(embeddings)  # Number of layers\n",
    "    # Initialize the similarity matrix with zeros\n",
    "    similarity_matrix = np.zeros((num_layers, num_layers))\n",
    "\n",
    "    # Iterate over each pair of layers\n",
    "    for i in range(num_layers):\n",
    "        for j in range(i, num_layers):\n",
    "            emb_i = embeddings[i]  # Embeddings for layer i\n",
    "            emb_j = embeddings[j]  # Embeddings for layer j\n",
    "\n",
    "            # Ensure both embeddings have the same number of samples by taking the minimum\n",
    "            min_samples = min(emb_i.shape[0], emb_j.shape[0])\n",
    "            emb_i = emb_i[:min_samples, :]\n",
    "            emb_j = emb_j[:min_samples, :]\n",
    "\n",
    "            # List to store mutual information scores for each dimension\n",
    "            mi_scores = []\n",
    "            # Compute mutual information between each dimension of emb_j and the entire emb_i\n",
    "            for dim in range(emb_j.shape[1]):\n",
    "                mi_score = mutual_info_regression(\n",
    "                    emb_i,\n",
    "                    emb_j[:, dim],\n",
    "                    discrete_features=False,\n",
    "                    n_neighbors=n_neighbors,\n",
    "                )\n",
    "                # Take the mean mutual information score for the current dimension\n",
    "                mi_scores.append(np.mean(mi_score))\n",
    "\n",
    "            # Compute the average mutual information across all dimensions\n",
    "            mutual_info = np.mean(mi_scores)\n",
    "            # Estimate the entropy for both embeddings\n",
    "            entropy_i = entropy_estimator_knn(emb_i, k=k_entropy)\n",
    "            entropy_j = entropy_estimator_knn(emb_j, k=k_entropy)\n",
    "            # Compute the normalized pointwise information bottleneck (NPIB)\n",
    "            npib = mutual_info / np.sqrt(entropy_i * entropy_j)\n",
    "\n",
    "            # Assign the computed similarity to the matrix (symmetrically)\n",
    "            similarity_matrix[i, j] = npib\n",
    "            similarity_matrix[j, i] = npib\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "def compute_fusion_ratios(similarity_matrix, sorted_pairs, beta=1.0):\n",
    "    \"\"\"\n",
    "    Computes fusion ratios based on the similarity matrix and sorted layer pairs.\n",
    "\n",
    "    Args:\n",
    "        similarity_matrix (np.ndarray): The similarity matrix between layers.\n",
    "        sorted_pairs (list of tuples): List of layer index pairs to fuse.\n",
    "        beta (float, optional): Scaling factor for the fusion ratio. Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: List containing (ratio_i, ratio_j) for each pair.\n",
    "    \"\"\"\n",
    "    fusion_ratios = []  # List to store fusion ratios for each pair\n",
    "    # Iterate over each sorted pair of layers\n",
    "    for i, j in sorted_pairs:\n",
    "        # Compute the mean similarity for each layer across all other layers\n",
    "        similarity_i = np.mean(similarity_matrix[i, :])\n",
    "        similarity_j = np.mean(similarity_matrix[j, :])\n",
    "        # Compute the total similarity for normalization\n",
    "        total_similarity = similarity_i + similarity_j\n",
    "\n",
    "        # Calculate the ratio for each layer based on their similarity\n",
    "        ratio_i = similarity_i / total_similarity\n",
    "        ratio_j = similarity_j / total_similarity\n",
    "\n",
    "        # Apply a sigmoid-like adjustment to the ratios using beta\n",
    "        adjusted_ratio_i = np.exp(beta * ratio_i) / (1 + np.exp(beta * ratio_i))\n",
    "        adjusted_ratio_j = 1 - adjusted_ratio_i\n",
    "\n",
    "        # Append the adjusted ratios as a tuple\n",
    "        fusion_ratios.append((adjusted_ratio_i, adjusted_ratio_j))\n",
    "\n",
    "    return fusion_ratios    \n",
    "\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"\n",
    "    Clears Python and CUDA memory to free up resources.\n",
    "    \"\"\"\n",
    "    gc.collect()  # Trigger garbage collection\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()  # Empty CUDA cache if available      \n",
    "\n",
    "def layer_fusion(model, layer1_idx, layer2_idx, ratio_i, weight_types):\n",
    "    \"\"\"\n",
    "    Fuses two specified layers of the model by blending their weights based on given ratios.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The language model.\n",
    "        layer1_idx (int): Index of the first layer to fuse.\n",
    "        layer2_idx (int): Index of the second layer to fuse.\n",
    "        ratio_i (float): Fusion ratio for the first layer.\n",
    "        weight_types (list): List of weight attribute names to fuse.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The model after layer fusion.\n",
    "    \"\"\"\n",
    "    print(f\"Starting fusion of layers {layer1_idx} and {layer2_idx} with ratio {ratio_i}\")\n",
    "\n",
    "    # Retrieve parameters from the first layer based on weight types\n",
    "    layer1_params = {\n",
    "        name: param\n",
    "        for name, param in model.named_parameters()\n",
    "        if f\"model.layers.{layer1_idx}.\" in name\n",
    "    }\n",
    "    # Retrieve parameters from the second layer based on weight types\n",
    "    layer2_params = {\n",
    "        name: param\n",
    "        for name, param in model.named_parameters()\n",
    "        if f\"model.layers.{layer2_idx}.\" in name\n",
    "    }\n",
    "\n",
    "    # Display parameters of the first layer before fusion\n",
    "    print(f\"Layer {layer1_idx} parameters before fusion:\")\n",
    "    for name in layer1_params:\n",
    "        print(f\"{name}: {layer1_params[name].shape}\")\n",
    "\n",
    "    # Display parameters of the second layer before fusion\n",
    "    print(f\"Layer {layer2_idx} parameters before fusion:\")\n",
    "    for name in layer2_params:\n",
    "        print(f\"{name}: {layer2_params[name].shape}\")\n",
    "\n",
    "    # Fuse each specified weight type\n",
    "    for weight_type in weight_types:\n",
    "        # Get weights from both layers\n",
    "        w1 = layer1_params.get(f\"model.layers.{layer1_idx}.{weight_type}\")\n",
    "        w2 = layer2_params.get(f\"model.layers.{layer2_idx}.{weight_type}\")\n",
    "        if w1 is not None and w2 is not None:\n",
    "            ratio_j = 1 - ratio_i  # Complementary ratio for the second layer\n",
    "            # Compute the fused weights as a weighted sum of both layers' weights\n",
    "            w_fused = ratio_i * w1.detach().float().cpu().numpy() + ratio_j * w2.detach().float().cpu().numpy()\n",
    "            # Convert the fused weights back to a PyTorch tensor and move to the appropriate device\n",
    "            w_fused_tensor = torch.tensor(w_fused).to(w1.device)\n",
    "            # Update the model's state dictionary with the fused weights\n",
    "            model.state_dict()[f\"model.layers.{layer1_idx}.{weight_type}\"] = w_fused_tensor.view_as(w1).to(w1.dtype)\n",
    "\n",
    "    # Display parameters of the first layer after fusion\n",
    "    print(f\"Layer {layer1_idx} parameters after fusion:\")\n",
    "    for name in layer1_params:\n",
    "        print(f\"{name}: {layer1_params[name].shape}\")\n",
    "\n",
    "    # Remove the second layer from the model's layer list\n",
    "    model.encoder.layer = torch.nn.ModuleList(\n",
    "        [layer for k, layer in enumerate(model.encoder.layer) if k != layer2_idx]\n",
    "    )\n",
    "\n",
    "    print(f\"Model layers after removal of layer {layer2_idx}\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_csv = r\"..\\val_csv\"\n",
    "val_images = r\"..\\val_images\"\n",
    "classA = 'n07920052'\n",
    "classB = 'n01484850'\n",
    "num_images = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centering(K):\n",
    "    n = K.shape[0]\n",
    "    data_type= K.dtype\n",
    "    unit = torch.ones([n, n]).to(device, dtype=data_type)\n",
    "    I = torch.eye(n).to(device, dtype=data_type)\n",
    "    H = I - unit / n\n",
    "\n",
    "    return ((H@ K)@ H)  # HKH are the same with KH, KH is the first centering, H(KH) do the second time, results are the sme with one time centering\n",
    "    # return np.dot(H, K)  # KH\n",
    "\n",
    "\n",
    "def rbf(X, sigma=None):\n",
    "    GX = (X@ X.T).to(device)\n",
    "    KX = torch.diag(GX) - GX + (torch.diag(GX) - GX).T\n",
    "    if sigma is None:\n",
    "        mdist = torch.median(KX[KX != 0])\n",
    "        sigma = torch.sqrt(mdist)\n",
    "    KX *= - 0.5 / (sigma * sigma)\n",
    "    KX = torch.exp(KX).to(device)\n",
    "    return KX\n",
    "\n",
    "\n",
    "def kernel_HSIC(X, Y, sigma):\n",
    "    return torch.sum(centering(rbf(X, sigma)) * centering(rbf(Y, sigma)))\n",
    "\n",
    "\n",
    "def linear_HSIC(X, Y):\n",
    "    L_X = (X@ X.T)\n",
    "    L_Y = (Y@ Y.T)\n",
    "    return torch.sum(centering(L_X) * centering(L_Y))\n",
    "\n",
    "\n",
    "def linear_CKA(X, Y):\n",
    "    hsic = linear_HSIC(X, Y)\n",
    "    var1 = torch.sqrt(linear_HSIC(X, X))\n",
    "    var2 = torch.sqrt(linear_HSIC(Y, Y))\n",
    "\n",
    "    return hsic / (var1 * var2)\n",
    "\n",
    "\n",
    "def kernel_CKA(X, Y, sigma=None):\n",
    "    hsic = kernel_HSIC(X, Y, sigma)\n",
    "    var1 = torch.sqrt(kernel_HSIC(X, X, sigma))\n",
    "    var2 = torch.sqrt(kernel_HSIC(Y, Y, sigma))\n",
    "\n",
    "    return hsic / (var1 * var2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def latent_embeddings_for_class(classA, num_images, model):\n",
    "    cur_csv = os.path.join(val_csv , (classA+ '.csv'))\n",
    "    df = pd.read_csv(cur_csv, header = None)\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(num_images)):\n",
    "        image_name = df.iloc[random.randint(1, 50), 0]\n",
    "        image_path = os.path.join(val_images, image_name)\n",
    "        image =Image.open(image_path)\n",
    "\n",
    "        inputs = tokenizer(image, return_tensors=\"pt\").to(device).to(torch.bfloat16)\n",
    "        \n",
    "        output = model(**inputs)\n",
    "        embeddings.append(output.last_hidden_state[0,1])\n",
    "\n",
    "        del inputs\n",
    "        del image\n",
    "        del output\n",
    "        clear_memory()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    embeddings = torch.stack(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "\n",
    "def eval_meth(classA, classB, num_images, model ):\n",
    "    \n",
    "    X= latent_embeddings_for_class( classA, num_images, model)\n",
    "    Y= latent_embeddings_for_class( classB, num_images, model)\n",
    "    \n",
    "    cka_score = linear_CKA(X,Y)\n",
    "    clear_memory()\n",
    "    return cka_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:16<00:00,  1.50it/s]\n",
      "100%|██████████| 25/25 [00:16<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model metadata:\n",
      "Number of layers: 24\n",
      "Config num_hidden_layers: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 0    tench, Tinca tinca: 100%|██████████| 1/1 [00:15<00:00, 15.81s/it]\n",
      "Processing 1    goldfish, Carassius auratus: 100%|██████████| 1/1 [00:15<00:00, 15.85s/it]\n",
      "Processing 2    great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias: 100%|██████████| 1/1 [00:15<00:00, 15.87s/it]\n",
      "Processing 3    tiger shark, Galeocerdo cuvieri: 100%|██████████| 1/1 [00:15<00:00, 15.91s/it]\n",
      "Processing 4    hammerhead, hammerhead shark: 100%|██████████| 1/1 [00:15<00:00, 15.95s/it]\n",
      "Processing 5    electric ray, crampfish, numbfish, torpedo: 100%|██████████| 1/1 [00:15<00:00, 15.97s/it]\n",
      "Processing 6    stingray: 100%|██████████| 1/1 [00:15<00:00, 15.98s/it]\n",
      "Processing 7    cock: 100%|██████████| 1/1 [00:16<00:00, 16.02s/it]\n",
      "Processing 8    hen: 100%|██████████| 1/1 [00:16<00:00, 16.07s/it]\n",
      "Processing 9    ostrich, Struthio camelus: 100%|██████████| 1/1 [00:16<00:00, 16.10s/it]\n",
      "Processing 10    brambling, Fringilla montifringilla: 100%|██████████| 1/1 [00:16<00:00, 16.08s/it]\n",
      "Processing 11    goldfinch, Carduelis carduelis: 100%|██████████| 1/1 [00:16<00:00, 16.09s/it]\n",
      "Processing 12    house finch, linnet, Carpodacus mexicanus: 100%|██████████| 1/1 [00:16<00:00, 16.11s/it]\n",
      "Processing 13    junco, snowbird: 100%|██████████| 1/1 [00:16<00:00, 16.12s/it]\n",
      "Processing 14    indigo bunting, indigo finch, indigo bird, Passerina cyanea: 100%|██████████| 1/1 [00:16<00:00, 16.12s/it]\n",
      "Processing 15    robin, American robin, Turdus migratorius: 100%|██████████| 1/1 [00:16<00:00, 16.10s/it]\n",
      "Processing 16    bulbul: 100%|██████████| 1/1 [00:16<00:00, 16.13s/it]\n",
      "Processing 17    jay: 100%|██████████| 1/1 [00:16<00:00, 16.12s/it]\n",
      "Processing 18    magpie: 100%|██████████| 1/1 [00:16<00:00, 16.12s/it]\n",
      "Processing 19    chickadee: 100%|██████████| 1/1 [00:16<00:00, 16.12s/it]\n",
      "Processing 20    water ouzel, dipper: 100%|██████████| 1/1 [00:16<00:00, 16.13s/it]\n",
      "Processing 21    kite: 100%|██████████| 1/1 [00:16<00:00, 16.13s/it]\n",
      "Processing 22    bald eagle, American eagle, Haliaeetus leucocephalus: 100%|██████████| 1/1 [00:16<00:00, 16.14s/it]\n",
      "Processing 23    vulture: 100%|██████████| 1/1 [00:16<00:00, 16.16s/it]\n",
      "Processing 24    great grey owl, great gray owl, Strix nebulosa: 100%|██████████| 1/1 [00:16<00:00, 16.18s/it]\n",
      "Processing 25    European fire salamander, Salamandra salamandra: 100%|██████████| 1/1 [00:16<00:00, 16.16s/it]\n",
      "Processing 26    common newt, Triturus vulgaris: 100%|██████████| 1/1 [00:16<00:00, 16.17s/it]\n",
      "Processing 27    eft: 100%|██████████| 1/1 [00:16<00:00, 16.19s/it]\n",
      "Processing 28    spotted salamander, Ambystoma maculatum: 100%|██████████| 1/1 [00:16<00:00, 16.17s/it]\n",
      "Processing 29    axolotl, mud puppy, Ambystoma mexicanum: 100%|██████████| 1/1 [00:16<00:00, 16.19s/it]\n",
      "Processing 30    bullfrog, Rana catesbeiana: 100%|██████████| 1/1 [00:16<00:00, 16.20s/it]\n",
      "Processing 31    tree frog, tree-frog: 100%|██████████| 1/1 [00:16<00:00, 16.20s/it]\n",
      "Processing 32    tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui: 100%|██████████| 1/1 [00:16<00:00, 16.21s/it]\n",
      "Processing 33    loggerhead, loggerhead turtle, Caretta caretta: 100%|██████████| 1/1 [00:16<00:00, 16.20s/it]\n",
      "Processing 34    leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea: 100%|██████████| 1/1 [00:16<00:00, 16.20s/it]\n",
      "Processing 35    mud turtle: 100%|██████████| 1/1 [00:16<00:00, 16.21s/it]\n",
      "Processing 36    terrapin: 100%|██████████| 1/1 [00:16<00:00, 16.22s/it]\n",
      "Processing 37    box turtle, box tortoise: 100%|██████████| 1/1 [00:16<00:00, 16.16s/it]\n",
      "Processing 38    banded gecko: 100%|██████████| 1/1 [00:16<00:00, 16.17s/it]\n",
      "Processing 39    common iguana, iguana, Iguana iguana: 100%|██████████| 1/1 [00:16<00:00, 16.15s/it]\n",
      "Processing 40    American chameleon, anole, Anolis carolinensis: 100%|██████████| 1/1 [00:16<00:00, 16.13s/it]\n",
      "Processing 41    whiptail, whiptail lizard: 100%|██████████| 1/1 [00:16<00:00, 16.15s/it]\n",
      "Processing 42    agama: 100%|██████████| 1/1 [00:16<00:00, 16.13s/it]\n",
      "Processing 43    frilled lizard, Chlamydosaurus kingi: 100%|██████████| 1/1 [00:16<00:00, 16.12s/it]\n",
      "Processing 44    alligator lizard: 100%|██████████| 1/1 [00:16<00:00, 16.13s/it]\n",
      "Processing 45    Gila monster, Heloderma suspectum: 100%|██████████| 1/1 [00:16<00:00, 16.13s/it]\n",
      "Processing 46    green lizard, Lacerta viridis: 100%|██████████| 1/1 [00:16<00:00, 16.12s/it]\n",
      "Processing 47    African chameleon, Chamaeleo chamaeleon: 100%|██████████| 1/1 [00:16<00:00, 16.12s/it]\n",
      "Processing 48    Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis: 100%|██████████| 1/1 [00:16<00:00, 16.12s/it]\n",
      "Processing 49    African crocodile, Nile crocodile, Crocodylus niloticus: 100%|██████████| 1/1 [00:16<00:00, 16.13s/it]\n",
      "Processing 50    American alligator, Alligator mississipiensis: 100%|██████████| 1/1 [00:16<00:00, 16.15s/it]\n",
      "Processing 51    triceratops: 100%|██████████| 1/1 [00:16<00:00, 16.15s/it]\n",
      "Processing 52    thunder snake, worm snake, Carphophis amoenus: 100%|██████████| 1/1 [00:16<00:00, 16.13s/it]\n",
      "Processing 53    ringneck snake, ring-necked snake, ring snake: 100%|██████████| 1/1 [00:16<00:00, 16.14s/it]\n",
      "Processing 54    hognose snake, puff adder, sand viper: 100%|██████████| 1/1 [00:16<00:00, 16.18s/it]\n",
      "Processing 55    green snake, grass snake: 100%|██████████| 1/1 [00:16<00:00, 16.13s/it]\n",
      "Processing 56    king snake, kingsnake: 100%|██████████| 1/1 [00:16<00:00, 16.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting diffusion kernel computation...\n",
      "Condition number: 69.52644348144531\n",
      "Diffusion kernel computation completed in 0.02 seconds.\n",
      "Starting diffusion kernel computation...\n",
      "Condition number: 24.198144912719727\n",
      "Diffusion kernel computation completed in 0.00 seconds.\n",
      "Starting diffusion kernel computation...\n",
      "Condition number: 20.080806732177734\n",
      "Diffusion kernel computation completed in 0.00 seconds.\n",
      "Starting diffusion kernel computation...\n",
      "Condition number: 16.430339813232422\n",
      "Diffusion kernel computation completed in 0.00 seconds.\n",
      "Starting diffusion kernel computation...\n",
      "Condition number: 14.496244430541992\n",
      "Diffusion kernel computation completed in 0.00 seconds.\n",
      "Starting diffusion kernel computation...\n",
      "Condition number: 12.957993507385254\n",
      "Diffusion kernel computation completed in 0.00 seconds.\n",
      "Starting diffusion kernel computation...\n",
      "Condition number: 11.433713912963867\n",
      "Diffusion kernel computation completed in 0.00 seconds.\n",
      "Starting diffusion kernel computation...\n",
      "Condition number: 10.419486999511719\n",
      "Diffusion kernel computation completed in 0.00 seconds.\n",
      "Starting diffusion kernel computation...\n",
      "Condition number: 9.724527359008789\n",
      "Diffusion kernel computation completed in 0.00 seconds.\n",
      "Starting diffusion kernel computation...\n",
      "Condition number: 9.406431198120117\n",
      "Diffusion kernel computation completed in 0.00 seconds.\n",
      "Starting diffusion kernel computation...\n",
      "Condition number: 9.183563232421875\n",
      "Diffusion kernel computation completed in 0.00 seconds.\n",
      "Starting diffusion kernel computation...\n",
      "Condition number: 8.878005981445312\n",
      "Diffusion kernel computation completed in 0.00 seconds.\n",
      "Starting diffusion kernel computation...\n",
      "Condition number: 8.469937324523926\n",
      "Diffusion kernel computation completed in 0.00 seconds.\n",
      "Starting diffusion kernel computation...\n",
      "Condition number: 8.32658863067627\n",
      "Diffusion kernel computation completed in 0.00 seconds.\n",
      "Starting diffusion kernel computation...\n",
      "Condition number: 8.195534706115723\n",
      "Diffusion kernel computation completed in 0.00 seconds.\n",
      "Starting diffusion kernel computation...\n",
      "Condition number: 8.197833061218262\n",
      "Diffusion kernel computation completed in 0.00 seconds.\n",
      "Starting diffusion kernel computation...\n",
      "Condition number: 8.165352821350098\n",
      "Diffusion kernel computation completed in 0.01 seconds.\n",
      "Starting diffusion kernel computation...\n",
      "Condition number: 8.14783000946045\n",
      "Diffusion kernel computation completed in 0.00 seconds.\n",
      "Starting diffusion kernel computation...\n",
      "Condition number: 7.973413467407227\n",
      "Diffusion kernel computation completed in 0.01 seconds.\n",
      "Starting diffusion kernel computation...\n",
      "Condition number: 7.650513172149658\n",
      "Diffusion kernel computation completed in 0.01 seconds.\n",
      "Starting diffusion kernel computation...\n",
      "Condition number: 7.621092319488525\n",
      "Diffusion kernel computation completed in 0.00 seconds.\n",
      "Starting diffusion kernel computation...\n",
      "Condition number: 7.508838653564453\n",
      "Diffusion kernel computation completed in 0.01 seconds.\n",
      "Starting diffusion kernel computation...\n",
      "Condition number: 7.256498336791992\n",
      "Diffusion kernel computation completed in 0.01 seconds.\n",
      "Starting diffusion kernel computation...\n",
      "Condition number: 6.888814926147461\n",
      "Diffusion kernel computation completed in 0.00 seconds.\n",
      "Merging Layer 22 (Fusion Ratio: 0.5991) and Layer 23 (Fusion Ratio: 0.4009)\n",
      "Starting fusion of layers 22 and 23 with ratio 0.5990815244517748\n",
      "Layer 22 parameters before fusion:\n",
      "Layer 23 parameters before fusion:\n",
      "Layer 22 parameters after fusion:\n",
      "Model layers after removal of layer 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:16<00:00,  1.52it/s]\n",
      "100%|██████████| 25/25 [00:16<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging Layer 21 (Fusion Ratio: 0.6235) and Layer 22 (Fusion Ratio: 0.3765)\n",
      "Starting fusion of layers 21 and 22 with ratio 0.6234746830946085\n",
      "Layer 21 parameters before fusion:\n",
      "Layer 22 parameters before fusion:\n",
      "Layer 21 parameters after fusion:\n",
      "Model layers after removal of layer 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:15<00:00,  1.62it/s]\n",
      "100%|██████████| 25/25 [00:15<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging Layer 20 (Fusion Ratio: 0.6231) and Layer 21 (Fusion Ratio: 0.3769)\n",
      "Starting fusion of layers 20 and 21 with ratio 0.6230752804979843\n",
      "Layer 20 parameters before fusion:\n",
      "Layer 21 parameters before fusion:\n",
      "Layer 20 parameters after fusion:\n",
      "Model layers after removal of layer 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:14<00:00,  1.69it/s]\n",
      "100%|██████████| 25/25 [00:14<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging Layer 19 (Fusion Ratio: 0.6223) and Layer 20 (Fusion Ratio: 0.3777)\n",
      "Starting fusion of layers 19 and 20 with ratio 0.6223029057952147\n",
      "Layer 19 parameters before fusion:\n",
      "Layer 20 parameters before fusion:\n",
      "Layer 19 parameters after fusion:\n",
      "Model layers after removal of layer 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:14<00:00,  1.78it/s]\n",
      "100%|██████████| 25/25 [00:14<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging Layer 18 (Fusion Ratio: 0.6209) and Layer 19 (Fusion Ratio: 0.3791)\n",
      "Starting fusion of layers 18 and 19 with ratio 0.6208968212644913\n",
      "Layer 18 parameters before fusion:\n",
      "Layer 19 parameters before fusion:\n",
      "Layer 18 parameters after fusion:\n",
      "Model layers after removal of layer 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:13<00:00,  1.87it/s]\n",
      "100%|██████████| 25/25 [00:13<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging Layer 17 (Fusion Ratio: 0.6384) and Layer 18 (Fusion Ratio: 0.3616)\n",
      "Starting fusion of layers 17 and 18 with ratio 0.6383665609429745\n",
      "Layer 17 parameters before fusion:\n",
      "Layer 18 parameters before fusion:\n",
      "Layer 17 parameters after fusion:\n",
      "Model layers after removal of layer 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:12<00:00,  1.97it/s]\n",
      "100%|██████████| 25/25 [00:12<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging Layer 16 (Fusion Ratio: 0.6225) and Layer 17 (Fusion Ratio: 0.3775)\n",
      "Starting fusion of layers 16 and 17 with ratio 0.6224619810161637\n",
      "Layer 16 parameters before fusion:\n",
      "Layer 17 parameters before fusion:\n",
      "Layer 16 parameters after fusion:\n",
      "Model layers after removal of layer 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:11<00:00,  2.08it/s]\n",
      "100%|██████████| 25/25 [00:12<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging Layer 15 (Fusion Ratio: 0.6231) and Layer 16 (Fusion Ratio: 0.3769)\n",
      "Starting fusion of layers 15 and 16 with ratio 0.6230627387811596\n",
      "Layer 15 parameters before fusion:\n",
      "Layer 16 parameters before fusion:\n",
      "Layer 15 parameters after fusion:\n",
      "Model layers after removal of layer 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:11<00:00,  2.21it/s]\n",
      "100%|██████████| 25/25 [00:11<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging Layer 14 (Fusion Ratio: 0.6220) and Layer 15 (Fusion Ratio: 0.3780)\n",
      "Starting fusion of layers 14 and 15 with ratio 0.6220356870935841\n",
      "Layer 14 parameters before fusion:\n",
      "Layer 15 parameters before fusion:\n",
      "Layer 14 parameters after fusion:\n",
      "Model layers after removal of layer 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:10<00:00,  2.35it/s]\n",
      "100%|██████████| 25/25 [00:10<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging Layer 13 (Fusion Ratio: 0.6225) and Layer 14 (Fusion Ratio: 0.3775)\n",
      "Starting fusion of layers 13 and 14 with ratio 0.6225075741929903\n",
      "Layer 13 parameters before fusion:\n",
      "Layer 14 parameters before fusion:\n",
      "Layer 13 parameters after fusion:\n",
      "Model layers after removal of layer 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:09<00:00,  2.53it/s]\n",
      "100%|██████████| 25/25 [00:10<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging Layer 12 (Fusion Ratio: 0.6196) and Layer 13 (Fusion Ratio: 0.3804)\n",
      "Starting fusion of layers 12 and 13 with ratio 0.6195735572645849\n",
      "Layer 12 parameters before fusion:\n",
      "Layer 13 parameters before fusion:\n",
      "Layer 12 parameters after fusion:\n",
      "Model layers after removal of layer 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:09<00:00,  2.71it/s]\n",
      "100%|██████████| 25/25 [00:09<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging Layer 11 (Fusion Ratio: 0.6207) and Layer 12 (Fusion Ratio: 0.3793)\n",
      "Starting fusion of layers 11 and 12 with ratio 0.6207128212055886\n",
      "Layer 11 parameters before fusion:\n",
      "Layer 12 parameters before fusion:\n",
      "Layer 11 parameters after fusion:\n",
      "Model layers after removal of layer 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:08<00:00,  2.92it/s]\n",
      "100%|██████████| 25/25 [00:08<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model state dict keys and tensor shapes after fusion:\n",
      "embeddings.cls_token: torch.Size([1, 1, 1024])\n",
      "embeddings.position_embeddings: torch.Size([1, 1025, 1024])\n",
      "embeddings.patch_embeddings.projection.weight: torch.Size([1024, 3, 7, 7])\n",
      "embeddings.patch_embeddings.projection.bias: torch.Size([1024])\n",
      "encoder.layer.0.attention.attention.query.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.0.attention.attention.query.bias: torch.Size([1024])\n",
      "encoder.layer.0.attention.attention.key.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.0.attention.attention.key.bias: torch.Size([1024])\n",
      "encoder.layer.0.attention.attention.value.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.0.attention.attention.value.bias: torch.Size([1024])\n",
      "encoder.layer.0.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.0.attention.output.dense.bias: torch.Size([1024])\n",
      "encoder.layer.0.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "encoder.layer.0.intermediate.dense.bias: torch.Size([4096])\n",
      "encoder.layer.0.output.dense.weight: torch.Size([1024, 4096])\n",
      "encoder.layer.0.output.dense.bias: torch.Size([1024])\n",
      "encoder.layer.0.layernorm_before.weight: torch.Size([1024])\n",
      "encoder.layer.0.layernorm_before.bias: torch.Size([1024])\n",
      "encoder.layer.0.layernorm_after.weight: torch.Size([1024])\n",
      "encoder.layer.0.layernorm_after.bias: torch.Size([1024])\n",
      "encoder.layer.1.attention.attention.query.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.1.attention.attention.query.bias: torch.Size([1024])\n",
      "encoder.layer.1.attention.attention.key.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.1.attention.attention.key.bias: torch.Size([1024])\n",
      "encoder.layer.1.attention.attention.value.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.1.attention.attention.value.bias: torch.Size([1024])\n",
      "encoder.layer.1.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.1.attention.output.dense.bias: torch.Size([1024])\n",
      "encoder.layer.1.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "encoder.layer.1.intermediate.dense.bias: torch.Size([4096])\n",
      "encoder.layer.1.output.dense.weight: torch.Size([1024, 4096])\n",
      "encoder.layer.1.output.dense.bias: torch.Size([1024])\n",
      "encoder.layer.1.layernorm_before.weight: torch.Size([1024])\n",
      "encoder.layer.1.layernorm_before.bias: torch.Size([1024])\n",
      "encoder.layer.1.layernorm_after.weight: torch.Size([1024])\n",
      "encoder.layer.1.layernorm_after.bias: torch.Size([1024])\n",
      "encoder.layer.2.attention.attention.query.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.2.attention.attention.query.bias: torch.Size([1024])\n",
      "encoder.layer.2.attention.attention.key.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.2.attention.attention.key.bias: torch.Size([1024])\n",
      "encoder.layer.2.attention.attention.value.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.2.attention.attention.value.bias: torch.Size([1024])\n",
      "encoder.layer.2.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.2.attention.output.dense.bias: torch.Size([1024])\n",
      "encoder.layer.2.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "encoder.layer.2.intermediate.dense.bias: torch.Size([4096])\n",
      "encoder.layer.2.output.dense.weight: torch.Size([1024, 4096])\n",
      "encoder.layer.2.output.dense.bias: torch.Size([1024])\n",
      "encoder.layer.2.layernorm_before.weight: torch.Size([1024])\n",
      "encoder.layer.2.layernorm_before.bias: torch.Size([1024])\n",
      "encoder.layer.2.layernorm_after.weight: torch.Size([1024])\n",
      "encoder.layer.2.layernorm_after.bias: torch.Size([1024])\n",
      "encoder.layer.3.attention.attention.query.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.3.attention.attention.query.bias: torch.Size([1024])\n",
      "encoder.layer.3.attention.attention.key.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.3.attention.attention.key.bias: torch.Size([1024])\n",
      "encoder.layer.3.attention.attention.value.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.3.attention.attention.value.bias: torch.Size([1024])\n",
      "encoder.layer.3.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.3.attention.output.dense.bias: torch.Size([1024])\n",
      "encoder.layer.3.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "encoder.layer.3.intermediate.dense.bias: torch.Size([4096])\n",
      "encoder.layer.3.output.dense.weight: torch.Size([1024, 4096])\n",
      "encoder.layer.3.output.dense.bias: torch.Size([1024])\n",
      "encoder.layer.3.layernorm_before.weight: torch.Size([1024])\n",
      "encoder.layer.3.layernorm_before.bias: torch.Size([1024])\n",
      "encoder.layer.3.layernorm_after.weight: torch.Size([1024])\n",
      "encoder.layer.3.layernorm_after.bias: torch.Size([1024])\n",
      "encoder.layer.4.attention.attention.query.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.4.attention.attention.query.bias: torch.Size([1024])\n",
      "encoder.layer.4.attention.attention.key.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.4.attention.attention.key.bias: torch.Size([1024])\n",
      "encoder.layer.4.attention.attention.value.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.4.attention.attention.value.bias: torch.Size([1024])\n",
      "encoder.layer.4.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.4.attention.output.dense.bias: torch.Size([1024])\n",
      "encoder.layer.4.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "encoder.layer.4.intermediate.dense.bias: torch.Size([4096])\n",
      "encoder.layer.4.output.dense.weight: torch.Size([1024, 4096])\n",
      "encoder.layer.4.output.dense.bias: torch.Size([1024])\n",
      "encoder.layer.4.layernorm_before.weight: torch.Size([1024])\n",
      "encoder.layer.4.layernorm_before.bias: torch.Size([1024])\n",
      "encoder.layer.4.layernorm_after.weight: torch.Size([1024])\n",
      "encoder.layer.4.layernorm_after.bias: torch.Size([1024])\n",
      "encoder.layer.5.attention.attention.query.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.5.attention.attention.query.bias: torch.Size([1024])\n",
      "encoder.layer.5.attention.attention.key.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.5.attention.attention.key.bias: torch.Size([1024])\n",
      "encoder.layer.5.attention.attention.value.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.5.attention.attention.value.bias: torch.Size([1024])\n",
      "encoder.layer.5.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.5.attention.output.dense.bias: torch.Size([1024])\n",
      "encoder.layer.5.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "encoder.layer.5.intermediate.dense.bias: torch.Size([4096])\n",
      "encoder.layer.5.output.dense.weight: torch.Size([1024, 4096])\n",
      "encoder.layer.5.output.dense.bias: torch.Size([1024])\n",
      "encoder.layer.5.layernorm_before.weight: torch.Size([1024])\n",
      "encoder.layer.5.layernorm_before.bias: torch.Size([1024])\n",
      "encoder.layer.5.layernorm_after.weight: torch.Size([1024])\n",
      "encoder.layer.5.layernorm_after.bias: torch.Size([1024])\n",
      "encoder.layer.6.attention.attention.query.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.6.attention.attention.query.bias: torch.Size([1024])\n",
      "encoder.layer.6.attention.attention.key.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.6.attention.attention.key.bias: torch.Size([1024])\n",
      "encoder.layer.6.attention.attention.value.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.6.attention.attention.value.bias: torch.Size([1024])\n",
      "encoder.layer.6.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.6.attention.output.dense.bias: torch.Size([1024])\n",
      "encoder.layer.6.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "encoder.layer.6.intermediate.dense.bias: torch.Size([4096])\n",
      "encoder.layer.6.output.dense.weight: torch.Size([1024, 4096])\n",
      "encoder.layer.6.output.dense.bias: torch.Size([1024])\n",
      "encoder.layer.6.layernorm_before.weight: torch.Size([1024])\n",
      "encoder.layer.6.layernorm_before.bias: torch.Size([1024])\n",
      "encoder.layer.6.layernorm_after.weight: torch.Size([1024])\n",
      "encoder.layer.6.layernorm_after.bias: torch.Size([1024])\n",
      "encoder.layer.7.attention.attention.query.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.7.attention.attention.query.bias: torch.Size([1024])\n",
      "encoder.layer.7.attention.attention.key.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.7.attention.attention.key.bias: torch.Size([1024])\n",
      "encoder.layer.7.attention.attention.value.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.7.attention.attention.value.bias: torch.Size([1024])\n",
      "encoder.layer.7.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.7.attention.output.dense.bias: torch.Size([1024])\n",
      "encoder.layer.7.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "encoder.layer.7.intermediate.dense.bias: torch.Size([4096])\n",
      "encoder.layer.7.output.dense.weight: torch.Size([1024, 4096])\n",
      "encoder.layer.7.output.dense.bias: torch.Size([1024])\n",
      "encoder.layer.7.layernorm_before.weight: torch.Size([1024])\n",
      "encoder.layer.7.layernorm_before.bias: torch.Size([1024])\n",
      "encoder.layer.7.layernorm_after.weight: torch.Size([1024])\n",
      "encoder.layer.7.layernorm_after.bias: torch.Size([1024])\n",
      "encoder.layer.8.attention.attention.query.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.8.attention.attention.query.bias: torch.Size([1024])\n",
      "encoder.layer.8.attention.attention.key.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.8.attention.attention.key.bias: torch.Size([1024])\n",
      "encoder.layer.8.attention.attention.value.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.8.attention.attention.value.bias: torch.Size([1024])\n",
      "encoder.layer.8.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.8.attention.output.dense.bias: torch.Size([1024])\n",
      "encoder.layer.8.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "encoder.layer.8.intermediate.dense.bias: torch.Size([4096])\n",
      "encoder.layer.8.output.dense.weight: torch.Size([1024, 4096])\n",
      "encoder.layer.8.output.dense.bias: torch.Size([1024])\n",
      "encoder.layer.8.layernorm_before.weight: torch.Size([1024])\n",
      "encoder.layer.8.layernorm_before.bias: torch.Size([1024])\n",
      "encoder.layer.8.layernorm_after.weight: torch.Size([1024])\n",
      "encoder.layer.8.layernorm_after.bias: torch.Size([1024])\n",
      "encoder.layer.9.attention.attention.query.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.9.attention.attention.query.bias: torch.Size([1024])\n",
      "encoder.layer.9.attention.attention.key.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.9.attention.attention.key.bias: torch.Size([1024])\n",
      "encoder.layer.9.attention.attention.value.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.9.attention.attention.value.bias: torch.Size([1024])\n",
      "encoder.layer.9.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.9.attention.output.dense.bias: torch.Size([1024])\n",
      "encoder.layer.9.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "encoder.layer.9.intermediate.dense.bias: torch.Size([4096])\n",
      "encoder.layer.9.output.dense.weight: torch.Size([1024, 4096])\n",
      "encoder.layer.9.output.dense.bias: torch.Size([1024])\n",
      "encoder.layer.9.layernorm_before.weight: torch.Size([1024])\n",
      "encoder.layer.9.layernorm_before.bias: torch.Size([1024])\n",
      "encoder.layer.9.layernorm_after.weight: torch.Size([1024])\n",
      "encoder.layer.9.layernorm_after.bias: torch.Size([1024])\n",
      "encoder.layer.10.attention.attention.query.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.10.attention.attention.query.bias: torch.Size([1024])\n",
      "encoder.layer.10.attention.attention.key.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.10.attention.attention.key.bias: torch.Size([1024])\n",
      "encoder.layer.10.attention.attention.value.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.10.attention.attention.value.bias: torch.Size([1024])\n",
      "encoder.layer.10.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.10.attention.output.dense.bias: torch.Size([1024])\n",
      "encoder.layer.10.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "encoder.layer.10.intermediate.dense.bias: torch.Size([4096])\n",
      "encoder.layer.10.output.dense.weight: torch.Size([1024, 4096])\n",
      "encoder.layer.10.output.dense.bias: torch.Size([1024])\n",
      "encoder.layer.10.layernorm_before.weight: torch.Size([1024])\n",
      "encoder.layer.10.layernorm_before.bias: torch.Size([1024])\n",
      "encoder.layer.10.layernorm_after.weight: torch.Size([1024])\n",
      "encoder.layer.10.layernorm_after.bias: torch.Size([1024])\n",
      "encoder.layer.11.attention.attention.query.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.11.attention.attention.query.bias: torch.Size([1024])\n",
      "encoder.layer.11.attention.attention.key.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.11.attention.attention.key.bias: torch.Size([1024])\n",
      "encoder.layer.11.attention.attention.value.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.11.attention.attention.value.bias: torch.Size([1024])\n",
      "encoder.layer.11.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "encoder.layer.11.attention.output.dense.bias: torch.Size([1024])\n",
      "encoder.layer.11.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "encoder.layer.11.intermediate.dense.bias: torch.Size([4096])\n",
      "encoder.layer.11.output.dense.weight: torch.Size([1024, 4096])\n",
      "encoder.layer.11.output.dense.bias: torch.Size([1024])\n",
      "encoder.layer.11.layernorm_before.weight: torch.Size([1024])\n",
      "encoder.layer.11.layernorm_before.bias: torch.Size([1024])\n",
      "encoder.layer.11.layernorm_after.weight: torch.Size([1024])\n",
      "encoder.layer.11.layernorm_after.bias: torch.Size([1024])\n",
      "layernorm.weight: torch.Size([1024])\n",
      "layernorm.bias: torch.Size([1024])\n",
      "\n",
      "Checking tensor data types in state dict:\n",
      "embeddings.cls_token: torch.bfloat16\n",
      "embeddings.position_embeddings: torch.bfloat16\n",
      "embeddings.patch_embeddings.projection.weight: torch.bfloat16\n",
      "embeddings.patch_embeddings.projection.bias: torch.bfloat16\n",
      "encoder.layer.0.attention.attention.query.weight: torch.bfloat16\n",
      "encoder.layer.0.attention.attention.query.bias: torch.bfloat16\n",
      "encoder.layer.0.attention.attention.key.weight: torch.bfloat16\n",
      "encoder.layer.0.attention.attention.key.bias: torch.bfloat16\n",
      "encoder.layer.0.attention.attention.value.weight: torch.bfloat16\n",
      "encoder.layer.0.attention.attention.value.bias: torch.bfloat16\n",
      "encoder.layer.0.attention.output.dense.weight: torch.bfloat16\n",
      "encoder.layer.0.attention.output.dense.bias: torch.bfloat16\n",
      "encoder.layer.0.intermediate.dense.weight: torch.bfloat16\n",
      "encoder.layer.0.intermediate.dense.bias: torch.bfloat16\n",
      "encoder.layer.0.output.dense.weight: torch.bfloat16\n",
      "encoder.layer.0.output.dense.bias: torch.bfloat16\n",
      "encoder.layer.0.layernorm_before.weight: torch.bfloat16\n",
      "encoder.layer.0.layernorm_before.bias: torch.bfloat16\n",
      "encoder.layer.0.layernorm_after.weight: torch.bfloat16\n",
      "encoder.layer.0.layernorm_after.bias: torch.bfloat16\n",
      "encoder.layer.1.attention.attention.query.weight: torch.bfloat16\n",
      "encoder.layer.1.attention.attention.query.bias: torch.bfloat16\n",
      "encoder.layer.1.attention.attention.key.weight: torch.bfloat16\n",
      "encoder.layer.1.attention.attention.key.bias: torch.bfloat16\n",
      "encoder.layer.1.attention.attention.value.weight: torch.bfloat16\n",
      "encoder.layer.1.attention.attention.value.bias: torch.bfloat16\n",
      "encoder.layer.1.attention.output.dense.weight: torch.bfloat16\n",
      "encoder.layer.1.attention.output.dense.bias: torch.bfloat16\n",
      "encoder.layer.1.intermediate.dense.weight: torch.bfloat16\n",
      "encoder.layer.1.intermediate.dense.bias: torch.bfloat16\n",
      "encoder.layer.1.output.dense.weight: torch.bfloat16\n",
      "encoder.layer.1.output.dense.bias: torch.bfloat16\n",
      "encoder.layer.1.layernorm_before.weight: torch.bfloat16\n",
      "encoder.layer.1.layernorm_before.bias: torch.bfloat16\n",
      "encoder.layer.1.layernorm_after.weight: torch.bfloat16\n",
      "encoder.layer.1.layernorm_after.bias: torch.bfloat16\n",
      "encoder.layer.2.attention.attention.query.weight: torch.bfloat16\n",
      "encoder.layer.2.attention.attention.query.bias: torch.bfloat16\n",
      "encoder.layer.2.attention.attention.key.weight: torch.bfloat16\n",
      "encoder.layer.2.attention.attention.key.bias: torch.bfloat16\n",
      "encoder.layer.2.attention.attention.value.weight: torch.bfloat16\n",
      "encoder.layer.2.attention.attention.value.bias: torch.bfloat16\n",
      "encoder.layer.2.attention.output.dense.weight: torch.bfloat16\n",
      "encoder.layer.2.attention.output.dense.bias: torch.bfloat16\n",
      "encoder.layer.2.intermediate.dense.weight: torch.bfloat16\n",
      "encoder.layer.2.intermediate.dense.bias: torch.bfloat16\n",
      "encoder.layer.2.output.dense.weight: torch.bfloat16\n",
      "encoder.layer.2.output.dense.bias: torch.bfloat16\n",
      "encoder.layer.2.layernorm_before.weight: torch.bfloat16\n",
      "encoder.layer.2.layernorm_before.bias: torch.bfloat16\n",
      "encoder.layer.2.layernorm_after.weight: torch.bfloat16\n",
      "encoder.layer.2.layernorm_after.bias: torch.bfloat16\n",
      "encoder.layer.3.attention.attention.query.weight: torch.bfloat16\n",
      "encoder.layer.3.attention.attention.query.bias: torch.bfloat16\n",
      "encoder.layer.3.attention.attention.key.weight: torch.bfloat16\n",
      "encoder.layer.3.attention.attention.key.bias: torch.bfloat16\n",
      "encoder.layer.3.attention.attention.value.weight: torch.bfloat16\n",
      "encoder.layer.3.attention.attention.value.bias: torch.bfloat16\n",
      "encoder.layer.3.attention.output.dense.weight: torch.bfloat16\n",
      "encoder.layer.3.attention.output.dense.bias: torch.bfloat16\n",
      "encoder.layer.3.intermediate.dense.weight: torch.bfloat16\n",
      "encoder.layer.3.intermediate.dense.bias: torch.bfloat16\n",
      "encoder.layer.3.output.dense.weight: torch.bfloat16\n",
      "encoder.layer.3.output.dense.bias: torch.bfloat16\n",
      "encoder.layer.3.layernorm_before.weight: torch.bfloat16\n",
      "encoder.layer.3.layernorm_before.bias: torch.bfloat16\n",
      "encoder.layer.3.layernorm_after.weight: torch.bfloat16\n",
      "encoder.layer.3.layernorm_after.bias: torch.bfloat16\n",
      "encoder.layer.4.attention.attention.query.weight: torch.bfloat16\n",
      "encoder.layer.4.attention.attention.query.bias: torch.bfloat16\n",
      "encoder.layer.4.attention.attention.key.weight: torch.bfloat16\n",
      "encoder.layer.4.attention.attention.key.bias: torch.bfloat16\n",
      "encoder.layer.4.attention.attention.value.weight: torch.bfloat16\n",
      "encoder.layer.4.attention.attention.value.bias: torch.bfloat16\n",
      "encoder.layer.4.attention.output.dense.weight: torch.bfloat16\n",
      "encoder.layer.4.attention.output.dense.bias: torch.bfloat16\n",
      "encoder.layer.4.intermediate.dense.weight: torch.bfloat16\n",
      "encoder.layer.4.intermediate.dense.bias: torch.bfloat16\n",
      "encoder.layer.4.output.dense.weight: torch.bfloat16\n",
      "encoder.layer.4.output.dense.bias: torch.bfloat16\n",
      "encoder.layer.4.layernorm_before.weight: torch.bfloat16\n",
      "encoder.layer.4.layernorm_before.bias: torch.bfloat16\n",
      "encoder.layer.4.layernorm_after.weight: torch.bfloat16\n",
      "encoder.layer.4.layernorm_after.bias: torch.bfloat16\n",
      "encoder.layer.5.attention.attention.query.weight: torch.bfloat16\n",
      "encoder.layer.5.attention.attention.query.bias: torch.bfloat16\n",
      "encoder.layer.5.attention.attention.key.weight: torch.bfloat16\n",
      "encoder.layer.5.attention.attention.key.bias: torch.bfloat16\n",
      "encoder.layer.5.attention.attention.value.weight: torch.bfloat16\n",
      "encoder.layer.5.attention.attention.value.bias: torch.bfloat16\n",
      "encoder.layer.5.attention.output.dense.weight: torch.bfloat16\n",
      "encoder.layer.5.attention.output.dense.bias: torch.bfloat16\n",
      "encoder.layer.5.intermediate.dense.weight: torch.bfloat16\n",
      "encoder.layer.5.intermediate.dense.bias: torch.bfloat16\n",
      "encoder.layer.5.output.dense.weight: torch.bfloat16\n",
      "encoder.layer.5.output.dense.bias: torch.bfloat16\n",
      "encoder.layer.5.layernorm_before.weight: torch.bfloat16\n",
      "encoder.layer.5.layernorm_before.bias: torch.bfloat16\n",
      "encoder.layer.5.layernorm_after.weight: torch.bfloat16\n",
      "encoder.layer.5.layernorm_after.bias: torch.bfloat16\n",
      "encoder.layer.6.attention.attention.query.weight: torch.bfloat16\n",
      "encoder.layer.6.attention.attention.query.bias: torch.bfloat16\n",
      "encoder.layer.6.attention.attention.key.weight: torch.bfloat16\n",
      "encoder.layer.6.attention.attention.key.bias: torch.bfloat16\n",
      "encoder.layer.6.attention.attention.value.weight: torch.bfloat16\n",
      "encoder.layer.6.attention.attention.value.bias: torch.bfloat16\n",
      "encoder.layer.6.attention.output.dense.weight: torch.bfloat16\n",
      "encoder.layer.6.attention.output.dense.bias: torch.bfloat16\n",
      "encoder.layer.6.intermediate.dense.weight: torch.bfloat16\n",
      "encoder.layer.6.intermediate.dense.bias: torch.bfloat16\n",
      "encoder.layer.6.output.dense.weight: torch.bfloat16\n",
      "encoder.layer.6.output.dense.bias: torch.bfloat16\n",
      "encoder.layer.6.layernorm_before.weight: torch.bfloat16\n",
      "encoder.layer.6.layernorm_before.bias: torch.bfloat16\n",
      "encoder.layer.6.layernorm_after.weight: torch.bfloat16\n",
      "encoder.layer.6.layernorm_after.bias: torch.bfloat16\n",
      "encoder.layer.7.attention.attention.query.weight: torch.bfloat16\n",
      "encoder.layer.7.attention.attention.query.bias: torch.bfloat16\n",
      "encoder.layer.7.attention.attention.key.weight: torch.bfloat16\n",
      "encoder.layer.7.attention.attention.key.bias: torch.bfloat16\n",
      "encoder.layer.7.attention.attention.value.weight: torch.bfloat16\n",
      "encoder.layer.7.attention.attention.value.bias: torch.bfloat16\n",
      "encoder.layer.7.attention.output.dense.weight: torch.bfloat16\n",
      "encoder.layer.7.attention.output.dense.bias: torch.bfloat16\n",
      "encoder.layer.7.intermediate.dense.weight: torch.bfloat16\n",
      "encoder.layer.7.intermediate.dense.bias: torch.bfloat16\n",
      "encoder.layer.7.output.dense.weight: torch.bfloat16\n",
      "encoder.layer.7.output.dense.bias: torch.bfloat16\n",
      "encoder.layer.7.layernorm_before.weight: torch.bfloat16\n",
      "encoder.layer.7.layernorm_before.bias: torch.bfloat16\n",
      "encoder.layer.7.layernorm_after.weight: torch.bfloat16\n",
      "encoder.layer.7.layernorm_after.bias: torch.bfloat16\n",
      "encoder.layer.8.attention.attention.query.weight: torch.bfloat16\n",
      "encoder.layer.8.attention.attention.query.bias: torch.bfloat16\n",
      "encoder.layer.8.attention.attention.key.weight: torch.bfloat16\n",
      "encoder.layer.8.attention.attention.key.bias: torch.bfloat16\n",
      "encoder.layer.8.attention.attention.value.weight: torch.bfloat16\n",
      "encoder.layer.8.attention.attention.value.bias: torch.bfloat16\n",
      "encoder.layer.8.attention.output.dense.weight: torch.bfloat16\n",
      "encoder.layer.8.attention.output.dense.bias: torch.bfloat16\n",
      "encoder.layer.8.intermediate.dense.weight: torch.bfloat16\n",
      "encoder.layer.8.intermediate.dense.bias: torch.bfloat16\n",
      "encoder.layer.8.output.dense.weight: torch.bfloat16\n",
      "encoder.layer.8.output.dense.bias: torch.bfloat16\n",
      "encoder.layer.8.layernorm_before.weight: torch.bfloat16\n",
      "encoder.layer.8.layernorm_before.bias: torch.bfloat16\n",
      "encoder.layer.8.layernorm_after.weight: torch.bfloat16\n",
      "encoder.layer.8.layernorm_after.bias: torch.bfloat16\n",
      "encoder.layer.9.attention.attention.query.weight: torch.bfloat16\n",
      "encoder.layer.9.attention.attention.query.bias: torch.bfloat16\n",
      "encoder.layer.9.attention.attention.key.weight: torch.bfloat16\n",
      "encoder.layer.9.attention.attention.key.bias: torch.bfloat16\n",
      "encoder.layer.9.attention.attention.value.weight: torch.bfloat16\n",
      "encoder.layer.9.attention.attention.value.bias: torch.bfloat16\n",
      "encoder.layer.9.attention.output.dense.weight: torch.bfloat16\n",
      "encoder.layer.9.attention.output.dense.bias: torch.bfloat16\n",
      "encoder.layer.9.intermediate.dense.weight: torch.bfloat16\n",
      "encoder.layer.9.intermediate.dense.bias: torch.bfloat16\n",
      "encoder.layer.9.output.dense.weight: torch.bfloat16\n",
      "encoder.layer.9.output.dense.bias: torch.bfloat16\n",
      "encoder.layer.9.layernorm_before.weight: torch.bfloat16\n",
      "encoder.layer.9.layernorm_before.bias: torch.bfloat16\n",
      "encoder.layer.9.layernorm_after.weight: torch.bfloat16\n",
      "encoder.layer.9.layernorm_after.bias: torch.bfloat16\n",
      "encoder.layer.10.attention.attention.query.weight: torch.bfloat16\n",
      "encoder.layer.10.attention.attention.query.bias: torch.bfloat16\n",
      "encoder.layer.10.attention.attention.key.weight: torch.bfloat16\n",
      "encoder.layer.10.attention.attention.key.bias: torch.bfloat16\n",
      "encoder.layer.10.attention.attention.value.weight: torch.bfloat16\n",
      "encoder.layer.10.attention.attention.value.bias: torch.bfloat16\n",
      "encoder.layer.10.attention.output.dense.weight: torch.bfloat16\n",
      "encoder.layer.10.attention.output.dense.bias: torch.bfloat16\n",
      "encoder.layer.10.intermediate.dense.weight: torch.bfloat16\n",
      "encoder.layer.10.intermediate.dense.bias: torch.bfloat16\n",
      "encoder.layer.10.output.dense.weight: torch.bfloat16\n",
      "encoder.layer.10.output.dense.bias: torch.bfloat16\n",
      "encoder.layer.10.layernorm_before.weight: torch.bfloat16\n",
      "encoder.layer.10.layernorm_before.bias: torch.bfloat16\n",
      "encoder.layer.10.layernorm_after.weight: torch.bfloat16\n",
      "encoder.layer.10.layernorm_after.bias: torch.bfloat16\n",
      "encoder.layer.11.attention.attention.query.weight: torch.bfloat16\n",
      "encoder.layer.11.attention.attention.query.bias: torch.bfloat16\n",
      "encoder.layer.11.attention.attention.key.weight: torch.bfloat16\n",
      "encoder.layer.11.attention.attention.key.bias: torch.bfloat16\n",
      "encoder.layer.11.attention.attention.value.weight: torch.bfloat16\n",
      "encoder.layer.11.attention.attention.value.bias: torch.bfloat16\n",
      "encoder.layer.11.attention.output.dense.weight: torch.bfloat16\n",
      "encoder.layer.11.attention.output.dense.bias: torch.bfloat16\n",
      "encoder.layer.11.intermediate.dense.weight: torch.bfloat16\n",
      "encoder.layer.11.intermediate.dense.bias: torch.bfloat16\n",
      "encoder.layer.11.output.dense.weight: torch.bfloat16\n",
      "encoder.layer.11.output.dense.bias: torch.bfloat16\n",
      "encoder.layer.11.layernorm_before.weight: torch.bfloat16\n",
      "encoder.layer.11.layernorm_before.bias: torch.bfloat16\n",
      "encoder.layer.11.layernorm_after.weight: torch.bfloat16\n",
      "encoder.layer.11.layernorm_after.bias: torch.bfloat16\n",
      "layernorm.weight: torch.bfloat16\n",
      "layernorm.bias: torch.bfloat16\n",
      "Model successfully saved to /visual_data/yangzhao/point/EMNLP2024/layer_fus/al/vit-msn-large-7/fused_12_layers\\iteration\\merged_weights\\pytorch_model.bin.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    The main function that orchestrates the entire process: parsing arguments, loading the model,\n",
    "    processing data, computing embeddings and similarities, fusing layers, and saving the modified model.\n",
    "    \"\"\"\n",
    "    if '--f' in sys.argv:\n",
    "        sys.argv.remove('--f')\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Define command-line arguments with descriptions and default values\n",
    "    parser.add_argument(\"--ntrain\", \"-k\", type=int, default=5, help=\"Number of training examples to include in prompts\")\n",
    "    parser.add_argument(\"--ngpu\", \"-g\", type=int, default=4, help=\"Number of GPUs to use\")\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"facebook/vit-msn-large-7\", help=\"Path to the pre-trained model\")\n",
    "    parser.add_argument(\"--num_tasks\", \"-n\", type=int, default=57, help=\"Number of MMLU tasks to process (default: 57)\")\n",
    "    parser.add_argument(\"--num_samples\", \"-m\", type=int, default=1, help=\"Number of samples per task (default: 1)\")\n",
    "    parser.add_argument(\"--data_dir\", \"-d\", type=str, default=r\"C:\\Users\\hp\\Desktop\\mka on VIT-mae\\Pruning-via-Merging\\val_csv\", help=\"Directory containing the data\")\n",
    "    parser.add_argument(\"--num_layer\", \"-i\", type=int, default=12, help=\"Number of layers to fuse (default: 1)\")\n",
    "    args = parser.parse_args([])\n",
    "\n",
    "    # Extract the model name from the provided model path\n",
    "    model_name = args.model_path.split(\"/\")[-1]\n",
    "    # Define the base directory for storing fused model information\n",
    "    base_dir = f\"/visual_data/yangzhao/point/EMNLP2024/layer_fus/al/{model_name}/fused_{args.num_layer}_layers\"\n",
    "\n",
    "    # Define directories for embeddings, fusion info, and merged weights\n",
    "    iteration_dir = os.path.join(base_dir, f\"iteration\")\n",
    "    embeddings_dir = os.path.join(iteration_dir, \"embeddings\")\n",
    "    fusion_info_dir = os.path.join(iteration_dir, \"fusion_info\")\n",
    "    merged_weights_dir = os.path.join(iteration_dir, \"merged_weights\")\n",
    "\n",
    "    # Create the necessary directories if they don't exist\n",
    "    os.makedirs(embeddings_dir, exist_ok=True)\n",
    "    os.makedirs(fusion_info_dir, exist_ok=True)\n",
    "    os.makedirs(merged_weights_dir, exist_ok=True)\n",
    "\n",
    "    # Configure logging to write logs to a file within fusion_info_dir\n",
    "    logging.basicConfig(filename=os.path.join(fusion_info_dir, 'experiment.log'), level=logging.INFO)\n",
    "    # Set random seeds for reproducibility\n",
    "    set_seed(1)\n",
    "\n",
    "    # Initialize the tokenizer from the pre-trained model\n",
    "    global tokenizer  # Declare as global to use in other functions if needed\n",
    "    global cka_score_list\n",
    "    cka_score_list = []\n",
    "\n",
    "    tokenizer = AutoImageProcessor.from_pretrained(\n",
    "        args.model_path,\n",
    "        use_fast=True,             # Use the fast tokenizer implementation\n",
    "        trust_remote_code=True,    # Trust remote code (required for some models)\n",
    "        add_bos_token=False,       # Do not add beginning-of-sequence token\n",
    "        add_eos_token=False,       # Do not add end-of-sequence token\n",
    "        padding_side=\"left\"        # Pad sequences on the left side\n",
    "    )\n",
    "\n",
    "    # Load the pre-trained causal language model with appropriate settings\n",
    "    model = AutoModel.from_pretrained(\n",
    "        args.model_path,\n",
    "        trust_remote_code=True,    # Trust remote code (required for some models)\n",
    "        device_map=\"auto\",         # Automatically map layers to available devices\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32,  # Use bfloat16 if supported\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    cka_score_list.append(eval_meth(classA, classB, num_images, model).item())\n",
    "\n",
    "        #The model is set in evaluation mode by default using model.eval()\n",
    "   # print(f\"Initial model configuration: {model.config}\")  # Display the model's configuration\n",
    "\n",
    "    # Define the types of weights to be fused between layers\n",
    "    # weight_types = [\n",
    "    #     \"mlp.down_proj.weight\",\n",
    "    #     \"mlp.up_proj.weight\", \n",
    "    #     \"mlp.gate_proj.weight\",\n",
    "    #     \"self_attn.k_proj.weight\",\n",
    "    #     \"self_attn.o_proj.weight\",\n",
    "    #     \"self_attn.q_proj.weight\",\n",
    "    #     \"self_attn.v_proj.weight\",\n",
    "    # ]\n",
    "    weight_types = [\n",
    "    \"attention.attention.query.weight\",\n",
    "    \"attention.attention.key.weight\",\n",
    "    \"attention.attention.value.weight\",\n",
    "    \"attention.output.dense.weight\",\n",
    "    \"intermediate.dense.weight\",\n",
    "    \"output.dense.weight\"\n",
    "                    ]\n",
    "\n",
    "    # Display metadata about the model\n",
    "    print(\"Model metadata:\")\n",
    "    print(f\"Number of layers: {len(model.encoder.layer)}\")\n",
    "    print(f\"Config num_hidden_layers: {model.config.num_hidden_layers}\")\n",
    "\n",
    "    # Identify all classes by listing test files and extracting subject names\n",
    "    classes = list(IMAGENET2012_CLASSES.items())  #data\n",
    "    \n",
    "\n",
    "    num_layers = model.config.num_hidden_layers  # Total number of hidden layers in the model\n",
    "    # Initialize a dictionary to store activations for each layer\n",
    "    all_layers_activations = {i: [] for i in range(num_layers)}\n",
    "\n",
    "    # Iterate over each subject up to the specified number of tasks\n",
    "    for cur_count, (current_class,val) in enumerate(classes[:args.num_tasks]):\n",
    "        # Load the test set for the current subject\n",
    "\n",
    "        test_df = pd.read_csv(os.path.join(args.data_dir, current_class + \".csv\"), header=None)  #data\n",
    "        test_df = test_df[1:]\n",
    "        # Determine the number of samples to process for the current subject\n",
    "        num_samples = min(args.num_samples, test_df.shape[0])\n",
    "        # Randomly select sample indices from the test set\n",
    "        sample_indices = random.sample(range(test_df.shape[0]), num_samples)\n",
    "\n",
    "        # Iterate over each selected sample index with a progress bar\n",
    "        for index in tqdm(sample_indices, desc=f\"Processing {cur_count}    {val}\"):\n",
    "            # Format the test example without the answer\n",
    "            prompt = format_example(test_df, index)                #data format\n",
    "            # Tokenize the prompt and move to GPU\n",
    "            input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "            input_ids = input_ids.to(torch.bfloat16)\n",
    "            # Iterate over each layer to extract activations\n",
    "            for layer_idx in range(num_layers):\n",
    "                activations = extract_layer_params(model, layer_idx, input_ids)\n",
    "                # Append the extracted activations to the corresponding layer's list\n",
    "                all_layers_activations[layer_idx].append(activations)\n",
    "\n",
    "            # Clear memory after processing each sample to free up resources\n",
    "            clear_memory()\n",
    "\n",
    "    # Apply manifold learning (diffusion kernel) to the stacked activations of each layer\n",
    "    for layer_idx in range(num_layers):\n",
    "        # Stack all activations for the current layer vertically\n",
    "        stacked_activations = np.vstack(all_layers_activations[layer_idx])\n",
    "        # Compute the embedded activations using the diffusion kernel\n",
    "        embedded_activations = diffusionKernel(stacked_activations, sigmaK=8, alpha=0.5, d=2, total_size=stacked_activations.shape[0])\n",
    "\n",
    "        # Define the output file path for the embedded activations\n",
    "        output_file = os.path.join(embeddings_dir, f\"layer_{layer_idx}_embedded.pkl\")\n",
    "        # Save the embedded activations to a pickle file\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            pickle.dump(embedded_activations, f)\n",
    "\n",
    "    # Load all precomputed embeddings from the embeddings directory\n",
    "    embeddings = load_embeddings(embeddings_dir)\n",
    "\n",
    "    # Compute the similarity matrix based on the loaded embeddings\n",
    "    similarity_matrix = compute_similarity_matrix_npib_global(embeddings)\n",
    "\n",
    "    #sort the layers here -shivam\n",
    "\n",
    "    # Merge layers iteratively from the last layer towards the first\n",
    "    for _ in range(args.num_layer):\n",
    "        if num_layers <= 1:\n",
    "            break  # Stop if there is only one layer left\n",
    "\n",
    "        # Define the indices of the two layers to fuse (last two layers)\n",
    "        layer1_idx = num_layers - 2\n",
    "        layer2_idx = num_layers - 1\n",
    "\n",
    "        # Compute fusion ratios for the current pair of layers based on similarity\n",
    "        fusion_ratios = compute_fusion_ratios(similarity_matrix, [(layer1_idx, layer2_idx)])\n",
    "        adjusted_ratio_i, adjusted_ratio_j = fusion_ratios[0]\n",
    "\n",
    "        print(f\"Merging Layer {layer1_idx} (Fusion Ratio: {adjusted_ratio_i:.4f}) and Layer {layer2_idx} (Fusion Ratio: {adjusted_ratio_j:.4f})\")\n",
    "\n",
    "        # Perform the actual layer fusion using the computed ratios\n",
    "        merged_model = layer_fusion(model, layer1_idx, layer2_idx, adjusted_ratio_i, weight_types)\n",
    "        model = merged_model  # Update the model with the fused layers\n",
    "\n",
    "        num_layers -= 1  # Decrement the layer count as one layer has been merged\n",
    "        cka_score_list.append(eval_meth(classA, classB, num_images, model).item())\n",
    "\n",
    "        #eval code\n",
    "\n",
    "    # Log the completion of layer fusion\n",
    "    logging.info(f\"Completed layer fusion with {args.num_layer} layers.\")\n",
    "\n",
    "    # Update the model's configuration to reflect the new number of hidden layers\n",
    "    model.config.num_hidden_layers = num_layers\n",
    "    # Save the model's configuration to the merged_weights directory\n",
    "    model.config.save_pretrained(merged_weights_dir)\n",
    "\n",
    "    # Save the fused model's state dictionary\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    # Display the keys and tensor shapes from the state dictionary for verification\n",
    "    print(\"Model state dict keys and tensor shapes after fusion:\")\n",
    "    for key, tensor in state_dict.items():\n",
    "        print(f\"{key}: {tensor.size()}\")\n",
    "\n",
    "    # Additionally, check and display tensor data types in the state dictionary\n",
    "    print(\"\\nChecking tensor data types in state dict:\")\n",
    "    for key, tensor in state_dict.items():\n",
    "        print(f\"{key}: {tensor.dtype}\")\n",
    "\n",
    "    # Define the save path for the merged model's state dictionary\n",
    "    save_path = os.path.join(merged_weights_dir, \"pytorch_model.bin\")\n",
    "    # Save the state dictionary to the specified path using PyTorch's save function\n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f\"Model successfully saved to {save_path}.\")\n",
    "\n",
    "    # Optional: Print example tensor values from the state dictionary for small tensors\n",
    "    # This helps in verifying the actual data without overwhelming the output\n",
    "    # print(\"\\nExample tensor values from state dict (limited to small tensors for readability):\")\n",
    "    # for key, tensor in state_dict.items():\n",
    "    #     if tensor.numel() < 10:  # Only print tensors with fewer than 10 elements\n",
    "    #         print(f\"{key}: {tensor.tolist()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#vit_msn_base\n",
    "#cka\n",
    "plt.plot(cka_score_list, color='green', linestyle=':', marker='o')  # color, dotted line, and dots for points\n",
    "\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mka_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
