{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIL version: 11.0.0\n",
      "Torch version: 2.5.1+cu124\n",
      "Transformers version: 4.48.0\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "import torch\n",
    "import transformers\n",
    "from PIL import Image\n",
    "import requests\n",
    "from torchsummary import summary\n",
    "print(f\"PIL version: {PIL.__version__}\")\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoImageProcessor, AutoModelForPreTraining, AutoModel\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/vit-mae-base\" , use_fast=True)\n",
    "model = AutoModel.from_pretrained(\"facebook/vit-mae-base\")\n",
    "model.eval()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.vit_mae.modeling_vit_mae.ViTMAEModel'>\n",
      "ViTMAEConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"facebook/vit-mae-base\",\n",
      "  \"architectures\": [\n",
      "    \"ViTMAEForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"decoder_hidden_size\": 512,\n",
      "  \"decoder_intermediate_size\": 2048,\n",
      "  \"decoder_num_attention_heads\": 16,\n",
      "  \"decoder_num_hidden_layers\": 8,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_ratio\": 0.75,\n",
      "  \"model_type\": \"vit_mae\",\n",
      "  \"norm_pix_loss\": false,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.48.0\"\n",
      "}\n",
      "\n",
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_assisted_decoding', '_auto_class', '_autoset_attn_implementation', '_backward_compatibility_gradient_checkpointing', '_backward_hooks', '_backward_pre_hooks', '_beam_search', '_buffers', '_call_impl', '_check_and_enable_flash_attn_2', '_check_and_enable_flex_attn', '_check_and_enable_sdpa', '_compiled_call_impl', '_constrained_beam_search', '_contrastive_search', '_convert_head_mask_to_5d', '_copy_lm_head_original_to_resized', '_create_repo', '_dispatch_accelerate_model', '_dola_decoding', '_expand_inputs_for_generation', '_extract_past_from_model_output', '_fix_state_dict_key_on_load', '_fix_state_dict_key_on_save', '_fix_state_dict_keys_on_load', '_fix_state_dict_keys_on_save', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_from_config', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_cache', '_get_candidate_generator', '_get_files_timestamps', '_get_initial_cache_position', '_get_logits_processor', '_get_name', '_get_no_split_modules', '_get_resized_embeddings', '_get_resized_lm_head', '_get_stopping_criteria', '_group_beam_search', '_has_unfinished_sequences', '_hf_peft_config_loaded', '_hook_rss_memory_post_forward', '_hook_rss_memory_pre_forward', '_init_added_embeddings_weights_with_mean', '_init_added_lm_head_bias_with_mean', '_init_added_lm_head_weights_with_mean', '_init_weights', '_initialize_weights', '_is_full_backward_hook', '_is_hf_initialized', '_is_stateful', '_keep_in_fp32_modules', '_keep_in_fp32_modules', '_keys_to_ignore_on_load_missing', '_keys_to_ignore_on_load_unexpected', '_keys_to_ignore_on_save', '_load_from_state_dict', '_load_pretrained_model', '_load_pretrained_model_low_mem', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_initialize_input_ids_for_generation', '_maybe_warn_non_full_backward_hook', '_merge_criteria_processor_list', '_modules', '_named_members', '_no_split_modules', '_non_persistent_buffers_set', '_parameters', '_prepare_attention_mask_for_generation', '_prepare_cache_for_generation', '_prepare_decoder_input_ids_for_generation', '_prepare_encoder_decoder_kwargs_for_generation', '_prepare_generated_length', '_prepare_generation_config', '_prepare_model_inputs', '_prepare_special_tokens', '_prune_heads', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_reorder_cache', '_replicate_for_data_parallel', '_resize_token_embeddings', '_sample', '_save_to_state_dict', '_set_default_torch_dtype', '_set_gradient_checkpointing', '_skip_keys_device_placement', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_supports_cache_class', '_supports_default_dynamic_cache', '_supports_flash_attn_2', '_supports_flex_attn', '_supports_num_logits_to_keep', '_supports_quantized_cache', '_supports_sdpa', '_supports_static_cache', '_temporary_reorder_cache', '_tie_encoder_decoder_weights', '_tie_or_clone_weights', '_tied_weights_keys', '_tp_plan', '_tp_plan', '_update_model_kwargs_for_generation', '_upload_modified_files', '_validate_assistant', '_validate_generated_length', '_validate_model_class', '_validate_model_kwargs', '_version', '_wrapped_call_impl', 'active_adapter', 'active_adapters', 'add_adapter', 'add_memory_hooks', 'add_model_tags', 'add_module', 'apply', 'base_model', 'base_model_prefix', 'bfloat16', 'buffers', 'call_super_init', 'can_generate', 'children', 'compile', 'compute_transition_scores', 'config', 'config_class', 'cpu', 'create_extended_attention_mask_for_decoder', 'cuda', 'delete_adapter', 'dequantize', 'device', 'disable_adapters', 'disable_input_require_grads', 'double', 'dtype', 'dummy_inputs', 'dump_patches', 'embeddings', 'enable_adapters', 'enable_input_require_grads', 'encoder', 'estimate_tokens', 'eval', 'extra_repr', 'float', 'floating_point_ops', 'forward', 'framework', 'from_pretrained', 'generate', 'generation_config', 'get_adapter_state_dict', 'get_buffer', 'get_compiled_call', 'get_extended_attention_mask', 'get_extra_state', 'get_head_mask', 'get_input_embeddings', 'get_memory_footprint', 'get_output_embeddings', 'get_parameter', 'get_position_embeddings', 'get_submodule', 'gradient_checkpointing_disable', 'gradient_checkpointing_enable', 'half', 'heal_tokens', 'init_weights', 'invert_attention_mask', 'ipu', 'is_gradient_checkpointing', 'is_parallelizable', 'layernorm', 'load_adapter', 'load_state_dict', 'loss_function', 'loss_type', 'main_input_name', 'model_tags', 'modules', 'mtia', 'name_or_path', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'num_parameters', 'parameters', 'post_init', 'prepare_inputs_for_generation', 'prune_heads', 'push_to_hub', 'register_backward_hook', 'register_buffer', 'register_for_auto_class', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'reset_memory_hooks_state', 'resize_position_embeddings', 'resize_token_embeddings', 'retrieve_modules_from_names', 'reverse_bettertransformer', 'save_pretrained', 'set_adapter', 'set_extra_state', 'set_input_embeddings', 'set_submodule', 'share_memory', 'state_dict', 'supports_gradient_checkpointing', 'supports_tp_plan', 'tensor_parallel', 'tie_weights', 'to', 'to_bettertransformer', 'to_empty', 'train', 'training', 'type', 'warn_if_padding_and_no_attention_mask', 'warnings_issued', 'xpu', 'zero_grad']\n"
     ]
    }
   ],
   "source": [
    "model = model.to('cuda:0')\n",
    "print(type(model))\n",
    "configuration = model.config\n",
    "print(configuration)\n",
    "print(dir(model))\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "inputs = inputs.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)\n",
    "outputs2 = model.forward(**inputs)\n",
    "#loss = outputs.loss\n",
    "mask = outputs.mask\n",
    "ids_restore = outputs.ids_restore\n",
    "\n",
    "\n",
    "# if(outputs == outputs2):\n",
    "#     print(\"True\")\n",
    "# print(type(outputs))\n",
    "# print(type(outputs2))\n",
    "\n",
    "# print((outputs.loss))\n",
    "# print((outputs2.loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T_destination',\n",
      " '__annotations__',\n",
      " '__call__',\n",
      " '__class__',\n",
      " '__delattr__',\n",
      " '__dict__',\n",
      " '__dir__',\n",
      " '__doc__',\n",
      " '__eq__',\n",
      " '__format__',\n",
      " '__ge__',\n",
      " '__getattr__',\n",
      " '__getattribute__',\n",
      " '__getstate__',\n",
      " '__gt__',\n",
      " '__hash__',\n",
      " '__init__',\n",
      " '__init_subclass__',\n",
      " '__le__',\n",
      " '__lt__',\n",
      " '__module__',\n",
      " '__ne__',\n",
      " '__new__',\n",
      " '__reduce__',\n",
      " '__reduce_ex__',\n",
      " '__repr__',\n",
      " '__setattr__',\n",
      " '__setstate__',\n",
      " '__sizeof__',\n",
      " '__str__',\n",
      " '__subclasshook__',\n",
      " '__weakref__',\n",
      " '_apply',\n",
      " '_assisted_decoding',\n",
      " '_auto_class',\n",
      " '_autoset_attn_implementation',\n",
      " '_backward_compatibility_gradient_checkpointing',\n",
      " '_backward_hooks',\n",
      " '_backward_pre_hooks',\n",
      " '_beam_search',\n",
      " '_buffers',\n",
      " '_call_impl',\n",
      " '_check_and_enable_flash_attn_2',\n",
      " '_check_and_enable_flex_attn',\n",
      " '_check_and_enable_sdpa',\n",
      " '_compiled_call_impl',\n",
      " '_constrained_beam_search',\n",
      " '_contrastive_search',\n",
      " '_convert_head_mask_to_5d',\n",
      " '_copy_lm_head_original_to_resized',\n",
      " '_create_repo',\n",
      " '_dispatch_accelerate_model',\n",
      " '_dola_decoding',\n",
      " '_expand_inputs_for_generation',\n",
      " '_extract_past_from_model_output',\n",
      " '_fix_state_dict_key_on_load',\n",
      " '_fix_state_dict_key_on_save',\n",
      " '_fix_state_dict_keys_on_load',\n",
      " '_fix_state_dict_keys_on_save',\n",
      " '_forward_hooks',\n",
      " '_forward_hooks_always_called',\n",
      " '_forward_hooks_with_kwargs',\n",
      " '_forward_pre_hooks',\n",
      " '_forward_pre_hooks_with_kwargs',\n",
      " '_from_config',\n",
      " '_get_backward_hooks',\n",
      " '_get_backward_pre_hooks',\n",
      " '_get_cache',\n",
      " '_get_candidate_generator',\n",
      " '_get_files_timestamps',\n",
      " '_get_initial_cache_position',\n",
      " '_get_logits_processor',\n",
      " '_get_name',\n",
      " '_get_no_split_modules',\n",
      " '_get_resized_embeddings',\n",
      " '_get_resized_lm_head',\n",
      " '_get_stopping_criteria',\n",
      " '_group_beam_search',\n",
      " '_has_unfinished_sequences',\n",
      " '_hf_peft_config_loaded',\n",
      " '_hook_rss_memory_post_forward',\n",
      " '_hook_rss_memory_pre_forward',\n",
      " '_init_added_embeddings_weights_with_mean',\n",
      " '_init_added_lm_head_bias_with_mean',\n",
      " '_init_added_lm_head_weights_with_mean',\n",
      " '_init_weights',\n",
      " '_initialize_weights',\n",
      " '_is_full_backward_hook',\n",
      " '_is_hf_initialized',\n",
      " '_is_stateful',\n",
      " '_keep_in_fp32_modules',\n",
      " '_keep_in_fp32_modules',\n",
      " '_keys_to_ignore_on_load_missing',\n",
      " '_keys_to_ignore_on_load_unexpected',\n",
      " '_keys_to_ignore_on_save',\n",
      " '_load_from_state_dict',\n",
      " '_load_pretrained_model',\n",
      " '_load_pretrained_model_low_mem',\n",
      " '_load_state_dict_post_hooks',\n",
      " '_load_state_dict_pre_hooks',\n",
      " '_maybe_initialize_input_ids_for_generation',\n",
      " '_maybe_warn_non_full_backward_hook',\n",
      " '_merge_criteria_processor_list',\n",
      " '_modules',\n",
      " '_named_members',\n",
      " '_no_split_modules',\n",
      " '_non_persistent_buffers_set',\n",
      " '_parameters',\n",
      " '_prepare_attention_mask_for_generation',\n",
      " '_prepare_cache_for_generation',\n",
      " '_prepare_decoder_input_ids_for_generation',\n",
      " '_prepare_encoder_decoder_kwargs_for_generation',\n",
      " '_prepare_generated_length',\n",
      " '_prepare_generation_config',\n",
      " '_prepare_model_inputs',\n",
      " '_prepare_special_tokens',\n",
      " '_prune_heads',\n",
      " '_register_load_state_dict_pre_hook',\n",
      " '_register_state_dict_hook',\n",
      " '_reorder_cache',\n",
      " '_replicate_for_data_parallel',\n",
      " '_resize_token_embeddings',\n",
      " '_sample',\n",
      " '_save_to_state_dict',\n",
      " '_set_default_torch_dtype',\n",
      " '_set_gradient_checkpointing',\n",
      " '_skip_keys_device_placement',\n",
      " '_slow_forward',\n",
      " '_state_dict_hooks',\n",
      " '_state_dict_pre_hooks',\n",
      " '_supports_cache_class',\n",
      " '_supports_default_dynamic_cache',\n",
      " '_supports_flash_attn_2',\n",
      " '_supports_flex_attn',\n",
      " '_supports_num_logits_to_keep',\n",
      " '_supports_quantized_cache',\n",
      " '_supports_sdpa',\n",
      " '_supports_static_cache',\n",
      " '_temporary_reorder_cache',\n",
      " '_tie_encoder_decoder_weights',\n",
      " '_tie_or_clone_weights',\n",
      " '_tied_weights_keys',\n",
      " '_tp_plan',\n",
      " '_tp_plan',\n",
      " '_update_model_kwargs_for_generation',\n",
      " '_upload_modified_files',\n",
      " '_validate_assistant',\n",
      " '_validate_generated_length',\n",
      " '_validate_model_class',\n",
      " '_validate_model_kwargs',\n",
      " '_version',\n",
      " '_wrapped_call_impl',\n",
      " 'active_adapter',\n",
      " 'active_adapters',\n",
      " 'add_adapter',\n",
      " 'add_memory_hooks',\n",
      " 'add_model_tags',\n",
      " 'add_module',\n",
      " 'apply',\n",
      " 'base_model',\n",
      " 'base_model_prefix',\n",
      " 'bfloat16',\n",
      " 'buffers',\n",
      " 'call_super_init',\n",
      " 'can_generate',\n",
      " 'children',\n",
      " 'compile',\n",
      " 'compute_transition_scores',\n",
      " 'config',\n",
      " 'config_class',\n",
      " 'cpu',\n",
      " 'create_extended_attention_mask_for_decoder',\n",
      " 'cuda',\n",
      " 'delete_adapter',\n",
      " 'dequantize',\n",
      " 'device',\n",
      " 'disable_adapters',\n",
      " 'disable_input_require_grads',\n",
      " 'double',\n",
      " 'dtype',\n",
      " 'dummy_inputs',\n",
      " 'dump_patches',\n",
      " 'embeddings',\n",
      " 'enable_adapters',\n",
      " 'enable_input_require_grads',\n",
      " 'encoder',\n",
      " 'estimate_tokens',\n",
      " 'eval',\n",
      " 'extra_repr',\n",
      " 'float',\n",
      " 'floating_point_ops',\n",
      " 'forward',\n",
      " 'framework',\n",
      " 'from_pretrained',\n",
      " 'generate',\n",
      " 'generation_config',\n",
      " 'get_adapter_state_dict',\n",
      " 'get_buffer',\n",
      " 'get_compiled_call',\n",
      " 'get_extended_attention_mask',\n",
      " 'get_extra_state',\n",
      " 'get_head_mask',\n",
      " 'get_input_embeddings',\n",
      " 'get_memory_footprint',\n",
      " 'get_output_embeddings',\n",
      " 'get_parameter',\n",
      " 'get_position_embeddings',\n",
      " 'get_submodule',\n",
      " 'gradient_checkpointing_disable',\n",
      " 'gradient_checkpointing_enable',\n",
      " 'half',\n",
      " 'heal_tokens',\n",
      " 'init_weights',\n",
      " 'invert_attention_mask',\n",
      " 'ipu',\n",
      " 'is_gradient_checkpointing',\n",
      " 'is_parallelizable',\n",
      " 'layernorm',\n",
      " 'load_adapter',\n",
      " 'load_state_dict',\n",
      " 'loss_function',\n",
      " 'loss_type',\n",
      " 'main_input_name',\n",
      " 'model_tags',\n",
      " 'modules',\n",
      " 'mtia',\n",
      " 'name_or_path',\n",
      " 'named_buffers',\n",
      " 'named_children',\n",
      " 'named_modules',\n",
      " 'named_parameters',\n",
      " 'num_parameters',\n",
      " 'parameters',\n",
      " 'post_init',\n",
      " 'prepare_inputs_for_generation',\n",
      " 'prune_heads',\n",
      " 'push_to_hub',\n",
      " 'register_backward_hook',\n",
      " 'register_buffer',\n",
      " 'register_for_auto_class',\n",
      " 'register_forward_hook',\n",
      " 'register_forward_pre_hook',\n",
      " 'register_full_backward_hook',\n",
      " 'register_full_backward_pre_hook',\n",
      " 'register_load_state_dict_post_hook',\n",
      " 'register_load_state_dict_pre_hook',\n",
      " 'register_module',\n",
      " 'register_parameter',\n",
      " 'register_state_dict_post_hook',\n",
      " 'register_state_dict_pre_hook',\n",
      " 'requires_grad_',\n",
      " 'reset_memory_hooks_state',\n",
      " 'resize_position_embeddings',\n",
      " 'resize_token_embeddings',\n",
      " 'retrieve_modules_from_names',\n",
      " 'reverse_bettertransformer',\n",
      " 'save_pretrained',\n",
      " 'set_adapter',\n",
      " 'set_extra_state',\n",
      " 'set_input_embeddings',\n",
      " 'set_submodule',\n",
      " 'share_memory',\n",
      " 'state_dict',\n",
      " 'supports_gradient_checkpointing',\n",
      " 'supports_tp_plan',\n",
      " 'tensor_parallel',\n",
      " 'tie_weights',\n",
      " 'to',\n",
      " 'to_bettertransformer',\n",
      " 'to_empty',\n",
      " 'train',\n",
      " 'training',\n",
      " 'type',\n",
      " 'warn_if_padding_and_no_attention_mask',\n",
      " 'warnings_issued',\n",
      " 'xpu',\n",
      " 'zero_grad']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "# Pretty print the directory list\n",
    "pprint.pprint(dir(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method PreTrainedModel.get_output_embeddings of ViTMAEModel(\n",
      "  (embeddings): ViTMAEEmbeddings(\n",
      "    (patch_embeddings): ViTMAEPatchEmbeddings(\n",
      "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "    )\n",
      "  )\n",
      "  (encoder): ViTMAEEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x ViTMAELayer(\n",
      "        (attention): ViTMAESdpaAttention(\n",
      "          (attention): ViTMAESdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (output): ViTMAESelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): ViTMAEIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): ViTMAEOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "output_embeddings = model.get_output_embeddings\n",
    "print(output_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196])\n",
      "tensor(147., device='cuda:0')\n",
      "['H', 'T', '__abs__', '__add__', '__and__', '__array__', '__array_priority__', '__array_wrap__', '__bool__', '__class__', '__complex__', '__contains__', '__cuda_array_interface__', '__deepcopy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__div__', '__dlpack__', '__dlpack_device__', '__doc__', '__eq__', '__float__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__iand__', '__idiv__', '__ifloordiv__', '__ilshift__', '__imod__', '__imul__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__ior__', '__ipow__', '__irshift__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__long__', '__lshift__', '__lt__', '__matmul__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rfloordiv__', '__rlshift__', '__rmatmul__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__torch_dispatch__', '__torch_function__', '__truediv__', '__weakref__', '__xor__', '_addmm_activation', '_autocast_to_full_precision', '_autocast_to_reduced_precision', '_backward_hooks', '_base', '_cdata', '_coalesced_', '_conj', '_conj_physical', '_dimI', '_dimV', '_fix_weakref', '_grad', '_grad_fn', '_has_symbolic_sizes_strides', '_indices', '_is_all_true', '_is_any_true', '_is_view', '_is_zerotensor', '_lazy_clone', '_make_subclass', '_make_wrapper_subclass', '_neg_view', '_nested_tensor_size', '_nested_tensor_storage_offsets', '_nested_tensor_strides', '_nnz', '_post_accumulate_grad_hooks', '_python_dispatch', '_reduce_ex_internal', '_rev_view_func_unsafe', '_sparse_mask_projection', '_to_dense', '_to_sparse', '_to_sparse_bsc', '_to_sparse_bsr', '_to_sparse_csc', '_to_sparse_csr', '_typed_storage', '_update_names', '_use_count', '_values', '_version', '_view_func', '_view_func_unsafe', 'abs', 'abs_', 'absolute', 'absolute_', 'acos', 'acos_', 'acosh', 'acosh_', 'add', 'add_', 'addbmm', 'addbmm_', 'addcdiv', 'addcdiv_', 'addcmul', 'addcmul_', 'addmm', 'addmm_', 'addmv', 'addmv_', 'addr', 'addr_', 'adjoint', 'align_as', 'align_to', 'all', 'allclose', 'amax', 'amin', 'aminmax', 'angle', 'any', 'apply_', 'arccos', 'arccos_', 'arccosh', 'arccosh_', 'arcsin', 'arcsin_', 'arcsinh', 'arcsinh_', 'arctan', 'arctan2', 'arctan2_', 'arctan_', 'arctanh', 'arctanh_', 'argmax', 'argmin', 'argsort', 'argwhere', 'as_strided', 'as_strided_', 'as_strided_scatter', 'as_subclass', 'asin', 'asin_', 'asinh', 'asinh_', 'atan', 'atan2', 'atan2_', 'atan_', 'atanh', 'atanh_', 'backward', 'baddbmm', 'baddbmm_', 'bernoulli', 'bernoulli_', 'bfloat16', 'bincount', 'bitwise_and', 'bitwise_and_', 'bitwise_left_shift', 'bitwise_left_shift_', 'bitwise_not', 'bitwise_not_', 'bitwise_or', 'bitwise_or_', 'bitwise_right_shift', 'bitwise_right_shift_', 'bitwise_xor', 'bitwise_xor_', 'bmm', 'bool', 'broadcast_to', 'byte', 'cauchy_', 'ccol_indices', 'cdouble', 'ceil', 'ceil_', 'cfloat', 'chalf', 'char', 'cholesky', 'cholesky_inverse', 'cholesky_solve', 'chunk', 'clamp', 'clamp_', 'clamp_max', 'clamp_max_', 'clamp_min', 'clamp_min_', 'clip', 'clip_', 'clone', 'coalesce', 'col_indices', 'conj', 'conj_physical', 'conj_physical_', 'contiguous', 'copy_', 'copysign', 'copysign_', 'corrcoef', 'cos', 'cos_', 'cosh', 'cosh_', 'count_nonzero', 'cov', 'cpu', 'cross', 'crow_indices', 'cuda', 'cummax', 'cummin', 'cumprod', 'cumprod_', 'cumsum', 'cumsum_', 'data', 'data_ptr', 'deg2rad', 'deg2rad_', 'dense_dim', 'dequantize', 'det', 'detach', 'detach_', 'device', 'diag', 'diag_embed', 'diagflat', 'diagonal', 'diagonal_scatter', 'diff', 'digamma', 'digamma_', 'dim', 'dim_order', 'dist', 'div', 'div_', 'divide', 'divide_', 'dot', 'double', 'dsplit', 'dtype', 'eig', 'element_size', 'eq', 'eq_', 'equal', 'erf', 'erf_', 'erfc', 'erfc_', 'erfinv', 'erfinv_', 'exp', 'exp2', 'exp2_', 'exp_', 'expand', 'expand_as', 'expm1', 'expm1_', 'exponential_', 'fill_', 'fill_diagonal_', 'fix', 'fix_', 'flatten', 'flip', 'fliplr', 'flipud', 'float', 'float_power', 'float_power_', 'floor', 'floor_', 'floor_divide', 'floor_divide_', 'fmax', 'fmin', 'fmod', 'fmod_', 'frac', 'frac_', 'frexp', 'gather', 'gcd', 'gcd_', 'ge', 'ge_', 'geometric_', 'geqrf', 'ger', 'get_device', 'grad', 'grad_fn', 'greater', 'greater_', 'greater_equal', 'greater_equal_', 'gt', 'gt_', 'half', 'hardshrink', 'has_names', 'heaviside', 'heaviside_', 'histc', 'histogram', 'hsplit', 'hypot', 'hypot_', 'i0', 'i0_', 'igamma', 'igamma_', 'igammac', 'igammac_', 'imag', 'index_add', 'index_add_', 'index_copy', 'index_copy_', 'index_fill', 'index_fill_', 'index_put', 'index_put_', 'index_reduce', 'index_reduce_', 'index_select', 'indices', 'inner', 'int', 'int_repr', 'inverse', 'ipu', 'is_coalesced', 'is_complex', 'is_conj', 'is_contiguous', 'is_cpu', 'is_cuda', 'is_distributed', 'is_floating_point', 'is_inference', 'is_ipu', 'is_leaf', 'is_maia', 'is_meta', 'is_mkldnn', 'is_mps', 'is_mtia', 'is_neg', 'is_nested', 'is_nonzero', 'is_pinned', 'is_quantized', 'is_same_size', 'is_set_to', 'is_shared', 'is_signed', 'is_sparse', 'is_sparse_csr', 'is_vulkan', 'is_xla', 'is_xpu', 'isclose', 'isfinite', 'isinf', 'isnan', 'isneginf', 'isposinf', 'isreal', 'istft', 'item', 'itemsize', 'kron', 'kthvalue', 'layout', 'lcm', 'lcm_', 'ldexp', 'ldexp_', 'le', 'le_', 'lerp', 'lerp_', 'less', 'less_', 'less_equal', 'less_equal_', 'lgamma', 'lgamma_', 'log', 'log10', 'log10_', 'log1p', 'log1p_', 'log2', 'log2_', 'log_', 'log_normal_', 'log_softmax', 'logaddexp', 'logaddexp2', 'logcumsumexp', 'logdet', 'logical_and', 'logical_and_', 'logical_not', 'logical_not_', 'logical_or', 'logical_or_', 'logical_xor', 'logical_xor_', 'logit', 'logit_', 'logsumexp', 'long', 'lstsq', 'lt', 'lt_', 'lu', 'lu_solve', 'mH', 'mT', 'map2_', 'map_', 'masked_fill', 'masked_fill_', 'masked_scatter', 'masked_scatter_', 'masked_select', 'matmul', 'matrix_exp', 'matrix_power', 'max', 'maximum', 'mean', 'median', 'min', 'minimum', 'mm', 'mode', 'module_load', 'moveaxis', 'movedim', 'msort', 'mtia', 'mul', 'mul_', 'multinomial', 'multiply', 'multiply_', 'mv', 'mvlgamma', 'mvlgamma_', 'name', 'names', 'nan_to_num', 'nan_to_num_', 'nanmean', 'nanmedian', 'nanquantile', 'nansum', 'narrow', 'narrow_copy', 'nbytes', 'ndim', 'ndimension', 'ne', 'ne_', 'neg', 'neg_', 'negative', 'negative_', 'nelement', 'new', 'new_empty', 'new_empty_strided', 'new_full', 'new_ones', 'new_tensor', 'new_zeros', 'nextafter', 'nextafter_', 'nonzero', 'nonzero_static', 'norm', 'normal_', 'not_equal', 'not_equal_', 'numel', 'numpy', 'orgqr', 'ormqr', 'outer', 'output_nr', 'permute', 'pin_memory', 'pinverse', 'polygamma', 'polygamma_', 'positive', 'pow', 'pow_', 'prelu', 'prod', 'put', 'put_', 'q_per_channel_axis', 'q_per_channel_scales', 'q_per_channel_zero_points', 'q_scale', 'q_zero_point', 'qr', 'qscheme', 'quantile', 'rad2deg', 'rad2deg_', 'random_', 'ravel', 'real', 'reciprocal', 'reciprocal_', 'record_stream', 'refine_names', 'register_hook', 'register_post_accumulate_grad_hook', 'reinforce', 'relu', 'relu_', 'remainder', 'remainder_', 'rename', 'rename_', 'renorm', 'renorm_', 'repeat', 'repeat_interleave', 'requires_grad', 'requires_grad_', 'reshape', 'reshape_as', 'resize', 'resize_', 'resize_as', 'resize_as_', 'resize_as_sparse_', 'resolve_conj', 'resolve_neg', 'retain_grad', 'retains_grad', 'roll', 'rot90', 'round', 'round_', 'row_indices', 'rsqrt', 'rsqrt_', 'scatter', 'scatter_', 'scatter_add', 'scatter_add_', 'scatter_reduce', 'scatter_reduce_', 'select', 'select_scatter', 'set_', 'sgn', 'sgn_', 'shape', 'share_memory_', 'short', 'sigmoid', 'sigmoid_', 'sign', 'sign_', 'signbit', 'sin', 'sin_', 'sinc', 'sinc_', 'sinh', 'sinh_', 'size', 'slice_inverse', 'slice_scatter', 'slogdet', 'smm', 'softmax', 'solve', 'sort', 'sparse_dim', 'sparse_mask', 'sparse_resize_', 'sparse_resize_and_clear_', 'split', 'split_with_sizes', 'sqrt', 'sqrt_', 'square', 'square_', 'squeeze', 'squeeze_', 'sspaddmm', 'std', 'stft', 'storage', 'storage_offset', 'storage_type', 'stride', 'sub', 'sub_', 'subtract', 'subtract_', 'sum', 'sum_to_size', 'svd', 'swapaxes', 'swapaxes_', 'swapdims', 'swapdims_', 'symeig', 't', 't_', 'take', 'take_along_dim', 'tan', 'tan_', 'tanh', 'tanh_', 'tensor_split', 'tile', 'to', 'to_dense', 'to_mkldnn', 'to_padded_tensor', 'to_sparse', 'to_sparse_bsc', 'to_sparse_bsr', 'to_sparse_coo', 'to_sparse_csc', 'to_sparse_csr', 'tolist', 'topk', 'trace', 'transpose', 'transpose_', 'triangular_solve', 'tril', 'tril_', 'triu', 'triu_', 'true_divide', 'true_divide_', 'trunc', 'trunc_', 'type', 'type_as', 'unbind', 'unflatten', 'unfold', 'uniform_', 'unique', 'unique_consecutive', 'unsafe_chunk', 'unsafe_split', 'unsafe_split_with_sizes', 'unsqueeze', 'unsqueeze_', 'untyped_storage', 'values', 'var', 'vdot', 'view', 'view_as', 'vsplit', 'where', 'xlogy', 'xlogy_', 'xpu', 'zero_']\n",
      "tensor([[0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(mask.shape)\n",
    "print(mask.sum())\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mka_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
